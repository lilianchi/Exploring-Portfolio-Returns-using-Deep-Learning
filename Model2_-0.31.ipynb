{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NcDhtxLCV8Tw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, LayerNormalization, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 426
    },
    "id": "vI12F-E9V8T1",
    "outputId": "869138a1-ef3d-4c37-86c8-e408129de90a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xl = pd.read_csv('Xl.csv',header=None)\n",
    "Xs = pd.read_csv('Xs.csv',header=None)\n",
    "R = pd.read_csv('y.csv',header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>127</th>\n",
       "      <th>128</th>\n",
       "      <th>129</th>\n",
       "      <th>130</th>\n",
       "      <th>131</th>\n",
       "      <th>132</th>\n",
       "      <th>133</th>\n",
       "      <th>134</th>\n",
       "      <th>135</th>\n",
       "      <th>136</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.451521</td>\n",
       "      <td>-2.928627</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>2437.296</td>\n",
       "      <td>...</td>\n",
       "      <td>11.358</td>\n",
       "      <td>2.13</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.04</td>\n",
       "      <td>86.897162</td>\n",
       "      <td>274.9</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>84.2043</td>\n",
       "      <td>19.214685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.447556</td>\n",
       "      <td>-2.903999</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.494946</td>\n",
       "      <td>0.027899</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>-0.0153</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.003448</td>\n",
       "      <td>2446.902</td>\n",
       "      <td>...</td>\n",
       "      <td>11.375</td>\n",
       "      <td>2.14</td>\n",
       "      <td>2.46</td>\n",
       "      <td>2.05</td>\n",
       "      <td>86.897162</td>\n",
       "      <td>276.0</td>\n",
       "      <td>6476.00</td>\n",
       "      <td>12298.00</td>\n",
       "      <td>83.5280</td>\n",
       "      <td>19.214685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.444322</td>\n",
       "      <td>-2.880679</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.516860</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>-0.0263</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2462.689</td>\n",
       "      <td>...</td>\n",
       "      <td>11.395</td>\n",
       "      <td>2.15</td>\n",
       "      <td>2.45</td>\n",
       "      <td>2.07</td>\n",
       "      <td>86.897162</td>\n",
       "      <td>277.4</td>\n",
       "      <td>6508.00</td>\n",
       "      <td>12349.00</td>\n",
       "      <td>81.6405</td>\n",
       "      <td>19.214685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.478608</td>\n",
       "      <td>-2.888116</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.498597</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.003460</td>\n",
       "      <td>2478.744</td>\n",
       "      <td>...</td>\n",
       "      <td>11.436</td>\n",
       "      <td>2.16</td>\n",
       "      <td>2.47</td>\n",
       "      <td>2.08</td>\n",
       "      <td>86.897162</td>\n",
       "      <td>278.1</td>\n",
       "      <td>6620.00</td>\n",
       "      <td>12484.00</td>\n",
       "      <td>81.8099</td>\n",
       "      <td>19.214685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.493617</td>\n",
       "      <td>-2.877168</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.483077</td>\n",
       "      <td>0.025887</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>-0.0289</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2493.228</td>\n",
       "      <td>...</td>\n",
       "      <td>11.454</td>\n",
       "      <td>2.17</td>\n",
       "      <td>2.48</td>\n",
       "      <td>2.08</td>\n",
       "      <td>95.300000</td>\n",
       "      <td>280.1</td>\n",
       "      <td>6753.00</td>\n",
       "      <td>12646.00</td>\n",
       "      <td>80.7315</td>\n",
       "      <td>19.214685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>-4.080892</td>\n",
       "      <td>-3.569975</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.235975</td>\n",
       "      <td>-0.008504</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.003153</td>\n",
       "      <td>17603.151</td>\n",
       "      <td>...</td>\n",
       "      <td>120.338</td>\n",
       "      <td>25.51</td>\n",
       "      <td>29.39</td>\n",
       "      <td>22.87</td>\n",
       "      <td>74.100000</td>\n",
       "      <td>21074.4</td>\n",
       "      <td>344023.25</td>\n",
       "      <td>726723.85</td>\n",
       "      <td>4367.8773</td>\n",
       "      <td>20.277700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>-4.045576</td>\n",
       "      <td>-3.533379</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.241482</td>\n",
       "      <td>-0.005698</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>17698.892</td>\n",
       "      <td>...</td>\n",
       "      <td>120.743</td>\n",
       "      <td>25.49</td>\n",
       "      <td>29.09</td>\n",
       "      <td>23.01</td>\n",
       "      <td>80.400000</td>\n",
       "      <td>21249.9</td>\n",
       "      <td>347627.43</td>\n",
       "      <td>730734.42</td>\n",
       "      <td>4425.5453</td>\n",
       "      <td>28.324100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>-4.020767</td>\n",
       "      <td>-3.519301</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.253146</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.000415</td>\n",
       "      <td>17573.127</td>\n",
       "      <td>...</td>\n",
       "      <td>120.871</td>\n",
       "      <td>25.58</td>\n",
       "      <td>29.40</td>\n",
       "      <td>22.99</td>\n",
       "      <td>81.800000</td>\n",
       "      <td>21369.3</td>\n",
       "      <td>348262.68</td>\n",
       "      <td>730398.69</td>\n",
       "      <td>4505.3741</td>\n",
       "      <td>30.940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>-4.126172</td>\n",
       "      <td>-3.635623</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.226352</td>\n",
       "      <td>-0.005262</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.000611</td>\n",
       "      <td>17355.669</td>\n",
       "      <td>...</td>\n",
       "      <td>120.874</td>\n",
       "      <td>25.69</td>\n",
       "      <td>29.55</td>\n",
       "      <td>23.10</td>\n",
       "      <td>76.900000</td>\n",
       "      <td>21565.0</td>\n",
       "      <td>350766.10</td>\n",
       "      <td>733096.73</td>\n",
       "      <td>4609.7215</td>\n",
       "      <td>24.804700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>-4.165889</td>\n",
       "      <td>-3.686452</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.219195</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>17386.005</td>\n",
       "      <td>...</td>\n",
       "      <td>121.328</td>\n",
       "      <td>25.77</td>\n",
       "      <td>29.64</td>\n",
       "      <td>23.12</td>\n",
       "      <td>80.700000</td>\n",
       "      <td>21741.0</td>\n",
       "      <td>350336.43</td>\n",
       "      <td>733463.42</td>\n",
       "      <td>4671.9751</td>\n",
       "      <td>21.680300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4       5       6       7    \\\n",
       "0   -3.451521 -2.928627  0.000585  0.502896  0.027736  0.0282 -0.0362  0.0075   \n",
       "1   -3.447556 -2.903999  0.000845  0.494946  0.027899  0.0270 -0.0153  0.0075   \n",
       "2   -3.444322 -2.880679  0.000492  0.516860  0.024984  0.0280 -0.0263  0.0072   \n",
       "3   -3.478608 -2.888116  0.000493  0.498597  0.024361  0.0295 -0.0412  0.0063   \n",
       "4   -3.493617 -2.877168  0.000428  0.483077  0.025887  0.0284 -0.0289  0.0059   \n",
       "..        ...       ...       ...       ...       ...     ...     ...     ...   \n",
       "739 -4.080892 -3.569975  0.000743  0.235975 -0.008504  0.0010 -0.0359  0.0102   \n",
       "740 -4.045576 -3.533379  0.004907  0.241482 -0.005698  0.0011  0.0069  0.0105   \n",
       "741 -4.020767 -3.519301  0.003661  0.253146 -0.001895  0.0010 -0.0248  0.0109   \n",
       "742 -4.126172 -3.635623  0.002492  0.226352 -0.005262  0.0009  0.0084  0.0100   \n",
       "743 -4.165889 -3.686452  0.000678  0.219195 -0.000094  0.0009 -0.0124  0.0090   \n",
       "\n",
       "          8          9    ...      127    128    129    130        131  \\\n",
       "0    0.003460   2437.296  ...   11.358   2.13   2.45   2.04  86.897162   \n",
       "1   -0.003448   2446.902  ...   11.375   2.14   2.46   2.05  86.897162   \n",
       "2    0.000000   2462.689  ...   11.395   2.15   2.45   2.07  86.897162   \n",
       "3    0.003460   2478.744  ...   11.436   2.16   2.47   2.08  86.897162   \n",
       "4    0.000000   2493.228  ...   11.454   2.17   2.48   2.08  95.300000   \n",
       "..        ...        ...  ...      ...    ...    ...    ...        ...   \n",
       "739  0.003153  17603.151  ...  120.338  25.51  29.39  22.87  74.100000   \n",
       "740  0.001393  17698.892  ...  120.743  25.49  29.09  23.01  80.400000   \n",
       "741  0.000415  17573.127  ...  120.871  25.58  29.40  22.99  81.800000   \n",
       "742 -0.000611  17355.669  ...  120.874  25.69  29.55  23.10  76.900000   \n",
       "743  0.000941  17386.005  ...  121.328  25.77  29.64  23.12  80.700000   \n",
       "\n",
       "         132        133        134        135        136  \n",
       "0      274.9    6476.00   12298.00    84.2043  19.214685  \n",
       "1      276.0    6476.00   12298.00    83.5280  19.214685  \n",
       "2      277.4    6508.00   12349.00    81.6405  19.214685  \n",
       "3      278.1    6620.00   12484.00    81.8099  19.214685  \n",
       "4      280.1    6753.00   12646.00    80.7315  19.214685  \n",
       "..       ...        ...        ...        ...        ...  \n",
       "739  21074.4  344023.25  726723.85  4367.8773  20.277700  \n",
       "740  21249.9  347627.43  730734.42  4425.5453  28.324100  \n",
       "741  21369.3  348262.68  730398.69  4505.3741  30.940800  \n",
       "742  21565.0  350766.10  733096.73  4609.7215  24.804700  \n",
       "743  21741.0  350336.43  733463.42  4671.9751  21.680300  \n",
       "\n",
       "[744 rows x 137 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-3.451521</td>\n",
       "      <td>-2.928627</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.502896</td>\n",
       "      <td>0.027736</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>-0.0362</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>0.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.447556</td>\n",
       "      <td>-2.903999</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.494946</td>\n",
       "      <td>0.027899</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>-0.0153</td>\n",
       "      <td>0.0075</td>\n",
       "      <td>-0.003448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.444322</td>\n",
       "      <td>-2.880679</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.516860</td>\n",
       "      <td>0.024984</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>-0.0263</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3.478608</td>\n",
       "      <td>-2.888116</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.498597</td>\n",
       "      <td>0.024361</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>-0.0412</td>\n",
       "      <td>0.0063</td>\n",
       "      <td>0.003460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.493617</td>\n",
       "      <td>-2.877168</td>\n",
       "      <td>0.000428</td>\n",
       "      <td>0.483077</td>\n",
       "      <td>0.025887</td>\n",
       "      <td>0.0284</td>\n",
       "      <td>-0.0289</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>-4.080892</td>\n",
       "      <td>-3.569975</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.235975</td>\n",
       "      <td>-0.008504</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0359</td>\n",
       "      <td>0.0102</td>\n",
       "      <td>0.003153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>-4.045576</td>\n",
       "      <td>-3.533379</td>\n",
       "      <td>0.004907</td>\n",
       "      <td>0.241482</td>\n",
       "      <td>-0.005698</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.001393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>-4.020767</td>\n",
       "      <td>-3.519301</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.253146</td>\n",
       "      <td>-0.001895</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>-0.0248</td>\n",
       "      <td>0.0109</td>\n",
       "      <td>0.000415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>-4.126172</td>\n",
       "      <td>-3.635623</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.226352</td>\n",
       "      <td>-0.005262</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>-0.000611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>-4.165889</td>\n",
       "      <td>-3.686452</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.219195</td>\n",
       "      <td>-0.000094</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0124</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.000941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2         3         4       5       6       7  \\\n",
       "0   -3.451521 -2.928627  0.000585  0.502896  0.027736  0.0282 -0.0362  0.0075   \n",
       "1   -3.447556 -2.903999  0.000845  0.494946  0.027899  0.0270 -0.0153  0.0075   \n",
       "2   -3.444322 -2.880679  0.000492  0.516860  0.024984  0.0280 -0.0263  0.0072   \n",
       "3   -3.478608 -2.888116  0.000493  0.498597  0.024361  0.0295 -0.0412  0.0063   \n",
       "4   -3.493617 -2.877168  0.000428  0.483077  0.025887  0.0284 -0.0289  0.0059   \n",
       "..        ...       ...       ...       ...       ...     ...     ...     ...   \n",
       "739 -4.080892 -3.569975  0.000743  0.235975 -0.008504  0.0010 -0.0359  0.0102   \n",
       "740 -4.045576 -3.533379  0.004907  0.241482 -0.005698  0.0011  0.0069  0.0105   \n",
       "741 -4.020767 -3.519301  0.003661  0.253146 -0.001895  0.0010 -0.0248  0.0109   \n",
       "742 -4.126172 -3.635623  0.002492  0.226352 -0.005262  0.0009  0.0084  0.0100   \n",
       "743 -4.165889 -3.686452  0.000678  0.219195 -0.000094  0.0009 -0.0124  0.0090   \n",
       "\n",
       "            8  \n",
       "0    0.003460  \n",
       "1   -0.003448  \n",
       "2    0.000000  \n",
       "3    0.003460  \n",
       "4    0.000000  \n",
       "..        ...  \n",
       "739  0.003153  \n",
       "740  0.001393  \n",
       "741  0.000415  \n",
       "742 -0.000611  \n",
       "743  0.000941  \n",
       "\n",
       "[744 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_agg=R.iloc[::-1].rolling(window=12).sum().iloc[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "Psf8guRAV8T1",
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_train_start = list(range(46*12))\n",
    "t_train_end =[x+120 for x in t_train_start]\n",
    "t_val_start= [x for x in t_train_end]\n",
    "t_val_end = [x+60 for x in t_val_start]\n",
    "t_test_start = [x for x in t_val_end]\n",
    "t_test_end = [x for x in t_test_start]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(552,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(t_test_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "731"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_test_end[551]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 1.8747 - val_loss: 0.0327\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 16.7953 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.1593 - val_loss: 0.0364\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.6898 - val_loss: 0.0373\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0571 - val_loss: 0.0402\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.3155 - val_loss: 0.0401\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.4021 - val_loss: 0.0429\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.7605 - val_loss: 0.0416\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0295 - val_loss: 0.0431\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1133 - val_loss: 0.0448\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2381 - val_loss: 0.0440\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4960 - val_loss: 0.0453\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2994 - val_loss: 0.0445\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2065 - val_loss: 0.0463\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2098 - val_loss: 0.0435\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2958 - val_loss: 0.0486\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_1 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.5619 - val_loss: 0.0351\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.8408 - val_loss: 0.0353\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 17.7260 - val_loss: 0.0382\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.1788 - val_loss: 0.0402\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.0700 - val_loss: 0.0396\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.0721 - val_loss: 0.0403\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 4.641 - 0s 35ms/step - loss: 4.6411 - val_loss: 0.0401\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2127 - val_loss: 0.0394\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5901 - val_loss: 0.0407\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.3540 - val_loss: 0.0382\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9506 - val_loss: 0.0409\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6481 - val_loss: 0.0381\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7882 - val_loss: 0.0418\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7688 - val_loss: 0.0371\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6332 - val_loss: 0.0419\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5650 - val_loss: 0.0365\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 1.2504 - val_loss: 0.0312\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.7373 - val_loss: 0.0368\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 21.7863 - val_loss: 0.0363\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.3022 - val_loss: 0.0375\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.0927 - val_loss: 0.0355\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.9794 - val_loss: 0.0350\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.7854 - val_loss: 0.0353\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.8349 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.7578 - val_loss: 0.0349\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4537 - val_loss: 0.0319\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5235 - val_loss: 0.0325\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7994 - val_loss: 0.0314\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6007 - val_loss: 0.0317\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6538 - val_loss: 0.0315\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6538 - val_loss: 0.0315\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5638 - val_loss: 0.0319\n",
      "Epoch 00016: early stopping\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 1.6538 - val_loss: 0.0351\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 16.2743 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 13.8622 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.4381 - val_loss: 0.0368\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.1386 - val_loss: 0.0393\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 7.4911 - val_loss: 0.0393\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.3887 - val_loss: 0.0388\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9791 - val_loss: 0.0409\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.9911 - val_loss: 0.0404\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6103 - val_loss: 0.0400\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4238 - val_loss: 0.0414\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.770 - 0s 39ms/step - loss: 0.7705 - val_loss: 0.0395\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3495 - val_loss: 0.0414\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1391 - val_loss: 0.0401\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0770 - val_loss: 0.0414\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0709 - val_loss: 0.0404\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.0937 - val_loss: 0.0416\n",
      "Epoch 00017: early stopping\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_4 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.3889 - val_loss: 0.0335\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.8377 - val_loss: 0.0336\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 17.8149 - val_loss: 0.0367\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 14.4224 - val_loss: 0.0356\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 8.3505 - val_loss: 0.0363\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.3695 - val_loss: 0.0358\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.5877 - val_loss: 0.0369\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.8663 - val_loss: 0.0360\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.6382 - val_loss: 0.0364\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7809 - val_loss: 0.0354\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4072 - val_loss: 0.0359\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4446 - val_loss: 0.0353\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6763 - val_loss: 0.0351\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7491 - val_loss: 0.0358\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5932 - val_loss: 0.0341\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3793 - val_loss: 0.0363\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC37EA040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.8164 - val_loss: 0.0316\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.9791 - val_loss: 0.0327\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 23.2009 - val_loss: 0.0389\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.1759 - val_loss: 0.0373\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10.0770 - val_loss: 0.0410\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.1425 - val_loss: 0.0402\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.9761 - val_loss: 0.0426\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.8114 - val_loss: 0.0411\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3656 - val_loss: 0.0449\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0755 - val_loss: 0.0440\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7582 - val_loss: 0.0465\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5418 - val_loss: 0.0471\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4016 - val_loss: 0.0485\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3215 - val_loss: 0.0502\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2621 - val_loss: 0.0515\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2301 - val_loss: 0.0531\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBDFB53A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 1.7355 - val_loss: 0.0323\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 14.7657 - val_loss: 0.0323\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 16.4143 - val_loss: 0.0333\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 13.5795 - val_loss: 0.0325\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.4827 - val_loss: 0.0337\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.3945 - val_loss: 0.0336\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.9615 - val_loss: 0.0340\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.0399 - val_loss: 0.0339\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.9297 - val_loss: 0.0346\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6746 - val_loss: 0.0345\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1084 - val_loss: 0.0351\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5316 - val_loss: 0.0347\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3526 - val_loss: 0.0350\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2625 - val_loss: 0.0353\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2368 - val_loss: 0.0352\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2352 - val_loss: 0.0351\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2613 - val_loss: 0.0357\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB97E0160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_7 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 287ms/step - loss: 2.3778 - val_loss: 0.0297\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.0102 - val_loss: 0.0309\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 26.9543 - val_loss: 0.0308\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.6094 - val_loss: 0.0312\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.3531 - val_loss: 0.0303\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.6479 - val_loss: 0.0314\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.9782 - val_loss: 0.0299\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1835 - val_loss: 0.0308\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2427 - val_loss: 0.0299\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7992 - val_loss: 0.0305\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6053 - val_loss: 0.0298\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4808 - val_loss: 0.0302\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3547 - val_loss: 0.0298\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3287 - val_loss: 0.0300\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3474 - val_loss: 0.0296\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4350 - val_loss: 0.0297\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5924 - val_loss: 0.0297\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8192 - val_loss: 0.0297\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0792 - val_loss: 0.0298\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0900 - val_loss: 0.0297\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8892 - val_loss: 0.0300\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6744 - val_loss: 0.0296\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5813 - val_loss: 0.0298\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5395 - val_loss: 0.0301\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5706 - val_loss: 0.0296\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5546 - val_loss: 0.0303\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5334 - val_loss: 0.0297\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5328 - val_loss: 0.0303\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4372 - val_loss: 0.0297\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3828 - val_loss: 0.0299\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:8 out of the last 8 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC192C9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 1.5920 - val_loss: 0.0310\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.8515 - val_loss: 0.0310\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 20.2666 - val_loss: 0.0307\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.2554 - val_loss: 0.0305\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 13.2405 - val_loss: 0.0305\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.3761 - val_loss: 0.0302\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.1769 - val_loss: 0.0304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.170 - 0s 40ms/step - loss: 3.1702 - val_loss: 0.0303\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3757 - val_loss: 0.0304\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9066 - val_loss: 0.0303\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4330 - val_loss: 0.0302\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3463 - val_loss: 0.0304\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3909 - val_loss: 0.0303\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4987 - val_loss: 0.0309\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6044 - val_loss: 0.0303\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6536 - val_loss: 0.0318\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5799 - val_loss: 0.0304\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4545 - val_loss: 0.0326\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3641 - val_loss: 0.0309\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2891 - val_loss: 0.0334\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2464 - val_loss: 0.0318\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:9 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAAF3280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.1035 - val_loss: 0.0317\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.6053 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.5503 - val_loss: 0.0325\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.7442 - val_loss: 0.0325\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9028 - val_loss: 0.0329\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.8440 - val_loss: 0.0328\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.8502 - val_loss: 0.0349\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.1968 - val_loss: 0.0334\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5482 - val_loss: 0.0340\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9936 - val_loss: 0.0345\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3020 - val_loss: 0.0340\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6818 - val_loss: 0.0341\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3155 - val_loss: 0.0337\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1543 - val_loss: 0.0338\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1271 - val_loss: 0.0335\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1594 - val_loss: 0.0339\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC5A430D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_10 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.2277 - val_loss: 0.0330\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.0829 - val_loss: 0.0320\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.3501 - val_loss: 0.0327\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.2128 - val_loss: 0.0325\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.9820 - val_loss: 0.0324\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6655 - val_loss: 0.0320\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.7052 - val_loss: 0.0322\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.7082 - val_loss: 0.0321\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 102ms/step - loss: 2.6880 - val_loss: 0.0323\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2995 - val_loss: 0.0322\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6636 - val_loss: 0.0323\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8346 - val_loss: 0.0328\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7000 - val_loss: 0.0322\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4351 - val_loss: 0.0326\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2619 - val_loss: 0.0322\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2519 - val_loss: 0.0325\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3382 - val_loss: 0.0324\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5289 - val_loss: 0.0322\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7733 - val_loss: 0.0329\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8453 - val_loss: 0.0321\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6108 - val_loss: 0.0329\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECF04CC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.9809 - val_loss: 0.0331\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 17.7961 - val_loss: 0.0328\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.6778 - val_loss: 0.0331\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.2481 - val_loss: 0.0331\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.6531 - val_loss: 0.0332\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.7524 - val_loss: 0.0338\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.6816 - val_loss: 0.0335\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.7563 - val_loss: 0.0341\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4669 - val_loss: 0.0337\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6886 - val_loss: 0.0339\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2568 - val_loss: 0.0341\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7864 - val_loss: 0.0337\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4900 - val_loss: 0.0343\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4094 - val_loss: 0.0337\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3439 - val_loss: 0.0347\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3688 - val_loss: 0.0336\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4032 - val_loss: 0.0352\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB97E0160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.6854 - val_loss: 0.0346\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.5816 - val_loss: 0.0343\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 15.7877 - val_loss: 0.0341\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.8240 - val_loss: 0.0342\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.9593 - val_loss: 0.0341\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.0410 - val_loss: 0.0341\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0209 - val_loss: 0.0348\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7019 - val_loss: 0.0344\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3940 - val_loss: 0.0359\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.8277 - val_loss: 0.0353\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1066 - val_loss: 0.0365\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9193 - val_loss: 0.0360\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6731 - val_loss: 0.0369\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5342 - val_loss: 0.0377\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3837 - val_loss: 0.0371\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2670 - val_loss: 0.0392\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1913 - val_loss: 0.0370\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1676 - val_loss: 0.0407\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC17EEEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_13 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 662ms/step - loss: 1.5396 - val_loss: 0.0355\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.7732 - val_loss: 0.0357\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 16.5949 - val_loss: 0.0354\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 77ms/step - loss: 11.4330 - val_loss: 0.0359\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.5523 - val_loss: 0.0356\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.8679 - val_loss: 0.0356\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.4608 - val_loss: 0.0355\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8174 - val_loss: 0.0355\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0544 - val_loss: 0.0353\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.6890 - val_loss: 0.0355\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0360 - val_loss: 0.0352\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7949 - val_loss: 0.0355\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8416 - val_loss: 0.0351\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6893 - val_loss: 0.0354\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4818 - val_loss: 0.0351\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3534 - val_loss: 0.0355\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3564 - val_loss: 0.0352\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4431 - val_loss: 0.0357\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6133 - val_loss: 0.0352\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7206 - val_loss: 0.0360\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7556 - val_loss: 0.0354\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6381 - val_loss: 0.0362\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4529 - val_loss: 0.0354\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3204 - val_loss: 0.0364\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2533 - val_loss: 0.0354\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2530 - val_loss: 0.0371\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3489 - val_loss: 0.0358\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5272 - val_loss: 0.0379\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6782 - val_loss: 0.0358\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6580 - val_loss: 0.0366\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC37EA040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.6151 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.1752 - val_loss: 0.0362\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 17.9971 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.7910 - val_loss: 0.0359\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.7928 - val_loss: 0.0356\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.3797 - val_loss: 0.0361\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0759 - val_loss: 0.0357\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.3857 - val_loss: 0.0360\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.1389 - val_loss: 0.0361\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4229 - val_loss: 0.0356\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5645 - val_loss: 0.0362\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2571 - val_loss: 0.0355\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9376 - val_loss: 0.0359\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4655 - val_loss: 0.0356\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2331 - val_loss: 0.0359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1387 - val_loss: 0.0359\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1158 - val_loss: 0.0360\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1300 - val_loss: 0.0360\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1759 - val_loss: 0.0361\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2581 - val_loss: 0.0362\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3885 - val_loss: 0.0363\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5452 - val_loss: 0.0360\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7112 - val_loss: 0.0371\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8679 - val_loss: 0.0361\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9584 - val_loss: 0.0379\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9119 - val_loss: 0.0359\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9690 - val_loss: 0.0375\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED061EDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 1.5370 - val_loss: 0.0406\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.1915 - val_loss: 0.0363\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 14.3718 - val_loss: 0.0366\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.6638 - val_loss: 0.0371\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.8089 - val_loss: 0.0371\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.2603 - val_loss: 0.0373\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5273 - val_loss: 0.0376\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.5092 - val_loss: 0.0378\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4173 - val_loss: 0.0380\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.7420 - val_loss: 0.0381\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9475 - val_loss: 0.0383\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4262 - val_loss: 0.0384\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1081 - val_loss: 0.0395\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8281 - val_loss: 0.0390\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5103 - val_loss: 0.0397\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3332 - val_loss: 0.0398\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2316 - val_loss: 0.0406\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB9728310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_16 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 2.2848 - val_loss: 0.0361\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.4918 - val_loss: 0.0367\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 18.5126 - val_loss: 0.0363\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.7431 - val_loss: 0.0370\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.0665 - val_loss: 0.0374\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.9102 - val_loss: 0.0386\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.0944 - val_loss: 0.0375\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.6807 - val_loss: 0.0400\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8315 - val_loss: 0.0375\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6524 - val_loss: 0.0415\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4987 - val_loss: 0.0387\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0276 - val_loss: 0.0424\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5157 - val_loss: 0.0405\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2533 - val_loss: 0.0440\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1449 - val_loss: 0.0425\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1124 - val_loss: 0.0453\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED42BB1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 4.6312 - val_loss: 0.0369\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.7863 - val_loss: 0.0358\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 20.5446 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.4949 - val_loss: 0.0359\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.2116 - val_loss: 0.0361\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.0357 - val_loss: 0.0356\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.6357 - val_loss: 0.0357\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6055 - val_loss: 0.0356\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6900 - val_loss: 0.0357\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8854 - val_loss: 0.0357\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3523 - val_loss: 0.0358\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3080 - val_loss: 0.0357\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3217 - val_loss: 0.0360\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3349 - val_loss: 0.0356\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3482 - val_loss: 0.0361\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3840 - val_loss: 0.0356\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4764 - val_loss: 0.0366\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5942 - val_loss: 0.0356\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6208 - val_loss: 0.0368\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7122 - val_loss: 0.0357\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 139ms/step - loss: 0.7677 - val_loss: 0.0361\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8097 - val_loss: 0.0368\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8764 - val_loss: 0.0357\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8405 - val_loss: 0.0388\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8030 - val_loss: 0.0357\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7181 - val_loss: 0.0388\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5800 - val_loss: 0.0356\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6235 - val_loss: 0.0377\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7850 - val_loss: 0.0369\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8009 - val_loss: 0.0361\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8568 - val_loss: 0.0397\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6538 - val_loss: 0.0357\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5384 - val_loss: 0.0420\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED1820700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 499ms/step - loss: 2.6840 - val_loss: 0.0346\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 16.8347 - val_loss: 0.0350\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 21.8155 - val_loss: 0.0348\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.3293 - val_loss: 0.0353\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.1626 - val_loss: 0.0351\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.1673 - val_loss: 0.0359\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3475 - val_loss: 0.0358\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8654 - val_loss: 0.0362\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1663 - val_loss: 0.0360\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2627 - val_loss: 0.0361\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1996 - val_loss: 0.0365\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1266 - val_loss: 0.0361\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9082 - val_loss: 0.0372\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5222 - val_loss: 0.0367\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3677 - val_loss: 0.0384\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2875 - val_loss: 0.0373\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED18201F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_19 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 1.2666 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.8739 - val_loss: 0.0338\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 22.6466 - val_loss: 0.0335\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.1568 - val_loss: 0.0332\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.3442 - val_loss: 0.0335\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.8350 - val_loss: 0.0335\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3058 - val_loss: 0.0336\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7402 - val_loss: 0.0336\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.7621 - val_loss: 0.0336\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.7067 - val_loss: 0.0336\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.9372 - val_loss: 0.0334\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9162 - val_loss: 0.0335\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1261 - val_loss: 0.0335\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7137 - val_loss: 0.0334\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0967 - val_loss: 0.0335\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6701 - val_loss: 0.0334\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3753 - val_loss: 0.0337\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2506 - val_loss: 0.0334\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2429 - val_loss: 0.0337\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED538C3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 392ms/step - loss: 1.5158 - val_loss: 0.0337\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0889 - val_loss: 0.0344\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 19.7882 - val_loss: 0.0340\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.5780 - val_loss: 0.0352\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.2108 - val_loss: 0.0342\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.7705 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.5368 - val_loss: 0.0348\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.5999 - val_loss: 0.0354\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6643 - val_loss: 0.0355\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2261 - val_loss: 0.0363\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4542 - val_loss: 0.0368\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1450 - val_loss: 0.0364\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0601 - val_loss: 0.0384\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6639 - val_loss: 0.0366\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4608 - val_loss: 0.0395\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3044 - val_loss: 0.0370\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED005DCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.9994 - val_loss: 0.0346\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.6277 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 24.4951 - val_loss: 0.0350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.9502 - val_loss: 0.0347\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.9391 - val_loss: 0.0356\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.2101 - val_loss: 0.0345\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.6116 - val_loss: 0.0349\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.8098 - val_loss: 0.0348\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2072 - val_loss: 0.0347\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9286 - val_loss: 0.0347\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3796 - val_loss: 0.0342\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0238 - val_loss: 0.0346\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6400 - val_loss: 0.0340\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3314 - val_loss: 0.0344\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2338 - val_loss: 0.0338\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1788 - val_loss: 0.0342\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1600 - val_loss: 0.0338\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1897 - val_loss: 0.0341\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2481 - val_loss: 0.0337\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3697 - val_loss: 0.0341\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5180 - val_loss: 0.0338\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7327 - val_loss: 0.0346\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9956 - val_loss: 0.0344\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0711 - val_loss: 0.0350\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7928 - val_loss: 0.0348\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5747 - val_loss: 0.0345\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4270 - val_loss: 0.0346\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3472 - val_loss: 0.0342\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3662 - val_loss: 0.0343\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4440 - val_loss: 0.0339\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5041 - val_loss: 0.0340\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4813 - val_loss: 0.0340\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4561 - val_loss: 0.0337\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4371 - val_loss: 0.0351\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4352 - val_loss: 0.0342\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6027 - val_loss: 0.0377\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9175 - val_loss: 0.0368\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2560 - val_loss: 0.0402\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7256 - val_loss: 0.0359\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6716 - val_loss: 0.0393\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED53EB9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_22 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 253ms/step - loss: 1.6558 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.7677 - val_loss: 0.0355\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 29.8901 - val_loss: 0.0348\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.4380 - val_loss: 0.0348\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.3994 - val_loss: 0.0354\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.8651 - val_loss: 0.0352\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.4638 - val_loss: 0.0362\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.6740 - val_loss: 0.0362\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4235 - val_loss: 0.0378\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.7081 - val_loss: 0.0373\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2019 - val_loss: 0.0393\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8671 - val_loss: 0.0380\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5162 - val_loss: 0.0410\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3551 - val_loss: 0.0394\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3026 - val_loss: 0.0425\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2800 - val_loss: 0.0407\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED830C670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 1.9148 - val_loss: 0.0400\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.5311 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 17.5984 - val_loss: 0.0340\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.7908 - val_loss: 0.0338\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.5347 - val_loss: 0.0338\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.3466 - val_loss: 0.0338\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.4510 - val_loss: 0.0336\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.2709 - val_loss: 0.0336\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1672 - val_loss: 0.0335\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6667 - val_loss: 0.0335\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1312 - val_loss: 0.0334\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0596 - val_loss: 0.0337\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7909 - val_loss: 0.0336\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5540 - val_loss: 0.0341\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3106 - val_loss: 0.0337\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1925 - val_loss: 0.0339\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1285 - val_loss: 0.0336\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1030 - val_loss: 0.0336\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1023 - val_loss: 0.0335\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1260 - val_loss: 0.0336\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1871 - val_loss: 0.0335\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3228 - val_loss: 0.0334\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5758 - val_loss: 0.0335\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9480 - val_loss: 0.0334\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1705 - val_loss: 0.0335\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2201 - val_loss: 0.0340\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED4DEFA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.9575 - val_loss: 0.0389\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 22.8467 - val_loss: 0.0342\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.7154 - val_loss: 0.0343\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.2761 - val_loss: 0.0342\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.3637 - val_loss: 0.0342\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6607 - val_loss: 0.0345\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.1407 - val_loss: 0.0342\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.5086 - val_loss: 0.0346\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.5775 - val_loss: 0.0348\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8194 - val_loss: 0.0347\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3340 - val_loss: 0.0357\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7468 - val_loss: 0.0354\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1423 - val_loss: 0.0366\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6228 - val_loss: 0.0364\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2697 - val_loss: 0.0381\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1333 - val_loss: 0.0373\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.0996 - val_loss: 0.0400\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBD646EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_25 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 2.6859 - val_loss: 0.0356\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 28.2939 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 18.5952 - val_loss: 0.0343\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.1235 - val_loss: 0.0346\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0195 - val_loss: 0.0350\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6501 - val_loss: 0.0357\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.3883 - val_loss: 0.0351\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5239 - val_loss: 0.0363\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1931 - val_loss: 0.0356\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5501 - val_loss: 0.0369\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7177 - val_loss: 0.0371\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1420 - val_loss: 0.0368\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2549 - val_loss: 0.0380\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8767 - val_loss: 0.0375\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5978 - val_loss: 0.0385\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3630 - val_loss: 0.0387\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2258 - val_loss: 0.0386\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAF28A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_26 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 1.6509 - val_loss: 0.0342\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.9944 - val_loss: 0.0341\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.2021 - val_loss: 0.0340\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.5572 - val_loss: 0.0346\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.7651 - val_loss: 0.0345\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.8385 - val_loss: 0.0352\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3738 - val_loss: 0.0352\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.6013 - val_loss: 0.0362\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.3343 - val_loss: 0.0356\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2559 - val_loss: 0.0353\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8730 - val_loss: 0.0358\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5969 - val_loss: 0.0350\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4690 - val_loss: 0.0366\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4597 - val_loss: 0.0348\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4437 - val_loss: 0.0375\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4159 - val_loss: 0.0351\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3720 - val_loss: 0.0380\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3225 - val_loss: 0.0355\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC01904C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_27 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 274ms/step - loss: 1.9487 - val_loss: 0.0343\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.4140 - val_loss: 0.0342\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 17.3787 - val_loss: 0.0343\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 15.3745 - val_loss: 0.0340\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.0396 - val_loss: 0.0339\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 5.9203 - val_loss: 0.0337\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.6177 - val_loss: 0.0339\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.1277 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1642 - val_loss: 0.0338\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.0165 - val_loss: 0.0338\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.7431 - val_loss: 0.0339\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1061 - val_loss: 0.0337\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6609 - val_loss: 0.0340\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3903 - val_loss: 0.0339\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3078 - val_loss: 0.0341\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3120 - val_loss: 0.0339\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3812 - val_loss: 0.0339\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4826 - val_loss: 0.0342\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6286 - val_loss: 0.0338\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7229 - val_loss: 0.0345\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7010 - val_loss: 0.0337\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6259 - val_loss: 0.0347\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5966 - val_loss: 0.0336\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5996 - val_loss: 0.0363\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6483 - val_loss: 0.0339\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7358 - val_loss: 0.0378\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8639 - val_loss: 0.0340\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8540 - val_loss: 0.0379\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7643 - val_loss: 0.0339\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6830 - val_loss: 0.0364\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5744 - val_loss: 0.0337\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4331 - val_loss: 0.0347\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3494 - val_loss: 0.0339\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3227 - val_loss: 0.0342\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3861 - val_loss: 0.0338\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4894 - val_loss: 0.0339\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6320 - val_loss: 0.0339\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7749 - val_loss: 0.0334\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.1274 - val_loss: 0.0339\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.7927 - val_loss: 0.0335\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAA395E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_28 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.6539 - val_loss: 0.0346\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.7574 - val_loss: 0.0343\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 13.5809 - val_loss: 0.0350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.3419 - val_loss: 0.0343\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.1440 - val_loss: 0.0357\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6.6830 - val_loss: 0.0341\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.4882 - val_loss: 0.0350\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.1396 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.5728 - val_loss: 0.0345\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5260 - val_loss: 0.0339\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0334 - val_loss: 0.0341\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.8495 - val_loss: 0.0342\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.3871 - val_loss: 0.0337\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8981 - val_loss: 0.0343\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5116 - val_loss: 0.0337\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3109 - val_loss: 0.0342\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2207 - val_loss: 0.0337\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2127 - val_loss: 0.0342\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2429 - val_loss: 0.0337\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2922 - val_loss: 0.0342\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3483 - val_loss: 0.0338\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4425 - val_loss: 0.0343\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5777 - val_loss: 0.0339\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6674 - val_loss: 0.0342\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7622 - val_loss: 0.0339\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6664 - val_loss: 0.0338\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6252 - val_loss: 0.0340\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6053 - val_loss: 0.0340\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6286 - val_loss: 0.0350\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6940 - val_loss: 0.0351\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED52DB670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_29 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.7357 - val_loss: 0.0355\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.5595 - val_loss: 0.0347\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.8404 - val_loss: 0.0340\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.7413 - val_loss: 0.0342\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.0233 - val_loss: 0.0346\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.7808 - val_loss: 0.0348\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.2402 - val_loss: 0.0344\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3551 - val_loss: 0.0345\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4342 - val_loss: 0.0342\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4973 - val_loss: 0.0347\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6481 - val_loss: 0.0342\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4592 - val_loss: 0.0344\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4343 - val_loss: 0.0340\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4773 - val_loss: 0.0343\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4848 - val_loss: 0.0340\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3951 - val_loss: 0.0341\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2521 - val_loss: 0.0339\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1679 - val_loss: 0.0340\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1154 - val_loss: 0.0339\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1011 - val_loss: 0.0339\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1136 - val_loss: 0.0340\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1736 - val_loss: 0.0339\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3385 - val_loss: 0.0343\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6851 - val_loss: 0.0346\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0773 - val_loss: 0.0358\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1616 - val_loss: 0.0363\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0560 - val_loss: 0.0367\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8511 - val_loss: 0.0368\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6960 - val_loss: 0.0360\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6911 - val_loss: 0.0358\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6097 - val_loss: 0.0342\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7959 - val_loss: 0.0343\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0402 - val_loss: 0.0339\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3539 - val_loss: 0.0339\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4555 - val_loss: 0.0340\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.1111 - val_loss: 0.0345\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7105 - val_loss: 0.0340\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7129 - val_loss: 0.0376\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7188 - val_loss: 0.0349\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6439 - val_loss: 0.0374\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBD646940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_30 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 3.6534 - val_loss: 0.0349\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 20.2450 - val_loss: 0.0338\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.9300 - val_loss: 0.0338\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.9638 - val_loss: 0.0349\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.7537 - val_loss: 0.0349\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3227 - val_loss: 0.0363\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1518 - val_loss: 0.0351\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4735 - val_loss: 0.0370\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4487 - val_loss: 0.0354\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1799 - val_loss: 0.0380\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.9020 - val_loss: 0.0360\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5973 - val_loss: 0.0380\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4390 - val_loss: 0.0370\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3612 - val_loss: 0.0378\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3186 - val_loss: 0.0386\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3980 - val_loss: 0.0374\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5710 - val_loss: 0.0407\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9973 - val_loss: 0.0375\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB97E0E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_31 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.5922 - val_loss: 0.0344\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.0882 - val_loss: 0.0336\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 17.1825 - val_loss: 0.0339\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.6857 - val_loss: 0.0339\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.2265 - val_loss: 0.0343\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.4182 - val_loss: 0.0340\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2816 - val_loss: 0.0341\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2210 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1662 - val_loss: 0.0346\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5574 - val_loss: 0.0337\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.2162 - val_loss: 0.0344\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6530 - val_loss: 0.0336\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3038 - val_loss: 0.0341\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1724 - val_loss: 0.0337\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1529 - val_loss: 0.0341\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1835 - val_loss: 0.0339\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2445 - val_loss: 0.0341\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3534 - val_loss: 0.0340\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4967 - val_loss: 0.0341\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6547 - val_loss: 0.0344\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7612 - val_loss: 0.0343\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7805 - val_loss: 0.0352\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8429 - val_loss: 0.0344\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7710 - val_loss: 0.0347\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5981 - val_loss: 0.0338\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4303 - val_loss: 0.0345\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2736 - val_loss: 0.0338\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE59DDDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.9507 - val_loss: 0.0340\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.9036 - val_loss: 0.0346\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.2870 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10.7564 - val_loss: 0.0365\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.1132 - val_loss: 0.0353\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.5864 - val_loss: 0.0390\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.3657 - val_loss: 0.0383\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.2468 - val_loss: 0.0404\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.9625 - val_loss: 0.0415\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3129 - val_loss: 0.0412\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0959 - val_loss: 0.0451\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0374 - val_loss: 0.0422\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7676 - val_loss: 0.0471\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6508 - val_loss: 0.0434\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5434 - val_loss: 0.0475\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4298 - val_loss: 0.0455\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC37EA670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_33 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 2.3374 - val_loss: 0.0352\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 17.7564 - val_loss: 0.0364\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 18.4042 - val_loss: 0.0355\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.3939 - val_loss: 0.0362\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.8300 - val_loss: 0.0377\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.7068 - val_loss: 0.0375\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.9287 - val_loss: 0.0381\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6686 - val_loss: 0.0376\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.3744 - val_loss: 0.0381\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.6234 - val_loss: 0.0381\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0974 - val_loss: 0.0383\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6657 - val_loss: 0.0404\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4088 - val_loss: 0.0391\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2847 - val_loss: 0.0424\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2554 - val_loss: 0.0401\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2718 - val_loss: 0.0450\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC51FB3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_34 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.9287 - val_loss: 0.0335\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.8138 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 24.2905 - val_loss: 0.0347\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.8616 - val_loss: 0.0345\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.5130 - val_loss: 0.0344\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.8872 - val_loss: 0.0342\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2661 - val_loss: 0.0341\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9408 - val_loss: 0.0341\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1972 - val_loss: 0.0338\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7845 - val_loss: 0.0340\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7517 - val_loss: 0.0337\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6933 - val_loss: 0.0337\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7742 - val_loss: 0.0337\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6729 - val_loss: 0.0338\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4899 - val_loss: 0.0338\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3493 - val_loss: 0.0338\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC37EAD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_35 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.3571 - val_loss: 0.0339\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 16.0128 - val_loss: 0.0361\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 18.5582 - val_loss: 0.0358\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.4325 - val_loss: 0.0357\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8.3090 - val_loss: 0.0385\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.8898 - val_loss: 0.0361\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3491 - val_loss: 0.0404\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5330 - val_loss: 0.0393\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0142 - val_loss: 0.0430\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7773 - val_loss: 0.0427\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8731 - val_loss: 0.0449\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9643 - val_loss: 0.0454\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7440 - val_loss: 0.0473\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5065 - val_loss: 0.0477\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4598 - val_loss: 0.0512\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5161 - val_loss: 0.0485\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAB3F310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 1.2923 - val_loss: 0.0347\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.3847 - val_loss: 0.0319\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.5678 - val_loss: 0.0356\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.4811 - val_loss: 0.0349\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.4880 - val_loss: 0.0364\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.5092 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0235 - val_loss: 0.0369\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5342 - val_loss: 0.0359\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7482 - val_loss: 0.0384\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.9694 - val_loss: 0.0365\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7095 - val_loss: 0.0388\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5845 - val_loss: 0.0362\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5137 - val_loss: 0.0380\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4117 - val_loss: 0.0366\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3241 - val_loss: 0.0375\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3015 - val_loss: 0.0369\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3410 - val_loss: 0.0372\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC51FBF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_37 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.3701 - val_loss: 0.0327\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 11.3787 - val_loss: 0.0346\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 17.6943 - val_loss: 0.0366\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.6790 - val_loss: 0.0377\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.4426 - val_loss: 0.0386\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.5067 - val_loss: 0.0386\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.2281 - val_loss: 0.0390\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.1865 - val_loss: 0.0398\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0655 - val_loss: 0.0409\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0153 - val_loss: 0.0413\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8414 - val_loss: 0.0428\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8837 - val_loss: 0.0435\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8534 - val_loss: 0.0453\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7497 - val_loss: 0.0433\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5663 - val_loss: 0.0490\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4648 - val_loss: 0.0435\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED1189F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_38 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 1.6480 - val_loss: 0.0328\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 9.7635 - val_loss: 0.0329\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.3896 - val_loss: 0.0333\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 16.0215 - val_loss: 0.0339\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.6061 - val_loss: 0.0337\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.5611 - val_loss: 0.0354\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.7561 - val_loss: 0.0334\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.3911 - val_loss: 0.0339\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2502 - val_loss: 0.0329\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6507 - val_loss: 0.0329\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5309 - val_loss: 0.0326\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4237 - val_loss: 0.0324\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9861 - val_loss: 0.0322\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5837 - val_loss: 0.0323\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2703 - val_loss: 0.0322\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1422 - val_loss: 0.0322\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1166 - val_loss: 0.0327\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1520 - val_loss: 0.0322\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2502 - val_loss: 0.0338\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4421 - val_loss: 0.0321\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7905 - val_loss: 0.0358\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0381 - val_loss: 0.0322\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0093 - val_loss: 0.0367\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7374 - val_loss: 0.0330\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4882 - val_loss: 0.0370\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3294 - val_loss: 0.0342\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2705 - val_loss: 0.0374\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2743 - val_loss: 0.0358\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3513 - val_loss: 0.0370\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4832 - val_loss: 0.0395\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6642 - val_loss: 0.0376\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7652 - val_loss: 0.0411\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8871 - val_loss: 0.0389\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0986 - val_loss: 0.0408\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3860 - val_loss: 0.0438\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EF0D19280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_39 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 1.5046 - val_loss: 0.0325\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.0439 - val_loss: 0.0341\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 12.4868 - val_loss: 0.0350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.4746 - val_loss: 0.0345\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.8597 - val_loss: 0.0355\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.9117 - val_loss: 0.0342\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.1217 - val_loss: 0.0354\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.7116 - val_loss: 0.0344\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.6597 - val_loss: 0.0338\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5259 - val_loss: 0.0332\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.2241 - val_loss: 0.0333\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5577 - val_loss: 0.0331\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8168 - val_loss: 0.0327\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5574 - val_loss: 0.0329\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3672 - val_loss: 0.0327\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3238 - val_loss: 0.0330\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFCF1040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_40 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 2.0749 - val_loss: 0.0327\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 15.5958 - val_loss: 0.0347\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 17.9611 - val_loss: 0.0364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 12.3014 - val_loss: 0.0383\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.5916 - val_loss: 0.0388\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.2738 - val_loss: 0.0397\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0932 - val_loss: 0.0385\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7544 - val_loss: 0.0390\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8489 - val_loss: 0.0381\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5444 - val_loss: 0.0393\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5966 - val_loss: 0.0376\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5598 - val_loss: 0.0392\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5240 - val_loss: 0.0381\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5695 - val_loss: 0.0390\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4865 - val_loss: 0.0388\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4828 - val_loss: 0.0379\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEFF7E670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 1.5552 - val_loss: 0.0330\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.7478 - val_loss: 0.0336\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 18.9281 - val_loss: 0.0395\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.8547 - val_loss: 0.0395\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.1468 - val_loss: 0.0404\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.9741 - val_loss: 0.0433\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.4614 - val_loss: 0.0411\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0719 - val_loss: 0.0452\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8710 - val_loss: 0.0425\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1240 - val_loss: 0.0478\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5574 - val_loss: 0.0445\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3944 - val_loss: 0.0494\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0286 - val_loss: 0.0475\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5959 - val_loss: 0.0506\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2981 - val_loss: 0.0506\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1658 - val_loss: 0.0530\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED52DB550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_42 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 2.7436 - val_loss: 0.0337\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10.8196 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 25.3680 - val_loss: 0.0342\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.5081 - val_loss: 0.0356\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.4686 - val_loss: 0.0348\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.4996 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5130 - val_loss: 0.0342\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.1388 - val_loss: 0.0340\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.3558 - val_loss: 0.0335\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2271 - val_loss: 0.0330\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.7073 - val_loss: 0.0329\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6497 - val_loss: 0.0327\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6219 - val_loss: 0.0329\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5661 - val_loss: 0.0327\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.549 - 0s 23ms/step - loss: 0.5490 - val_loss: 0.0327\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5649 - val_loss: 0.0326\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6034 - val_loss: 0.0328\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5737 - val_loss: 0.0327\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4759 - val_loss: 0.0332\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3877 - val_loss: 0.0327\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3095 - val_loss: 0.0333\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2733 - val_loss: 0.0328\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2867 - val_loss: 0.0333\n",
      "Epoch 24/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3680 - val_loss: 0.0331\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5692 - val_loss: 0.0329\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7081 - val_loss: 0.0342\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8078 - val_loss: 0.0326\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8466 - val_loss: 0.0365\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9861 - val_loss: 0.0330\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0856 - val_loss: 0.0386\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.0576 - val_loss: 0.0332\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9867 - val_loss: 0.0373\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8094 - val_loss: 0.0326\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8600 - val_loss: 0.0337\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8664 - val_loss: 0.0350\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0911 - val_loss: 0.0336\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4864 - val_loss: 0.0418\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5567 - val_loss: 0.0326\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3839 - val_loss: 0.0373\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4991 - val_loss: 0.0329\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE69A74C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_43 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.4859 - val_loss: 0.0368\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.7338 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 19.3592 - val_loss: 0.0372\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 12.1323 - val_loss: 0.0378\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.4659 - val_loss: 0.0372\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 7.5494 - val_loss: 0.0369\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.1377 - val_loss: 0.0366\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4053 - val_loss: 0.0364\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.4392 - val_loss: 0.0375\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.4506 - val_loss: 0.0365\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4333 - val_loss: 0.0381\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8829 - val_loss: 0.0384\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2547 - val_loss: 0.0386\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8355 - val_loss: 0.0398\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5457 - val_loss: 0.0390\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2493 - val_loss: 0.0400\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1573 - val_loss: 0.0394\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB881B4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_44 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 3.7032 - val_loss: 0.0328\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.0838 - val_loss: 0.0329\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.6603 - val_loss: 0.0364\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.3499 - val_loss: 0.0353\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.3962 - val_loss: 0.0363\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.0199 - val_loss: 0.0369\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.6196 - val_loss: 0.0382\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.7038 - val_loss: 0.0365\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8923 - val_loss: 0.0381\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7073 - val_loss: 0.0355\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2336 - val_loss: 0.0383\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8477 - val_loss: 0.0354\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7079 - val_loss: 0.0388\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8096 - val_loss: 0.0345\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0594 - val_loss: 0.0395\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2081 - val_loss: 0.0341\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE6429A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 1.4767 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.1641 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 16.1831 - val_loss: 0.0392\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.8383 - val_loss: 0.0370\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.2241 - val_loss: 0.0378\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.7868 - val_loss: 0.0354\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.0202 - val_loss: 0.0358\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1662 - val_loss: 0.0343\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.7534 - val_loss: 0.0348\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.3308 - val_loss: 0.0326\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.8119 - val_loss: 0.0341\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4131 - val_loss: 0.0322\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1106 - val_loss: 0.0338\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8269 - val_loss: 0.0323\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7050 - val_loss: 0.0332\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5225 - val_loss: 0.0323\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4072 - val_loss: 0.0326\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3309 - val_loss: 0.0324\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2877 - val_loss: 0.0324\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2715 - val_loss: 0.0324\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3151 - val_loss: 0.0324\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4053 - val_loss: 0.0323\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5370 - val_loss: 0.0327\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7344 - val_loss: 0.0323\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8407 - val_loss: 0.0329\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7436 - val_loss: 0.0324\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8187 - val_loss: 0.0329\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE54F41F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_46 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.9102 - val_loss: 0.0324\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.3641 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.5296 - val_loss: 0.0371\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 11.5560 - val_loss: 0.0368\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.6722 - val_loss: 0.0378\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.7678 - val_loss: 0.0370\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.0282 - val_loss: 0.0384\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.9740 - val_loss: 0.0372\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.8321 - val_loss: 0.0382\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9129 - val_loss: 0.0384\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5347 - val_loss: 0.0373\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5033 - val_loss: 0.0388\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5254 - val_loss: 0.0371\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5594 - val_loss: 0.0379\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6032 - val_loss: 0.0380\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6803 - val_loss: 0.0366\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE2318700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_47 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 1.8704 - val_loss: 0.0343\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.1622 - val_loss: 0.0348\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 17.5459 - val_loss: 0.0373\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 12.4560 - val_loss: 0.0350\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.8048 - val_loss: 0.0377\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.9440 - val_loss: 0.0377\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.1052 - val_loss: 0.0405\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4752 - val_loss: 0.0389\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1091 - val_loss: 0.0416\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5642 - val_loss: 0.0398\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3206 - val_loss: 0.0422\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2435 - val_loss: 0.0405\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2670 - val_loss: 0.0429\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3287 - val_loss: 0.0404\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3397 - val_loss: 0.0438\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3157 - val_loss: 0.0404\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC5A43700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 2.1955 - val_loss: 0.0334\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 16.9234 - val_loss: 0.0342\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17.4978 - val_loss: 0.0336\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.0733 - val_loss: 0.0334\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.9070 - val_loss: 0.0337\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.9442 - val_loss: 0.0329\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.6548 - val_loss: 0.0330\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.0499 - val_loss: 0.0328\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6783 - val_loss: 0.0327\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1148 - val_loss: 0.0331\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7766 - val_loss: 0.0326\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4946 - val_loss: 0.0330\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3043 - val_loss: 0.0326\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2241 - val_loss: 0.0328\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1986 - val_loss: 0.0325\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2022 - val_loss: 0.0325\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2101 - val_loss: 0.0326\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2283 - val_loss: 0.0324\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2395 - val_loss: 0.0328\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2663 - val_loss: 0.0327\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3057 - val_loss: 0.0328\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.4577 - val_loss: 0.0334\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7901 - val_loss: 0.0329\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3991 - val_loss: 0.0357\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8416 - val_loss: 0.0329\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.5862 - val_loss: 0.0382\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.3170 - val_loss: 0.0328\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1604 - val_loss: 0.0384\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7948 - val_loss: 0.0330\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5725 - val_loss: 0.0374\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4749 - val_loss: 0.0332\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4239 - val_loss: 0.0372\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4338 - val_loss: 0.0334\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDA61A940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_49 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 1.4164 - val_loss: 0.0333\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.8431 - val_loss: 0.0350\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 24.1950 - val_loss: 0.0347\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.6841 - val_loss: 0.0354\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.7985 - val_loss: 0.0339\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.2266 - val_loss: 0.0362\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.7083 - val_loss: 0.0341\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7267 - val_loss: 0.0352\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6149 - val_loss: 0.0344\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0034 - val_loss: 0.0356\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5419 - val_loss: 0.0348\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4916 - val_loss: 0.0362\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5746 - val_loss: 0.0353\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6006 - val_loss: 0.0372\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.5521 - val_loss: 0.0359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4945 - val_loss: 0.0381\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8114F8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_50 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.5897 - val_loss: 0.0326\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 19.3821 - val_loss: 0.0337\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.7796 - val_loss: 0.0325\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.4723 - val_loss: 0.0345\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3050 - val_loss: 0.0336\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.1944 - val_loss: 0.0339\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0722 - val_loss: 0.0324\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.8344 - val_loss: 0.0335\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.2450 - val_loss: 0.0325\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8436 - val_loss: 0.0329\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.3153 - val_loss: 0.0324\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0578 - val_loss: 0.0327\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6901 - val_loss: 0.0329\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6425 - val_loss: 0.0321\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8757 - val_loss: 0.0332\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9478 - val_loss: 0.0323\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7708 - val_loss: 0.0325\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6192 - val_loss: 0.0330\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6004 - val_loss: 0.0318\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5734 - val_loss: 0.0350\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5886 - val_loss: 0.0321\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5826 - val_loss: 0.0372\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5404 - val_loss: 0.0324\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4848 - val_loss: 0.0365\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4015 - val_loss: 0.0326\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3026 - val_loss: 0.0353\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2401 - val_loss: 0.0322\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2339 - val_loss: 0.0342\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3167 - val_loss: 0.0321\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5378 - val_loss: 0.0330\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9079 - val_loss: 0.0325\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2529 - val_loss: 0.0321\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3899 - val_loss: 0.0336\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2391 - val_loss: 0.0320\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED538CA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.6409 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.3336 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.9335 - val_loss: 0.0315\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.7683 - val_loss: 0.0317\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.3716 - val_loss: 0.0312\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.4798 - val_loss: 0.0316\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.2920 - val_loss: 0.0312\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4657 - val_loss: 0.0317\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.9059 - val_loss: 0.0314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.7132 - val_loss: 0.0317\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2991 - val_loss: 0.0318\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6941 - val_loss: 0.0315\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5425 - val_loss: 0.0322\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5381 - val_loss: 0.0318\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4837 - val_loss: 0.0326\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4473 - val_loss: 0.0319\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3847 - val_loss: 0.0330\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3679 - val_loss: 0.0318\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4468 - val_loss: 0.0337\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6107 - val_loss: 0.0316\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE0599CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_52 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.8504 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.3655 - val_loss: 0.0330\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 15.4784 - val_loss: 0.0321\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.2035 - val_loss: 0.0316\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.5635 - val_loss: 0.0320\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.0488 - val_loss: 0.0314\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.6911 - val_loss: 0.0323\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.8975 - val_loss: 0.0325\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.4070 - val_loss: 0.0318\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7342 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.4024 - val_loss: 0.0320\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8914 - val_loss: 0.0317\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5778 - val_loss: 0.0319\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3588 - val_loss: 0.0318\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.2289 - val_loss: 0.0320\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2446 - val_loss: 0.0322\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3604 - val_loss: 0.0319\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4955 - val_loss: 0.0327\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5801 - val_loss: 0.0318\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6241 - val_loss: 0.0328\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4947 - val_loss: 0.0316\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDB602820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 2.2101 - val_loss: 0.0307\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.0208 - val_loss: 0.0314\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 23.1860 - val_loss: 0.0305\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.0030 - val_loss: 0.0309\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.3114 - val_loss: 0.0306\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.9400 - val_loss: 0.0307\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.5028 - val_loss: 0.0307\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.6798 - val_loss: 0.0306\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6767 - val_loss: 0.0308\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0720 - val_loss: 0.0308\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5400 - val_loss: 0.0310\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4348 - val_loss: 0.0316\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4518 - val_loss: 0.0310\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 0.5162 - val_loss: 0.0331\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6212 - val_loss: 0.0310\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7352 - val_loss: 0.0352\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7632 - val_loss: 0.0311\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6175 - val_loss: 0.0359\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E841BAAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_54 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 1.4659 - val_loss: 0.0328\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.7272 - val_loss: 0.0309\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 19.1494 - val_loss: 0.0305\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.0754 - val_loss: 0.0305\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.2482 - val_loss: 0.0303\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.0400 - val_loss: 0.0308\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.5268 - val_loss: 0.0304\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6439 - val_loss: 0.0306\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1685 - val_loss: 0.0304\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5241 - val_loss: 0.0303\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0887 - val_loss: 0.0305\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0225 - val_loss: 0.0303\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6732 - val_loss: 0.0307\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4314 - val_loss: 0.0304\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3001 - val_loss: 0.0309\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2599 - val_loss: 0.0304\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2690 - val_loss: 0.0311\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3184 - val_loss: 0.0307\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3791 - val_loss: 0.0312\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4155 - val_loss: 0.0312\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4287 - val_loss: 0.0313\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4119 - val_loss: 0.0319\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3850 - val_loss: 0.0313\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3682 - val_loss: 0.0326\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3755 - val_loss: 0.0316\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4204 - val_loss: 0.0333\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5080 - val_loss: 0.0324\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFCF1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_55 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.6820 - val_loss: 0.0303\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.9615 - val_loss: 0.0300\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 20.5142 - val_loss: 0.0296\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.4558 - val_loss: 0.0293\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.3641 - val_loss: 0.0297\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.1271 - val_loss: 0.0298\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.1478 - val_loss: 0.0299\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1.9286 - val_loss: 0.0297\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9821 - val_loss: 0.0298\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5871 - val_loss: 0.0298\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8111 - val_loss: 0.0296\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5398 - val_loss: 0.0303\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.5339 - val_loss: 0.0295\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9359 - val_loss: 0.0301\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5691 - val_loss: 0.0297\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3832 - val_loss: 0.0301\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2837 - val_loss: 0.0298\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3230 - val_loss: 0.0303\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4474 - val_loss: 0.0299\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAA39E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 251ms/step - loss: 2.0054 - val_loss: 0.0278\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.1249 - val_loss: 0.0286\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.1809 - val_loss: 0.0280\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.7716 - val_loss: 0.0282\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.3422 - val_loss: 0.0280\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.9171 - val_loss: 0.0278\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.6494 - val_loss: 0.0283\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1170 - val_loss: 0.0281\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.3159 - val_loss: 0.0282\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7142 - val_loss: 0.0283\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.2396 - val_loss: 0.0282\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3222 - val_loss: 0.0283\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.4025 - val_loss: 0.0285\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9227 - val_loss: 0.0282\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.6359 - val_loss: 0.0289\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5616 - val_loss: 0.0282\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECB03A700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 2.2807 - val_loss: 0.0253\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 14.0210 - val_loss: 0.0266\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.7410 - val_loss: 0.0271\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.5778 - val_loss: 0.0264\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 9.216 - 0s 28ms/step - loss: 9.2167 - val_loss: 0.0265\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.0519 - val_loss: 0.0266\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8949 - val_loss: 0.0266\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.8549 - val_loss: 0.0264\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.2935 - val_loss: 0.0259\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8715 - val_loss: 0.0257\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9949 - val_loss: 0.0256\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8929 - val_loss: 0.0255\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4210 - val_loss: 0.0256\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2410 - val_loss: 0.0256\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2454 - val_loss: 0.0260\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3593 - val_loss: 0.0258\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFCF1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_58 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 2.2032 - val_loss: 0.0208\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.4607 - val_loss: 0.0230\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.0055 - val_loss: 0.0215\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.4385 - val_loss: 0.0227\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.6340 - val_loss: 0.0228\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.9204 - val_loss: 0.0245\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.9216 - val_loss: 0.0236\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3610 - val_loss: 0.0249\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.0843 - val_loss: 0.0237\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1752 - val_loss: 0.0256\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6985 - val_loss: 0.0235\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5733 - val_loss: 0.0251\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5272 - val_loss: 0.0233\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4552 - val_loss: 0.0248\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3931 - val_loss: 0.0232\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2892 - val_loss: 0.0246\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEA982EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_59 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.8308 - val_loss: 0.0223\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.5612 - val_loss: 0.0217\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 16.8566 - val_loss: 0.0229\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.0722 - val_loss: 0.0217\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.3850 - val_loss: 0.0213\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.0134 - val_loss: 0.0227\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.0482 - val_loss: 0.0232\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.2542 - val_loss: 0.0229\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3140 - val_loss: 0.0241\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.0202 - val_loss: 0.0229\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.8869 - val_loss: 0.0273\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8863 - val_loss: 0.0230\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4726 - val_loss: 0.0290\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1885 - val_loss: 0.0236\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0120 - val_loss: 0.0294\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9381 - val_loss: 0.0243\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6213 - val_loss: 0.0288\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5309 - val_loss: 0.0258\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4536 - val_loss: 0.0288\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4251 - val_loss: 0.0279\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF4520D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_60 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.4848 - val_loss: 0.0244\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.4303 - val_loss: 0.0191\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.8860 - val_loss: 0.0179\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.7583 - val_loss: 0.0180\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8883 - val_loss: 0.0172\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.2689 - val_loss: 0.0173\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.6072 - val_loss: 0.0168\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.9035 - val_loss: 0.0171\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2522 - val_loss: 0.0166\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4846 - val_loss: 0.0166\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8960 - val_loss: 0.0164\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5298 - val_loss: 0.0165\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4199 - val_loss: 0.0164\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4738 - val_loss: 0.0166\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5146 - val_loss: 0.0166\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4433 - val_loss: 0.0171\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4495 - val_loss: 0.0167\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5466 - val_loss: 0.0185\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6391 - val_loss: 0.0166\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7109 - val_loss: 0.0207\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6869 - val_loss: 0.0167\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6259 - val_loss: 0.0226\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5624 - val_loss: 0.0173\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4858 - val_loss: 0.0223\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4573 - val_loss: 0.0183\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4601 - val_loss: 0.0224\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5334 - val_loss: 0.0185\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6098 - val_loss: 0.0242\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB952E700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_61 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 257ms/step - loss: 1.5884 - val_loss: 0.0240\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.3706 - val_loss: 0.0163\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17.6419 - val_loss: 0.0175\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.6535 - val_loss: 0.0191\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.7377 - val_loss: 0.0167\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.2584 - val_loss: 0.0207\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.6281 - val_loss: 0.0173\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.2093 - val_loss: 0.0202\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 3.8241 - val_loss: 0.0193\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.0724 - val_loss: 0.0187\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9595 - val_loss: 0.0203\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8475 - val_loss: 0.0178\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9848 - val_loss: 0.0202\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4292 - val_loss: 0.0190\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1793 - val_loss: 0.0203\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0984 - val_loss: 0.0197\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.0704 - val_loss: 0.0207\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED48C1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_62 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 1.5965 - val_loss: 0.0165\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.5875 - val_loss: 0.0194\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.5025 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.3091 - val_loss: 0.0195\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.3093 - val_loss: 0.0208\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.4362 - val_loss: 0.0187\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7379 - val_loss: 0.0216\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7120 - val_loss: 0.0188\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9884 - val_loss: 0.0206\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5818 - val_loss: 0.0191\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2212 - val_loss: 0.0188\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6984 - val_loss: 0.0203\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7144 - val_loss: 0.0168\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8238 - val_loss: 0.0204\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8044 - val_loss: 0.0163\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6250 - val_loss: 0.0193\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6764 - val_loss: 0.0158\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7806 - val_loss: 0.0187\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7841 - val_loss: 0.0156\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7272 - val_loss: 0.0179\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5744 - val_loss: 0.0156\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3780 - val_loss: 0.0159\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2441 - val_loss: 0.0158\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1699 - val_loss: 0.0144\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1481 - val_loss: 0.0170\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1419 - val_loss: 0.0133\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1550 - val_loss: 0.0178\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1765 - val_loss: 0.0132\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2215 - val_loss: 0.0175\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2787 - val_loss: 0.0131\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3684 - val_loss: 0.0186\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4488 - val_loss: 0.0134\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5706 - val_loss: 0.0214\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6577 - val_loss: 0.0144\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8126 - val_loss: 0.0232\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0436 - val_loss: 0.0170\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.5802 - val_loss: 0.0333\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0315 - val_loss: 0.0135\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3108 - val_loss: 0.0193\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.0304 - val_loss: 0.0147\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8D5920D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_63 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 251ms/step - loss: 2.0090 - val_loss: 0.0245\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 17.8843 - val_loss: 0.0204\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 16.1995 - val_loss: 0.0175\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.8157 - val_loss: 0.0192\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.9266 - val_loss: 0.0180\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.2058 - val_loss: 0.0206\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.2603 - val_loss: 0.0200\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.6065 - val_loss: 0.0224\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.3272 - val_loss: 0.0203\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2496 - val_loss: 0.0215\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0916 - val_loss: 0.0217\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7092 - val_loss: 0.0209\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4850 - val_loss: 0.0227\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3122 - val_loss: 0.0218\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2092 - val_loss: 0.0232\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1683 - val_loss: 0.0235\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2099 - val_loss: 0.0224\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4197 - val_loss: 0.0266\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDB402790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_64 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.1223 - val_loss: 0.0271\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 21.3847 - val_loss: 0.0188\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 11.7579 - val_loss: 0.0194\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.5342 - val_loss: 0.0176\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.6515 - val_loss: 0.0165\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.9124 - val_loss: 0.0171\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.4776 - val_loss: 0.0161\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9622 - val_loss: 0.0179\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6858 - val_loss: 0.0165\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4247 - val_loss: 0.0187\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6673 - val_loss: 0.0175\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.4012 - val_loss: 0.0189\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.2374 - val_loss: 0.0177\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9405 - val_loss: 0.0192\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6458 - val_loss: 0.0196\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3606 - val_loss: 0.0197\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2710 - val_loss: 0.0201\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3029 - val_loss: 0.0201\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3970 - val_loss: 0.0210\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4755 - val_loss: 0.0206\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5479 - val_loss: 0.0215\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7059 - val_loss: 0.0206\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8E1FD310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.8007 - val_loss: 0.0170\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.5950 - val_loss: 0.0185\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.9290 - val_loss: 0.0200\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.5189 - val_loss: 0.0200\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.6290 - val_loss: 0.0202\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.0437 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.6200 - val_loss: 0.0199\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.4881 - val_loss: 0.0225\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.5877 - val_loss: 0.0192\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4992 - val_loss: 0.0223\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8698 - val_loss: 0.0203\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4614 - val_loss: 0.0223\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2466 - val_loss: 0.0202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2673 - val_loss: 0.0240\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4686 - val_loss: 0.0185\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7975 - val_loss: 0.0276\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E800944C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_66 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.4814 - val_loss: 0.0217\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.1348 - val_loss: 0.0202\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 20.3710 - val_loss: 0.0206\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.7180 - val_loss: 0.0209\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1936 - val_loss: 0.0170\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.6978 - val_loss: 0.0181\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.8386 - val_loss: 0.0153\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2908 - val_loss: 0.0177\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8058 - val_loss: 0.0161\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.4924 - val_loss: 0.0155\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4039 - val_loss: 0.0187\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2394 - val_loss: 0.0148\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7280 - val_loss: 0.0207\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8073 - val_loss: 0.0144\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3426 - val_loss: 0.0190\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8005 - val_loss: 0.0151\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5687 - val_loss: 0.0188\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3874 - val_loss: 0.0158\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2763 - val_loss: 0.0178\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2493 - val_loss: 0.0166\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2404 - val_loss: 0.0169\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2486 - val_loss: 0.0184\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2836 - val_loss: 0.0153\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3719 - val_loss: 0.0189\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4911 - val_loss: 0.0161\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5867 - val_loss: 0.0180\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6498 - val_loss: 0.0168\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6219 - val_loss: 0.0176\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5503 - val_loss: 0.0160\n",
      "Epoch 00029: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAB3F1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_67 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.5450 - val_loss: 0.0237\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.1876 - val_loss: 0.0182\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 17.8927 - val_loss: 0.0218\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.7153 - val_loss: 0.0226\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.3195 - val_loss: 0.0224\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.0389 - val_loss: 0.0268\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6330 - val_loss: 0.0203\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.6515 - val_loss: 0.0259\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 3.1243 - val_loss: 0.0222\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6572 - val_loss: 0.0227\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.704 - 0s 33ms/step - loss: 2.7043 - val_loss: 0.0242\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.8503 - val_loss: 0.0209\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.4377 - val_loss: 0.0265\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.7105 - val_loss: 0.0214\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2006 - val_loss: 0.0244\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8450 - val_loss: 0.0247\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4101 - val_loss: 0.0255\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED48C1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.0581 - val_loss: 0.0273\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.2493 - val_loss: 0.0208\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 23.0733 - val_loss: 0.0231\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.7903 - val_loss: 0.0206\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.2001 - val_loss: 0.0221\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.7816 - val_loss: 0.0210\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.9825 - val_loss: 0.0223\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.1132 - val_loss: 0.0196\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.4677 - val_loss: 0.0208\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1582 - val_loss: 0.0188\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2084 - val_loss: 0.0194\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7014 - val_loss: 0.0172\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4941 - val_loss: 0.0187\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4784 - val_loss: 0.0161\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5450 - val_loss: 0.0184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6959 - val_loss: 0.0149\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7481 - val_loss: 0.0180\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6497 - val_loss: 0.0140\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5375 - val_loss: 0.0180\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4477 - val_loss: 0.0136\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3889 - val_loss: 0.0181\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4488 - val_loss: 0.0132\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5701 - val_loss: 0.0180\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.7853 - val_loss: 0.0131\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0344 - val_loss: 0.0179\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0724 - val_loss: 0.0128\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0170 - val_loss: 0.0176\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7345 - val_loss: 0.0135\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5248 - val_loss: 0.0144\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5499 - val_loss: 0.0156\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6101 - val_loss: 0.0126\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7142 - val_loss: 0.0192\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7415 - val_loss: 0.0126\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6337 - val_loss: 0.0183\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5892 - val_loss: 0.0126\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5501 - val_loss: 0.0180\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6012 - val_loss: 0.0127\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6140 - val_loss: 0.0200\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5688 - val_loss: 0.0126\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7059 - val_loss: 0.0215\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8D55C670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 3.6555 - val_loss: 0.0222\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.9605 - val_loss: 0.0260\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 23.9994 - val_loss: 0.0261\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.3345 - val_loss: 0.0259\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.9506 - val_loss: 0.0228\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.2365 - val_loss: 0.0253\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.4599 - val_loss: 0.0228\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.8176 - val_loss: 0.0265\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.8635 - val_loss: 0.0252\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3294 - val_loss: 0.0273\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1372 - val_loss: 0.0268\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9364 - val_loss: 0.0260\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9629 - val_loss: 0.0291\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9402 - val_loss: 0.0263\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0043 - val_loss: 0.0302\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7737 - val_loss: 0.0272\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E867463A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_70 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 236ms/step - loss: 2.2057 - val_loss: 0.0134\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.4050 - val_loss: 0.0177\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 22.5475 - val_loss: 0.0281\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.5293 - val_loss: 0.0246\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.5497 - val_loss: 0.0271\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.6377 - val_loss: 0.0236\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.8189 - val_loss: 0.0262\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.1049 - val_loss: 0.0232\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5519 - val_loss: 0.0269\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7838 - val_loss: 0.0238\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9468 - val_loss: 0.0288\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7879 - val_loss: 0.0253\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7634 - val_loss: 0.0316\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6308 - val_loss: 0.0269\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4247 - val_loss: 0.0335\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3272 - val_loss: 0.0284\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_71 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 1.9938 - val_loss: 0.0216\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.9895 - val_loss: 0.0132\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.5880 - val_loss: 0.0195\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.6812 - val_loss: 0.0192\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.8448 - val_loss: 0.0195\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.5446 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.0067 - val_loss: 0.0193\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.2183 - val_loss: 0.0202\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.1097 - val_loss: 0.0225\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.5593 - val_loss: 0.0223\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1375 - val_loss: 0.0252\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.6309 - val_loss: 0.0247\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.5507 - val_loss: 0.0260\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0961 - val_loss: 0.0255\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6236 - val_loss: 0.0261\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3690 - val_loss: 0.0289\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3292 - val_loss: 0.0255\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDD49D700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 1.5928 - val_loss: 0.0190\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.2338 - val_loss: 0.0178\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 17.6794 - val_loss: 0.0230\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.1121 - val_loss: 0.0209\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.1898 - val_loss: 0.0252\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8168 - val_loss: 0.0255\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.8643 - val_loss: 0.0273\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.3243 - val_loss: 0.0267\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3081 - val_loss: 0.0255\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.3913 - val_loss: 0.0272\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6609 - val_loss: 0.0264\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9183 - val_loss: 0.0273\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7310 - val_loss: 0.0249\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6767 - val_loss: 0.0280\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6857 - val_loss: 0.0241\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4919 - val_loss: 0.0280\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2856 - val_loss: 0.0241\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E82FB3B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_73 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.9246 - val_loss: 0.0134\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 17.9760 - val_loss: 0.0152\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 16.8795 - val_loss: 0.0152\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.3392 - val_loss: 0.0154\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.5066 - val_loss: 0.0146\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3541 - val_loss: 0.0152\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.5260 - val_loss: 0.0144\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.0190 - val_loss: 0.0138\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7871 - val_loss: 0.0133\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0958 - val_loss: 0.0129\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7620 - val_loss: 0.0132\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6350 - val_loss: 0.0122\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6885 - val_loss: 0.0126\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7261 - val_loss: 0.0116\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4877 - val_loss: 0.0120\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4011 - val_loss: 0.0114\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3603 - val_loss: 0.0119\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3636 - val_loss: 0.0117\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4830 - val_loss: 0.0121\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7178 - val_loss: 0.0125\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8863 - val_loss: 0.0121\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7877 - val_loss: 0.0125\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6034 - val_loss: 0.0114\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4790 - val_loss: 0.0120\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4662 - val_loss: 0.0114\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4853 - val_loss: 0.0119\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5260 - val_loss: 0.0116\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5235 - val_loss: 0.0129\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5262 - val_loss: 0.0119\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6305 - val_loss: 0.0124\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6370 - val_loss: 0.0129\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7038 - val_loss: 0.0119\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6701 - val_loss: 0.0133\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6311 - val_loss: 0.0124\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5933 - val_loss: 0.0145\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6917 - val_loss: 0.0126\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8699 - val_loss: 0.0187\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0421 - val_loss: 0.0115\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAAF3550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_74 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.8901 - val_loss: 0.0299\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 20.7325 - val_loss: 0.0139\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.4159 - val_loss: 0.0167\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 13.3498 - val_loss: 0.0139\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.7857 - val_loss: 0.0140\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9452 - val_loss: 0.0175\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.8651 - val_loss: 0.0169\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.8043 - val_loss: 0.0201\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.2551 - val_loss: 0.0191\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.2711 - val_loss: 0.0215\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4225 - val_loss: 0.0224\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2593 - val_loss: 0.0202\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0677 - val_loss: 0.0240\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8104 - val_loss: 0.0218\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6332 - val_loss: 0.0224\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5566 - val_loss: 0.0245\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5140 - val_loss: 0.0210\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4365 - val_loss: 0.0255\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3261 - val_loss: 0.0209\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E80025280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_75 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.8960 - val_loss: 0.0270\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.2620 - val_loss: 0.0145\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.9854 - val_loss: 0.0152\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.5385 - val_loss: 0.0147\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6004 - val_loss: 0.0168\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9169 - val_loss: 0.0181\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.2859 - val_loss: 0.0166\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.3406 - val_loss: 0.0195\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4121 - val_loss: 0.0156\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 3.3688 - val_loss: 0.0175\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.6009 - val_loss: 0.0157\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.2475 - val_loss: 0.0149\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.1475 - val_loss: 0.0151\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.0476 - val_loss: 0.0144\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5033 - val_loss: 0.0144\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9249 - val_loss: 0.0135\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5629 - val_loss: 0.0133\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3590 - val_loss: 0.0129\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2603 - val_loss: 0.0130\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2754 - val_loss: 0.0123\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3652 - val_loss: 0.0129\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4875 - val_loss: 0.0119\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5209 - val_loss: 0.0129\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4964 - val_loss: 0.0118\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4549 - val_loss: 0.0127\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3561 - val_loss: 0.0117\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3566 - val_loss: 0.0127\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3844 - val_loss: 0.0114\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4258 - val_loss: 0.0136\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4733 - val_loss: 0.0114\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5459 - val_loss: 0.0133\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5678 - val_loss: 0.0115\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6809 - val_loss: 0.0138\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7833 - val_loss: 0.0120\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7473 - val_loss: 0.0167\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7626 - val_loss: 0.0130\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7769 - val_loss: 0.0182\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6629 - val_loss: 0.0120\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5877 - val_loss: 0.0178\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6346 - val_loss: 0.0123\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB97A4430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_76 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 1.9569 - val_loss: 0.0215\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 16.3364 - val_loss: 0.0160\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 15.0599 - val_loss: 0.0210\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.7413 - val_loss: 0.0172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.8619 - val_loss: 0.0179\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.8541 - val_loss: 0.0165\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 5.7525 - val_loss: 0.0152\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.4347 - val_loss: 0.0145\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1418 - val_loss: 0.0153\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.7423 - val_loss: 0.0154\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.5051 - val_loss: 0.0151\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9623 - val_loss: 0.0159\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8722 - val_loss: 0.0141\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8530 - val_loss: 0.0167\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8255 - val_loss: 0.0136\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7605 - val_loss: 0.0174\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6370 - val_loss: 0.0136\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5272 - val_loss: 0.0174\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4037 - val_loss: 0.0138\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3099 - val_loss: 0.0170\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2529 - val_loss: 0.0142\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2161 - val_loss: 0.0163\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2104 - val_loss: 0.0148\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2355 - val_loss: 0.0158\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3080 - val_loss: 0.0154\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4349 - val_loss: 0.0149\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5985 - val_loss: 0.0158\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7763 - val_loss: 0.0141\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9462 - val_loss: 0.0182\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9059 - val_loss: 0.0142\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEE4379D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 2.2034 - val_loss: 0.0203\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.7870 - val_loss: 0.0183\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 17.1912 - val_loss: 0.0202\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 11.6280 - val_loss: 0.0200\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.9034 - val_loss: 0.0228\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.1166 - val_loss: 0.0209\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.5549 - val_loss: 0.0255\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.3551 - val_loss: 0.0213\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8248 - val_loss: 0.0251\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.0861 - val_loss: 0.0217\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1482 - val_loss: 0.0272\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9930 - val_loss: 0.0277\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7903 - val_loss: 0.0277\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1479 - val_loss: 0.0342\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7339 - val_loss: 0.0299\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4105 - val_loss: 0.0366\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2804 - val_loss: 0.0337\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEE924310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_78 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 1.8956 - val_loss: 0.0134\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.3877 - val_loss: 0.0224\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.2385 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 12.5582 - val_loss: 0.0175\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.0250 - val_loss: 0.0148\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.6747 - val_loss: 0.0156\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.5688 - val_loss: 0.0142\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.8419 - val_loss: 0.0147\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.1337 - val_loss: 0.0136\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2855 - val_loss: 0.0151\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2187 - val_loss: 0.0135\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8348 - val_loss: 0.0140\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4939 - val_loss: 0.0136\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3074 - val_loss: 0.0127\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3001 - val_loss: 0.0139\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3163 - val_loss: 0.0120\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3150 - val_loss: 0.0139\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3135 - val_loss: 0.0118\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2916 - val_loss: 0.0137\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2717 - val_loss: 0.0117\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2652 - val_loss: 0.0130\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3620 - val_loss: 0.0116\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6136 - val_loss: 0.0126\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9206 - val_loss: 0.0118\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8333 - val_loss: 0.0126\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5942 - val_loss: 0.0120\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3771 - val_loss: 0.0119\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2590 - val_loss: 0.0119\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2186 - val_loss: 0.0115\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2391 - val_loss: 0.0114\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3457 - val_loss: 0.0132\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6637 - val_loss: 0.0126\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1022 - val_loss: 0.0168\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2780 - val_loss: 0.0123\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3736 - val_loss: 0.0152\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4367 - val_loss: 0.0118\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.6717 - val_loss: 0.0179\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2680 - val_loss: 0.0142\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2215 - val_loss: 0.0173\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6825 - val_loss: 0.0117\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8EB95C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_79 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.1525 - val_loss: 0.0247\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 18.9539 - val_loss: 0.0172\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6565 - val_loss: 0.0195\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.0607 - val_loss: 0.0198\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.0825 - val_loss: 0.0182\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.5757 - val_loss: 0.0206\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.9552 - val_loss: 0.0174\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.2503 - val_loss: 0.0216\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4768 - val_loss: 0.0180\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6137 - val_loss: 0.0219\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.1047 - val_loss: 0.0193\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1274 - val_loss: 0.0223\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4942 - val_loss: 0.0235\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.9432 - val_loss: 0.0227\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.4955 - val_loss: 0.0253\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8882 - val_loss: 0.0227\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4766 - val_loss: 0.0276\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E80EC3A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_80 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.7823 - val_loss: 0.0147\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.2675 - val_loss: 0.0159\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 23.7561 - val_loss: 0.0147\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.0632 - val_loss: 0.0134\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0166 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3446 - val_loss: 0.0130\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 4.9396 - val_loss: 0.0150\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.6165 - val_loss: 0.0133\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7037 - val_loss: 0.0143\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.8460 - val_loss: 0.0128\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3810 - val_loss: 0.0145\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7546 - val_loss: 0.0128\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6425 - val_loss: 0.0135\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8555 - val_loss: 0.0127\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0228 - val_loss: 0.0132\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8911 - val_loss: 0.0123\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8663 - val_loss: 0.0133\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8567 - val_loss: 0.0120\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.9609 - val_loss: 0.0133\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2657 - val_loss: 0.0120\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.6200 - val_loss: 0.0128\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.6067 - val_loss: 0.0121\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.4517 - val_loss: 0.0125\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.2846 - val_loss: 0.0122\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0100 - val_loss: 0.0119\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1395 - val_loss: 0.0131\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0075 - val_loss: 0.0120\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8095 - val_loss: 0.0134\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6323 - val_loss: 0.0122\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6334 - val_loss: 0.0125\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5655 - val_loss: 0.0139\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5185 - val_loss: 0.0120\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5203 - val_loss: 0.0146\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5194 - val_loss: 0.0121\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4985 - val_loss: 0.0141\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5073 - val_loss: 0.0125\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4957 - val_loss: 0.0151\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4999 - val_loss: 0.0123\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5478 - val_loss: 0.0160\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6127 - val_loss: 0.0124\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA69DBD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_81 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.3088 - val_loss: 0.0127\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.7294 - val_loss: 0.0154\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.8855 - val_loss: 0.0146\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.5247 - val_loss: 0.0143\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.1447 - val_loss: 0.0155\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3743 - val_loss: 0.0139\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.6645 - val_loss: 0.0145\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7848 - val_loss: 0.0135\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.4935 - val_loss: 0.0136\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3167 - val_loss: 0.0127\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1795 - val_loss: 0.0135\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3967 - val_loss: 0.0127\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9121 - val_loss: 0.0130\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8681 - val_loss: 0.0130\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6567 - val_loss: 0.0134\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7052 - val_loss: 0.0125\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8729 - val_loss: 0.0138\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.0402 - val_loss: 0.0126\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.8541 - val_loss: 0.0139\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6825 - val_loss: 0.0124\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6200 - val_loss: 0.0146\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6232 - val_loss: 0.0123\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5910 - val_loss: 0.0146\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5998 - val_loss: 0.0122\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5626 - val_loss: 0.0157\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5681 - val_loss: 0.0120\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5996 - val_loss: 0.0173\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6685 - val_loss: 0.0123\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6980 - val_loss: 0.0193\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6093 - val_loss: 0.0125\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5055 - val_loss: 0.0179\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4715 - val_loss: 0.0128\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4381 - val_loss: 0.0180\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4307 - val_loss: 0.0124\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4401 - val_loss: 0.0145\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4491 - val_loss: 0.0123\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4828 - val_loss: 0.0132\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4563 - val_loss: 0.0134\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5933 - val_loss: 0.0126\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6162 - val_loss: 0.0176\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA69E0940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_82 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 2.2119 - val_loss: 0.0198\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.8198 - val_loss: 0.0132\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 17.8415 - val_loss: 0.0145\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.9227 - val_loss: 0.0131\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.6141 - val_loss: 0.0125\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.9554 - val_loss: 0.0136\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.1235 - val_loss: 0.0126\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0577 - val_loss: 0.0143\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.5214 - val_loss: 0.0122\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8312 - val_loss: 0.0140\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0698 - val_loss: 0.0122\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7068 - val_loss: 0.0136\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7318 - val_loss: 0.0121\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0159 - val_loss: 0.0133\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2294 - val_loss: 0.0122\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2080 - val_loss: 0.0129\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8585 - val_loss: 0.0120\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7301 - val_loss: 0.0127\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5567 - val_loss: 0.0118\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3825 - val_loss: 0.0122\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3278 - val_loss: 0.0119\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3432 - val_loss: 0.0118\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4330 - val_loss: 0.0120\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5326 - val_loss: 0.0118\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6095 - val_loss: 0.0118\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6952 - val_loss: 0.0119\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7772 - val_loss: 0.0118\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8492 - val_loss: 0.0122\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9483 - val_loss: 0.0129\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9008 - val_loss: 0.0131\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8782 - val_loss: 0.0120\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9106 - val_loss: 0.0119\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8337 - val_loss: 0.0120\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0441 - val_loss: 0.0149\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1734 - val_loss: 0.0132\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2833 - val_loss: 0.0180\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.5029 - val_loss: 0.0141\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0296 - val_loss: 0.0175\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2240 - val_loss: 0.0131\n",
      "Epoch 00039: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED3200D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_83 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.4565 - val_loss: 0.0192\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.8452 - val_loss: 0.0149\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 21.0396 - val_loss: 0.0132\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.7107 - val_loss: 0.0162\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 11.6398 - val_loss: 0.0154\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.0322 - val_loss: 0.0161\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.9276 - val_loss: 0.0166\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.2286 - val_loss: 0.0147\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 2.3160 - val_loss: 0.0161\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.3314 - val_loss: 0.0152\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8647 - val_loss: 0.0156\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5007 - val_loss: 0.0160\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4020 - val_loss: 0.0155\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3973 - val_loss: 0.0160\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.4648 - val_loss: 0.0156\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.5604 - val_loss: 0.0165\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7118 - val_loss: 0.0143\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9164 - val_loss: 0.0180\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA24B5040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_84 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 2.8170 - val_loss: 0.0135\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.0309 - val_loss: 0.0135\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.2983 - val_loss: 0.0142\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 15.1892 - val_loss: 0.0128\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.4597 - val_loss: 0.0138\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.1845 - val_loss: 0.0129\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.2417 - val_loss: 0.0130\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.2381 - val_loss: 0.0131\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7636 - val_loss: 0.0124\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.3159 - val_loss: 0.0134\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8510 - val_loss: 0.0123\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.912 - 0s 39ms/step - loss: 1.9129 - val_loss: 0.0135\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4958 - val_loss: 0.0125\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0000 - val_loss: 0.0135\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6796 - val_loss: 0.0126\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7330 - val_loss: 0.0138\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1411 - val_loss: 0.0125\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1487 - val_loss: 0.0133\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7478 - val_loss: 0.0124\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4546 - val_loss: 0.0133\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3036 - val_loss: 0.0123\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2129 - val_loss: 0.0134\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2024 - val_loss: 0.0126\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2608 - val_loss: 0.0142\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 0.4001 - val_loss: 0.0139\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5292 - val_loss: 0.0152\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8E1FDDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_85 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 1.6128 - val_loss: 0.0176\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5653 - val_loss: 0.0157\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.9780 - val_loss: 0.0153\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 10.4111 - val_loss: 0.0156\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.2499 - val_loss: 0.0156\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.8177 - val_loss: 0.0143\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.9802 - val_loss: 0.0167\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.6220 - val_loss: 0.0143\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7910 - val_loss: 0.0171\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.3776 - val_loss: 0.0144\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5965 - val_loss: 0.0178\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9785 - val_loss: 0.0155\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5510 - val_loss: 0.0176\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2958 - val_loss: 0.0173\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2490 - val_loss: 0.0175\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2729 - val_loss: 0.0196\n",
      "Epoch 17/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2938 - val_loss: 0.0177\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3158 - val_loss: 0.0217\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.3274 - val_loss: 0.0178\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3221 - val_loss: 0.0236\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2841 - val_loss: 0.0179\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2508 - val_loss: 0.0253\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2458 - val_loss: 0.0173\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E927C78B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_86 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 2.6197 - val_loss: 0.0136\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.5877 - val_loss: 0.0141\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 17.9202 - val_loss: 0.0131\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.5999 - val_loss: 0.0131\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.2049 - val_loss: 0.0133\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.4063 - val_loss: 0.0128\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.6264 - val_loss: 0.0128\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.8614 - val_loss: 0.0127\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.2995 - val_loss: 0.0127\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 4.3589 - val_loss: 0.0130\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.6165 - val_loss: 0.0127\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0904 - val_loss: 0.0128\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6314 - val_loss: 0.0127\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0077 - val_loss: 0.0127\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6066 - val_loss: 0.0128\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4055 - val_loss: 0.0127\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2782 - val_loss: 0.0128\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2288 - val_loss: 0.0127\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2398 - val_loss: 0.0127\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3476 - val_loss: 0.0131\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5313 - val_loss: 0.0127\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6880 - val_loss: 0.0136\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7608 - val_loss: 0.0129\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6290 - val_loss: 0.0134\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5089 - val_loss: 0.0130\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4358 - val_loss: 0.0134\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3572 - val_loss: 0.0129\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3198 - val_loss: 0.0132\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3675 - val_loss: 0.0129\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5244 - val_loss: 0.0128\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7885 - val_loss: 0.0133\n",
      "Epoch 00031: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_87 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 2.2254 - val_loss: 0.0159\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 18.5034 - val_loss: 0.0136\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.6472 - val_loss: 0.0155\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 14.2887 - val_loss: 0.0133\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2669 - val_loss: 0.0135\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.3157 - val_loss: 0.0136\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3476 - val_loss: 0.0134\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6978 - val_loss: 0.0137\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.6760 - val_loss: 0.0134\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0906 - val_loss: 0.0134\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.822 - 0s 37ms/step - loss: 1.8225 - val_loss: 0.0133\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9767 - val_loss: 0.0134\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5926 - val_loss: 0.0133\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0216 - val_loss: 0.0133\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5871 - val_loss: 0.0133\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4049 - val_loss: 0.0132\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3179 - val_loss: 0.0134\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2860 - val_loss: 0.0131\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3399 - val_loss: 0.0134\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3839 - val_loss: 0.0131\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4289 - val_loss: 0.0135\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5066 - val_loss: 0.0132\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6388 - val_loss: 0.0134\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8094 - val_loss: 0.0131\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0131 - val_loss: 0.0147\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0729 - val_loss: 0.0133\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8660 - val_loss: 0.0144\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6280 - val_loss: 0.0133\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4552 - val_loss: 0.0135\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3657 - val_loss: 0.0139\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3125 - val_loss: 0.0133\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3233 - val_loss: 0.0154\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.4450 - val_loss: 0.0139\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6722 - val_loss: 0.0190\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9165 - val_loss: 0.0159\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0113 - val_loss: 0.0222\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0155 - val_loss: 0.0150\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0591 - val_loss: 0.0197\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4545 - val_loss: 0.0138\n",
      "Epoch 00039: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E99D6A4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_88 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 1.9860 - val_loss: 0.0198\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.3249 - val_loss: 0.0152\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.8418 - val_loss: 0.0163\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.0957 - val_loss: 0.0146\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8764 - val_loss: 0.0161\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.0316 - val_loss: 0.0157\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.6123 - val_loss: 0.0170\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.4286 - val_loss: 0.0166\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9931 - val_loss: 0.0174\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5452 - val_loss: 0.0179\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9189 - val_loss: 0.0185\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.7281 - val_loss: 0.0203\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4294 - val_loss: 0.0190\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0928 - val_loss: 0.0233\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6571 - val_loss: 0.0198\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6098 - val_loss: 0.0264\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6020 - val_loss: 0.0205\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5020 - val_loss: 0.0282\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4248 - val_loss: 0.0225\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDB4029D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_89 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 735ms/step - loss: 1.8071 - val_loss: 0.0147\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 13.9010 - val_loss: 0.0168\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 23.8646 - val_loss: 0.0157\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 13.8330 - val_loss: 0.0163\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.8536 - val_loss: 0.0169\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.4586 - val_loss: 0.0154\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4.7502 - val_loss: 0.0163\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.5072 - val_loss: 0.0150\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4845 - val_loss: 0.0155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6325 - val_loss: 0.0153\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0832 - val_loss: 0.0156\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6209 - val_loss: 0.0156\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4146 - val_loss: 0.0158\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3799 - val_loss: 0.0163\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5018 - val_loss: 0.0162\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6231 - val_loss: 0.0168\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E80490280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_90 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 2.1676 - val_loss: 0.0211\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.6529 - val_loss: 0.0152\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.8140 - val_loss: 0.0160\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.1774 - val_loss: 0.0149\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.4944 - val_loss: 0.0148\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.4814 - val_loss: 0.0150\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.5476 - val_loss: 0.0149\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.4228 - val_loss: 0.0151\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3476 - val_loss: 0.0153\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.4754 - val_loss: 0.0151\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.7676 - val_loss: 0.0155\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.4745 - val_loss: 0.0149\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1968 - val_loss: 0.0153\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6787 - val_loss: 0.0149\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3656 - val_loss: 0.0154\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1698 - val_loss: 0.0151\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1141 - val_loss: 0.0157\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1402 - val_loss: 0.0156\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2574 - val_loss: 0.0164\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4130 - val_loss: 0.0163\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA68D1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_91 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 2.0055 - val_loss: 0.0163\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14.9040 - val_loss: 0.0203\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.1565 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.6064 - val_loss: 0.0179\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.9341 - val_loss: 0.0174\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.4007 - val_loss: 0.0176\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.2355 - val_loss: 0.0170\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.7625 - val_loss: 0.0173\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8483 - val_loss: 0.0189\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.6194 - val_loss: 0.0182\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5201 - val_loss: 0.0199\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9914 - val_loss: 0.0191\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6428 - val_loss: 0.0206\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6324 - val_loss: 0.0201\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7894 - val_loss: 0.0220\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0015 - val_loss: 0.0209\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED3200280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_92 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 268ms/step - loss: 1.8099 - val_loss: 0.0158\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.7859 - val_loss: 0.0166\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 17.3575 - val_loss: 0.0155\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.1760 - val_loss: 0.0160\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.1740 - val_loss: 0.0157\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.7184 - val_loss: 0.0155\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.5068 - val_loss: 0.0157\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.9662 - val_loss: 0.0157\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.2083 - val_loss: 0.0154\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.2630 - val_loss: 0.0153\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.4394 - val_loss: 0.0155\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8358 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2596 - val_loss: 0.0156\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6426 - val_loss: 0.0154\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3092 - val_loss: 0.0156\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8185 - val_loss: 0.0156\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4642 - val_loss: 0.0155\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2636 - val_loss: 0.0159\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1870 - val_loss: 0.0154\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1528 - val_loss: 0.0162\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1361 - val_loss: 0.0154\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1341 - val_loss: 0.0166\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1478 - val_loss: 0.0154\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1980 - val_loss: 0.0172\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3015 - val_loss: 0.0155\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 86ms/step - loss: 0.4539 - val_loss: 0.0179\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5631 - val_loss: 0.0155\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC0190430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_93 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 1.8654 - val_loss: 0.0188\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.3762 - val_loss: 0.0157\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 21.0001 - val_loss: 0.0175\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.3093 - val_loss: 0.0175\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.8231 - val_loss: 0.0177\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.5862 - val_loss: 0.0182\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.8541 - val_loss: 0.0174\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0227 - val_loss: 0.0191\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7121 - val_loss: 0.0194\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0990 - val_loss: 0.0184\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1256 - val_loss: 0.0202\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6958 - val_loss: 0.0191\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2214 - val_loss: 0.0214\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8721 - val_loss: 0.0198\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5153 - val_loss: 0.0220\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3137 - val_loss: 0.0208\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2122 - val_loss: 0.0224\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8EB32E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_94 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 1.3376 - val_loss: 0.0153\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.1776 - val_loss: 0.0208\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 24.0475 - val_loss: 0.0153\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1077 - val_loss: 0.0169\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.9393 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.5093 - val_loss: 0.0159\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0018 - val_loss: 0.0156\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0949 - val_loss: 0.0155\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.8675 - val_loss: 0.0153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5582 - val_loss: 0.0154\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6806 - val_loss: 0.0152\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6674 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2625 - val_loss: 0.0156\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8491 - val_loss: 0.0154\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4884 - val_loss: 0.0162\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4177 - val_loss: 0.0158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4708 - val_loss: 0.0173\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5576 - val_loss: 0.0166\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6012 - val_loss: 0.0186\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6408 - val_loss: 0.0176\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED0C5A8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_95 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1.8972 - val_loss: 0.0246\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.6880 - val_loss: 0.0150\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 22.2623 - val_loss: 0.0175\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.3207 - val_loss: 0.0164\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6427 - val_loss: 0.0158\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.7238 - val_loss: 0.0160\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.5902 - val_loss: 0.0158\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.1165 - val_loss: 0.0158\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.8566 - val_loss: 0.0156\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2909 - val_loss: 0.0153\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4804 - val_loss: 0.0159\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9640 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8248 - val_loss: 0.0161\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6425 - val_loss: 0.0153\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4163 - val_loss: 0.0161\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3248 - val_loss: 0.0154\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2978 - val_loss: 0.0161\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE69A7280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_96 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 2.0481 - val_loss: 0.0145\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.8345 - val_loss: 0.0199\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 15.9060 - val_loss: 0.0157\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.7018 - val_loss: 0.0164\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.5998 - val_loss: 0.0156\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.1156 - val_loss: 0.0157\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.5094 - val_loss: 0.0153\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.4258 - val_loss: 0.0152\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.4301 - val_loss: 0.0151\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.2616 - val_loss: 0.0152\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.6448 - val_loss: 0.0149\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.0723 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.9691 - val_loss: 0.0149\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2796 - val_loss: 0.0152\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6826 - val_loss: 0.0149\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3787 - val_loss: 0.0151\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED48C11F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_97 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 248ms/step - loss: 2.1703 - val_loss: 0.0213\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.3039 - val_loss: 0.0167\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.2157 - val_loss: 0.0197\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.5006 - val_loss: 0.0169\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.1556 - val_loss: 0.0168\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.2018 - val_loss: 0.0169\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3921 - val_loss: 0.0164\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.5652 - val_loss: 0.0167\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.9932 - val_loss: 0.0152\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1368 - val_loss: 0.0156\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.6772 - val_loss: 0.0156\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.1932 - val_loss: 0.0151\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.7873 - val_loss: 0.0155\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2499 - val_loss: 0.0150\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.9985 - val_loss: 0.0151\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7995 - val_loss: 0.0148\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 138ms/step - loss: 0.5489 - val_loss: 0.0148\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3440 - val_loss: 0.0147\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2039 - val_loss: 0.0146\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1383 - val_loss: 0.0146\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1189 - val_loss: 0.0145\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1308 - val_loss: 0.0146\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1758 - val_loss: 0.0150\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2800 - val_loss: 0.0149\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5014 - val_loss: 0.0172\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8037 - val_loss: 0.0156\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9025 - val_loss: 0.0179\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0112 - val_loss: 0.0159\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1993 - val_loss: 0.0171\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2779 - val_loss: 0.0145\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3476 - val_loss: 0.0154\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2789 - val_loss: 0.0165\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2294 - val_loss: 0.0145\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1496 - val_loss: 0.0215\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0420 - val_loss: 0.0144\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9658 - val_loss: 0.0183\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8125 - val_loss: 0.0147\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6629 - val_loss: 0.0163\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5534 - val_loss: 0.0159\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5329 - val_loss: 0.0153\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFFC1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_98 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 323ms/step - loss: 1.4730 - val_loss: 0.0195\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.4384 - val_loss: 0.0195\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.3278 - val_loss: 0.0188\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.6148 - val_loss: 0.0196\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.7173 - val_loss: 0.0218\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7398 - val_loss: 0.0218\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.8499 - val_loss: 0.0231\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.5827 - val_loss: 0.0214\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.1217 - val_loss: 0.0219\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.3552 - val_loss: 0.0233\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4359 - val_loss: 0.0223\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.2268 - val_loss: 0.0238\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8543 - val_loss: 0.0210\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5089 - val_loss: 0.0250\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8670 - val_loss: 0.0223\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4481 - val_loss: 0.0243\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3003 - val_loss: 0.0247\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2739 - val_loss: 0.0245\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EFA98BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_99 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 324ms/step - loss: 2.3846 - val_loss: 0.0164\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 16.2994 - val_loss: 0.0189\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 18.3482 - val_loss: 0.0180\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.8325 - val_loss: 0.0176\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.1994 - val_loss: 0.0173\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.148 - 0s 29ms/step - loss: 7.1485 - val_loss: 0.0170\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.3988 - val_loss: 0.0169\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.6317 - val_loss: 0.0166\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3068 - val_loss: 0.0165\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.8339 - val_loss: 0.0162\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.5995 - val_loss: 0.0157\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9301 - val_loss: 0.0160\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7874 - val_loss: 0.0154\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7937 - val_loss: 0.0156\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5872 - val_loss: 0.0154\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4183 - val_loss: 0.0154\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3033 - val_loss: 0.0154\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2264 - val_loss: 0.0154\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1811 - val_loss: 0.0154\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1778 - val_loss: 0.0155\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2340 - val_loss: 0.0155\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3391 - val_loss: 0.0154\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4978 - val_loss: 0.0160\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6714 - val_loss: 0.0154\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7231 - val_loss: 0.0172\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7983 - val_loss: 0.0155\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7877 - val_loss: 0.0179\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8089 - val_loss: 0.0155\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7944 - val_loss: 0.0194\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8841 - val_loss: 0.0156\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E88B49DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_100 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.7786 - val_loss: 0.0382\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.8020 - val_loss: 0.0207\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 20.3992 - val_loss: 0.0244\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.8754 - val_loss: 0.0205\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.3153 - val_loss: 0.0222\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.8869 - val_loss: 0.0212\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.9293 - val_loss: 0.0212\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.6790 - val_loss: 0.0224\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9072 - val_loss: 0.0210\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9625 - val_loss: 0.0211\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7582 - val_loss: 0.0197\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.6517 - val_loss: 0.0208\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9870 - val_loss: 0.0196\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.6173 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3846 - val_loss: 0.0204\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0528 - val_loss: 0.0208\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6824 - val_loss: 0.0202\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3679 - val_loss: 0.0211\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2023 - val_loss: 0.0200\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1371 - val_loss: 0.0215\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1419 - val_loss: 0.0198\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2014 - val_loss: 0.0221\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2807 - val_loss: 0.0198\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3910 - val_loss: 0.0227\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4298 - val_loss: 0.0207\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5619 - val_loss: 0.0224\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5724 - val_loss: 0.0227\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5627 - val_loss: 0.0203\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E80E13310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_101 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 916ms/step - loss: 2.2326 - val_loss: 0.0276\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.9896 - val_loss: 0.0286\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 17.8395 - val_loss: 0.0272\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.6750 - val_loss: 0.0261\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.9794 - val_loss: 0.0273\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.9238 - val_loss: 0.0282\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.6630 - val_loss: 0.0272\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.0260 - val_loss: 0.0296\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.2976 - val_loss: 0.0288\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3110 - val_loss: 0.0313\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9142 - val_loss: 0.0308\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9146 - val_loss: 0.0324\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1783 - val_loss: 0.0304\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7878 - val_loss: 0.0309\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5561 - val_loss: 0.0327\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6012 - val_loss: 0.0284\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7057 - val_loss: 0.0361\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6703 - val_loss: 0.0275\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4878 - val_loss: 0.0363\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E82FB3F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.6965 - val_loss: 0.0287\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.5793 - val_loss: 0.0269\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.5977 - val_loss: 0.0288\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.2432 - val_loss: 0.0293\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.5606 - val_loss: 0.0302\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.2922 - val_loss: 0.0287\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.6093 - val_loss: 0.0293\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.0419 - val_loss: 0.0281\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.9476 - val_loss: 0.0280\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.6263 - val_loss: 0.0293\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9101 - val_loss: 0.0270\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.5716 - val_loss: 0.0285\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0532 - val_loss: 0.0265\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5494 - val_loss: 0.0283\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8363 - val_loss: 0.0272\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4268 - val_loss: 0.0277\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2929 - val_loss: 0.0273\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2652 - val_loss: 0.0270\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2560 - val_loss: 0.0275\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2239 - val_loss: 0.0262\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1901 - val_loss: 0.0281\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1810 - val_loss: 0.0251\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2300 - val_loss: 0.0296\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3759 - val_loss: 0.0232\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5940 - val_loss: 0.0323\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7353 - val_loss: 0.0219\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7184 - val_loss: 0.0314\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6604 - val_loss: 0.0216\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5847 - val_loss: 0.0296\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6251 - val_loss: 0.0216\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6496 - val_loss: 0.0291\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5791 - val_loss: 0.0222\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5410 - val_loss: 0.0262\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4938 - val_loss: 0.0249\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4515 - val_loss: 0.0236\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4942 - val_loss: 0.0249\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5958 - val_loss: 0.0266\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6402 - val_loss: 0.0221\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7147 - val_loss: 0.0289\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7885 - val_loss: 0.0199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED52DB670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_103 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 327ms/step - loss: 1.6494 - val_loss: 0.0227\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.6887 - val_loss: 0.0346\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 15.3485 - val_loss: 0.0273\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.0803 - val_loss: 0.0323\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.7386 - val_loss: 0.0306\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.2910 - val_loss: 0.0303\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.0078 - val_loss: 0.0271\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.8043 - val_loss: 0.0280\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.2905 - val_loss: 0.0275\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.5347 - val_loss: 0.0277\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.0726 - val_loss: 0.0273\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1221 - val_loss: 0.0285\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9588 - val_loss: 0.0280\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9013 - val_loss: 0.0264\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1187 - val_loss: 0.0291\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8466 - val_loss: 0.0245\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F16677820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_104 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 0.9729 - val_loss: 0.0359\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.6322 - val_loss: 0.0266\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 19.6937 - val_loss: 0.0416\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.2251 - val_loss: 0.0360\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.1354 - val_loss: 0.0346\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.9681 - val_loss: 0.0337\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.9798 - val_loss: 0.0291\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.7232 - val_loss: 0.0315\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.3590 - val_loss: 0.0313\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.3572 - val_loss: 0.0306\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4866 - val_loss: 0.0312\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.6050 - val_loss: 0.0313\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0600 - val_loss: 0.0322\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6417 - val_loss: 0.0319\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5103 - val_loss: 0.0323\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4333 - val_loss: 0.0325\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3797 - val_loss: 0.0330\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED0C5AC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_105 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 1.6710 - val_loss: 0.0378\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.5772 - val_loss: 0.0477\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 29.7582 - val_loss: 0.0402\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.0898 - val_loss: 0.0412\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.6322 - val_loss: 0.0404\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.9654 - val_loss: 0.0420\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.0743 - val_loss: 0.0434\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.6396 - val_loss: 0.0474\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4852 - val_loss: 0.0448\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 1.4662 - val_loss: 0.0511\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9720 - val_loss: 0.0465\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5887 - val_loss: 0.0536\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4296 - val_loss: 0.0515\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3543 - val_loss: 0.0547\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4416 - val_loss: 0.0568\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5671 - val_loss: 0.0555\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F142813A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_106 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 2.0890 - val_loss: 0.0478\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.0104 - val_loss: 0.0354\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18.7760 - val_loss: 0.0360\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 11.3994 - val_loss: 0.0381\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.9248 - val_loss: 0.0372\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.3745 - val_loss: 0.0370\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.5164 - val_loss: 0.0420\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.5731 - val_loss: 0.0355\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.7644 - val_loss: 0.0443\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6596 - val_loss: 0.0390\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.0482 - val_loss: 0.0448\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3671 - val_loss: 0.0403\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8141 - val_loss: 0.0440\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7755 - val_loss: 0.0410\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8112 - val_loss: 0.0457\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6379 - val_loss: 0.0420\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4348 - val_loss: 0.0467\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F17437160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_107 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 799ms/step - loss: 2.6865 - val_loss: 0.0476\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.5061 - val_loss: 0.0359\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 16.6504 - val_loss: 0.0375\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.2005 - val_loss: 0.0425\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.3472 - val_loss: 0.0392\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.8620 - val_loss: 0.0449\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.3273 - val_loss: 0.0416\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.4938 - val_loss: 0.0438\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.0049 - val_loss: 0.0470\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.7118 - val_loss: 0.0467\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7003 - val_loss: 0.0520\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.7210 - val_loss: 0.0489\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4238 - val_loss: 0.0577\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2997 - val_loss: 0.0529\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0917 - val_loss: 0.0587\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8726 - val_loss: 0.0616\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6968 - val_loss: 0.0615\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDD49DE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_108 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 2.9736 - val_loss: 0.0328\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 10.9678 - val_loss: 0.0357\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 25.5930 - val_loss: 0.0406\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.9811 - val_loss: 0.0414\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.3326 - val_loss: 0.0486\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.0658 - val_loss: 0.0487\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.1098 - val_loss: 0.0526\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.2650 - val_loss: 0.0520\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.8920 - val_loss: 0.0572\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.5617 - val_loss: 0.0572\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6208 - val_loss: 0.0608\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0743 - val_loss: 0.0629\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5739 - val_loss: 0.0662\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3704 - val_loss: 0.0703\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3555 - val_loss: 0.0715\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4424 - val_loss: 0.0780\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F13EF3C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_109 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 1.4979 - val_loss: 0.0297\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.5006 - val_loss: 0.0395\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 18.4342 - val_loss: 0.0380\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.3521 - val_loss: 0.0357\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.0839 - val_loss: 0.0365\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.2408 - val_loss: 0.0338\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.8343 - val_loss: 0.0347\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.5231 - val_loss: 0.0344\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.3833 - val_loss: 0.0364\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1301 - val_loss: 0.0356\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.9804 - val_loss: 0.0399\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8447 - val_loss: 0.0369\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6226 - val_loss: 0.0418\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0106 - val_loss: 0.0414\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6085 - val_loss: 0.0445\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3688 - val_loss: 0.0461\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1DC7BEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 2.2293 - val_loss: 0.0350\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.4729 - val_loss: 0.0374\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 17.3945 - val_loss: 0.0394\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.8413 - val_loss: 0.0394\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.8527 - val_loss: 0.0366\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.3521 - val_loss: 0.0387\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.2311 - val_loss: 0.0365\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.8994 - val_loss: 0.0381\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3496 - val_loss: 0.0382\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.5531 - val_loss: 0.0374\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6572 - val_loss: 0.0388\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7065 - val_loss: 0.0383\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8226 - val_loss: 0.0389\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6150 - val_loss: 0.0378\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7842 - val_loss: 0.0379\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1015 - val_loss: 0.0365\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED1189AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_111 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 284ms/step - loss: 2.5434 - val_loss: 0.0383\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.7663 - val_loss: 0.0451\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 13.7959 - val_loss: 0.0425\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 12.4284 - val_loss: 0.0419\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.4257 - val_loss: 0.0438\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9781 - val_loss: 0.0425\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.0075 - val_loss: 0.0462\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.0565 - val_loss: 0.0456\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.9804 - val_loss: 0.0491\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.4901 - val_loss: 0.0485\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0592 - val_loss: 0.0511\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0995 - val_loss: 0.0537\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3956 - val_loss: 0.0530\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9193 - val_loss: 0.0580\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5425 - val_loss: 0.0585\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3639 - val_loss: 0.0622\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E81C42EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_112 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 2.8269 - val_loss: 0.0331\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 13.9703 - val_loss: 0.0361\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 24.1669 - val_loss: 0.0325\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.7364 - val_loss: 0.0331\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.9468 - val_loss: 0.0309\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.5304 - val_loss: 0.0319\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.8515 - val_loss: 0.0307\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.0109 - val_loss: 0.0308\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6445 - val_loss: 0.0296\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.9995 - val_loss: 0.0285\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2109 - val_loss: 0.0274\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.7811 - val_loss: 0.0263\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1034 - val_loss: 0.0263\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6187 - val_loss: 0.0250\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4393 - val_loss: 0.0253\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2571 - val_loss: 0.0243\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2344 - val_loss: 0.0245\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2966 - val_loss: 0.0238\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4228 - val_loss: 0.0241\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5764 - val_loss: 0.0233\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5873 - val_loss: 0.0239\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5713 - val_loss: 0.0230\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5025 - val_loss: 0.0235\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4509 - val_loss: 0.0230\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4245 - val_loss: 0.0229\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4650 - val_loss: 0.0232\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5570 - val_loss: 0.0231\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6702 - val_loss: 0.0235\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6417 - val_loss: 0.0233\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6250 - val_loss: 0.0238\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7375 - val_loss: 0.0236\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9643 - val_loss: 0.0249\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1156 - val_loss: 0.0252\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2920 - val_loss: 0.0245\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9480 - val_loss: 0.0240\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7557 - val_loss: 0.0250\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7811 - val_loss: 0.0237\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8662 - val_loss: 0.0236\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0271 - val_loss: 0.0229\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9467 - val_loss: 0.0235\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1EFF61F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_113 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 332ms/step - loss: 3.2305 - val_loss: 0.0394\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.4867 - val_loss: 0.0354\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 20.3102 - val_loss: 0.0356\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.8194 - val_loss: 0.0361\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.7822 - val_loss: 0.0385\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.5306 - val_loss: 0.0384\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.5681 - val_loss: 0.0392\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.4129 - val_loss: 0.0362\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.9480 - val_loss: 0.0363\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2711 - val_loss: 0.0364\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.8012 - val_loss: 0.0347\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.8124 - val_loss: 0.0332\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.3910 - val_loss: 0.0334\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.7765 - val_loss: 0.0312\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4673 - val_loss: 0.0338\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2373 - val_loss: 0.0295\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9516 - val_loss: 0.0304\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6968 - val_loss: 0.0296\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5080 - val_loss: 0.0284\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3897 - val_loss: 0.0300\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3097 - val_loss: 0.0268\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2579 - val_loss: 0.0296\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2564 - val_loss: 0.0258\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2837 - val_loss: 0.0294\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3183 - val_loss: 0.0251\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3446 - val_loss: 0.0291\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3773 - val_loss: 0.0244\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4718 - val_loss: 0.0280\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6666 - val_loss: 0.0251\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8794 - val_loss: 0.0252\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0080 - val_loss: 0.0264\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8989 - val_loss: 0.0236\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7497 - val_loss: 0.0265\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7315 - val_loss: 0.0234\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6989 - val_loss: 0.0256\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7695 - val_loss: 0.0235\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9756 - val_loss: 0.0281\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0373 - val_loss: 0.0235\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2995 - val_loss: 0.0276\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5177 - val_loss: 0.0235\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F16F59040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_114 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.8683 - val_loss: 0.0404\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.1834 - val_loss: 0.0374\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.7045 - val_loss: 0.0409\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.6785 - val_loss: 0.0400\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.0289 - val_loss: 0.0430\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.2229 - val_loss: 0.0442\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.1683 - val_loss: 0.0470\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.3144 - val_loss: 0.0469\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9682 - val_loss: 0.0473\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8085 - val_loss: 0.0448\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.4628 - val_loss: 0.0500\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.1188 - val_loss: 0.0455\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7918 - val_loss: 0.0520\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6481 - val_loss: 0.0515\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2463 - val_loss: 0.0494\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0437 - val_loss: 0.0540\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6142 - val_loss: 0.0494\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F166773A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_115 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 267ms/step - loss: 2.1251 - val_loss: 0.0390\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.2987 - val_loss: 0.0400\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 21.6031 - val_loss: 0.0368\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.9138 - val_loss: 0.0370\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.2477 - val_loss: 0.0382\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.5679 - val_loss: 0.0381\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.6955 - val_loss: 0.0405\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.5970 - val_loss: 0.0399\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4110 - val_loss: 0.0409\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8059 - val_loss: 0.0406\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.9413 - val_loss: 0.0399\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9406 - val_loss: 0.0390\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4091 - val_loss: 0.0400\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1995 - val_loss: 0.0391\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8118 - val_loss: 0.0374\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4502 - val_loss: 0.0386\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2806 - val_loss: 0.0365\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2471 - val_loss: 0.0381\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2392 - val_loss: 0.0363\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2600 - val_loss: 0.0374\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3129 - val_loss: 0.0365\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5029 - val_loss: 0.0367\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7674 - val_loss: 0.0372\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8778 - val_loss: 0.0353\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8569 - val_loss: 0.0393\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6940 - val_loss: 0.0373\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7183 - val_loss: 0.0400\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6903 - val_loss: 0.0360\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6058 - val_loss: 0.0416\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6058 - val_loss: 0.0347\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7089 - val_loss: 0.0429\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7359 - val_loss: 0.0399\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7580 - val_loss: 0.0354\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6335 - val_loss: 0.0449\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5217 - val_loss: 0.0345\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5180 - val_loss: 0.0485\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5860 - val_loss: 0.0343\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7509 - val_loss: 0.0480\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0293 - val_loss: 0.0422\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1439 - val_loss: 0.0348\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E80A0CF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_116 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 2.0174 - val_loss: 0.0353\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 20.8441 - val_loss: 0.0371\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 21.8565 - val_loss: 0.0387\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 11.1870 - val_loss: 0.0396\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.2015 - val_loss: 0.0394\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 5.9865 - val_loss: 0.0429\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.9348 - val_loss: 0.0429\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1394 - val_loss: 0.0472\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2732 - val_loss: 0.0487\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4090 - val_loss: 0.0487\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9449 - val_loss: 0.0540\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6756 - val_loss: 0.0521\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5050 - val_loss: 0.0575\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3322 - val_loss: 0.0580\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2693 - val_loss: 0.0614\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2592 - val_loss: 0.0639\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC192C940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_117 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 241ms/step - loss: 1.8341 - val_loss: 0.0366\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.9377 - val_loss: 0.0421\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 17.7259 - val_loss: 0.0401\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.9884 - val_loss: 0.0399\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.9234 - val_loss: 0.0365\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.0437 - val_loss: 0.0361\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.2266 - val_loss: 0.0385\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.9036 - val_loss: 0.0364\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7217 - val_loss: 0.0391\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8580 - val_loss: 0.0373\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1044 - val_loss: 0.0381\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8015 - val_loss: 0.0386\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.2344 - val_loss: 0.0376\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.7620 - val_loss: 0.0394\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.2332 - val_loss: 0.0393\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9435 - val_loss: 0.0381\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5959 - val_loss: 0.0395\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3276 - val_loss: 0.0392\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1808 - val_loss: 0.0397\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1250 - val_loss: 0.0399\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1209 - val_loss: 0.0395\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F0C4B00D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_118 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 1.7237 - val_loss: 0.0397\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 13.6292 - val_loss: 0.0460\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 15.8192 - val_loss: 0.0355\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 7.8378 - val_loss: 0.0405\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.0791 - val_loss: 0.0389\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.9185 - val_loss: 0.0384\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.7208 - val_loss: 0.0388\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.9182 - val_loss: 0.0389\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.4056 - val_loss: 0.0386\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.1011 - val_loss: 0.0371\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6561 - val_loss: 0.0432\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.0538 - val_loss: 0.0376\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3400 - val_loss: 0.0432\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.7770 - val_loss: 0.0403\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0295 - val_loss: 0.0410\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7081 - val_loss: 0.0421\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6368 - val_loss: 0.0390\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4777 - val_loss: 0.0419\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA28F0A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_119 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 1.8674 - val_loss: 0.0392\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 19.1609 - val_loss: 0.0376\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 28.3781 - val_loss: 0.0346\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 14.7824 - val_loss: 0.0318\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.2071 - val_loss: 0.0316\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.5299 - val_loss: 0.0300\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.0693 - val_loss: 0.0316\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.1688 - val_loss: 0.0303\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.1983 - val_loss: 0.0299\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1051 - val_loss: 0.0293\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.9284 - val_loss: 0.0297\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9458 - val_loss: 0.0288\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1111 - val_loss: 0.0286\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6191 - val_loss: 0.0270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5806 - val_loss: 0.0280\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2756 - val_loss: 0.0273\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9770 - val_loss: 0.0270\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6179 - val_loss: 0.0274\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3605 - val_loss: 0.0263\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2628 - val_loss: 0.0273\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3078 - val_loss: 0.0261\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5549 - val_loss: 0.0265\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1.0404 - val_loss: 0.0261\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3824 - val_loss: 0.0255\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4708 - val_loss: 0.0258\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2663 - val_loss: 0.0258\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0528 - val_loss: 0.0253\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8727 - val_loss: 0.0255\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7964 - val_loss: 0.0253\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8115 - val_loss: 0.0257\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5984 - val_loss: 0.0253\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4783 - val_loss: 0.0251\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4220 - val_loss: 0.0251\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3774 - val_loss: 0.0251\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4202 - val_loss: 0.0249\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5563 - val_loss: 0.0257\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7621 - val_loss: 0.0251\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8457 - val_loss: 0.0258\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7443 - val_loss: 0.0248\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7976 - val_loss: 0.0262\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E927C7940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_120 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 1.8860 - val_loss: 0.0333\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.8095 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 23.8697 - val_loss: 0.0338\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.7587 - val_loss: 0.0332\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.0058 - val_loss: 0.0329\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.7619 - val_loss: 0.0319\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.5586 - val_loss: 0.0340\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.8514 - val_loss: 0.0329\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0561 - val_loss: 0.0358\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.3471 - val_loss: 0.0326\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6971 - val_loss: 0.0344\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6106 - val_loss: 0.0334\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2836 - val_loss: 0.0353\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0829 - val_loss: 0.0339\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8349 - val_loss: 0.0352\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7950 - val_loss: 0.0363\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6348 - val_loss: 0.0363\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4328 - val_loss: 0.0376\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2600 - val_loss: 0.0372\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2018 - val_loss: 0.0391\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1828 - val_loss: 0.0378\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1DC7B1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_121 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 427ms/step - loss: 2.6382 - val_loss: 0.0526\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.9760 - val_loss: 0.0320\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 16.6397 - val_loss: 0.0379\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.1230 - val_loss: 0.0338\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1478 - val_loss: 0.0371\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.0047 - val_loss: 0.0369\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.3199 - val_loss: 0.0373\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.6495 - val_loss: 0.0385\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2777 - val_loss: 0.0392\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.4485 - val_loss: 0.0403\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7121 - val_loss: 0.0390\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2400 - val_loss: 0.0430\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3646 - val_loss: 0.0409\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7438 - val_loss: 0.0423\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3408 - val_loss: 0.0423\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1969 - val_loss: 0.0431\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2110 - val_loss: 0.0425\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E81C9C5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_122 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 363ms/step - loss: 1.4457 - val_loss: 0.0432\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.4407 - val_loss: 0.0319\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 19.2179 - val_loss: 0.0333\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 14.7052 - val_loss: 0.0311\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.4649 - val_loss: 0.0335\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.6200 - val_loss: 0.0318\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.3874 - val_loss: 0.0334\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.9513 - val_loss: 0.0308\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.7632 - val_loss: 0.0361\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.8517 - val_loss: 0.0311\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.9699 - val_loss: 0.0347\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.5523 - val_loss: 0.0311\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3415 - val_loss: 0.0343\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 2.4248 - val_loss: 0.0321\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.6593 - val_loss: 0.0317\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.3270 - val_loss: 0.0335\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6287 - val_loss: 0.0315\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4472 - val_loss: 0.0334\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4869 - val_loss: 0.0312\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5505 - val_loss: 0.0327\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4380 - val_loss: 0.0307\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3135 - val_loss: 0.0322\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2045 - val_loss: 0.0307\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1474 - val_loss: 0.0325\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1130 - val_loss: 0.0303\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1105 - val_loss: 0.0334\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1363 - val_loss: 0.0291\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2103 - val_loss: 0.0360\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3410 - val_loss: 0.0273\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5366 - val_loss: 0.0373\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7313 - val_loss: 0.0281\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9152 - val_loss: 0.0323\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9626 - val_loss: 0.0318\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1815 - val_loss: 0.0287\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1163 - val_loss: 0.0349\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1126 - val_loss: 0.0272\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1817 - val_loss: 0.0347\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6046 - val_loss: 0.0314\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5243 - val_loss: 0.0270\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4090 - val_loss: 0.0388\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EF2B47D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_123 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.1059 - val_loss: 0.0417\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.0104 - val_loss: 0.0311\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 18.5188 - val_loss: 0.0330\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.5933 - val_loss: 0.0345\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.2599 - val_loss: 0.0335\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.7857 - val_loss: 0.0341\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.6740 - val_loss: 0.0335\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.4144 - val_loss: 0.0340\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7253 - val_loss: 0.0322\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9362 - val_loss: 0.0340\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6974 - val_loss: 0.0330\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.2514 - val_loss: 0.0351\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9537 - val_loss: 0.0335\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8884 - val_loss: 0.0360\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7714 - val_loss: 0.0338\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7905 - val_loss: 0.0367\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 0.7410 - val_loss: 0.0347\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EC00BD9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_124 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 1.8918 - val_loss: 0.0329\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.1037 - val_loss: 0.0383\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 18.7196 - val_loss: 0.0302\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.2188 - val_loss: 0.0304\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.9391 - val_loss: 0.0323\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.5541 - val_loss: 0.0317\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.0249 - val_loss: 0.0323\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4084 - val_loss: 0.0298\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8190 - val_loss: 0.0305\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0074 - val_loss: 0.0291\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5494 - val_loss: 0.0309\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4005 - val_loss: 0.0282\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7749 - val_loss: 0.0300\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5399 - val_loss: 0.0274\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6301 - val_loss: 0.0284\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7530 - val_loss: 0.0277\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7921 - val_loss: 0.0271\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7862 - val_loss: 0.0277\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8579 - val_loss: 0.0261\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1149 - val_loss: 0.0284\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1986 - val_loss: 0.0261\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3151 - val_loss: 0.0280\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3213 - val_loss: 0.0262\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1169 - val_loss: 0.0261\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9654 - val_loss: 0.0267\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7780 - val_loss: 0.0255\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.537 - 0s 26ms/step - loss: 0.5378 - val_loss: 0.0266\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3584 - val_loss: 0.0252\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2692 - val_loss: 0.0266\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2188 - val_loss: 0.0250\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2076 - val_loss: 0.0267\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2159 - val_loss: 0.0247\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2405 - val_loss: 0.0272\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2689 - val_loss: 0.0245\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3283 - val_loss: 0.0277\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3866 - val_loss: 0.0246\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5631 - val_loss: 0.0270\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7679 - val_loss: 0.0248\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9602 - val_loss: 0.0262\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3513 - val_loss: 0.0249\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8A9584C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_125 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 2.0636 - val_loss: 0.0371\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.8035 - val_loss: 0.0344\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 15.3067 - val_loss: 0.0314\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 14.6139 - val_loss: 0.0310\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.9380 - val_loss: 0.0294\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.3866 - val_loss: 0.0291\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.8298 - val_loss: 0.0272\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.0133 - val_loss: 0.0273\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.6747 - val_loss: 0.0260\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.0792 - val_loss: 0.0278\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.3565 - val_loss: 0.0259\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6285 - val_loss: 0.0264\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7798 - val_loss: 0.0254\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4915 - val_loss: 0.0255\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3514 - val_loss: 0.0252\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2517 - val_loss: 0.0252\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2564 - val_loss: 0.0253\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3924 - val_loss: 0.0259\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4877 - val_loss: 0.0258\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5691 - val_loss: 0.0278\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.5087 - val_loss: 0.0270\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.4424 - val_loss: 0.0296\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3768 - val_loss: 0.0287\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3177 - val_loss: 0.0307\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2807 - val_loss: 0.0317\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2701 - val_loss: 0.0323\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2769 - val_loss: 0.0344\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2971 - val_loss: 0.0351\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3624 - val_loss: 0.0360\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5408 - val_loss: 0.0411\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8467 - val_loss: 0.0366\n",
      "Epoch 00031: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA6BD6040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_126 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 2.1433 - val_loss: 0.0413\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.2866 - val_loss: 0.0341\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18.1030 - val_loss: 0.0392\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.9209 - val_loss: 0.0399\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.9211 - val_loss: 0.0424\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.8188 - val_loss: 0.0454\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.7887 - val_loss: 0.0472\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.7120 - val_loss: 0.0479\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.7955 - val_loss: 0.0523\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9728 - val_loss: 0.0527\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7600 - val_loss: 0.0578\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4096 - val_loss: 0.0589\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5087 - val_loss: 0.0650\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3579 - val_loss: 0.0673\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0116 - val_loss: 0.0719\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8437 - val_loss: 0.0718\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5751 - val_loss: 0.0803\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA105EAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_127 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 233ms/step - loss: 1.8766 - val_loss: 0.0352\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8.6792 - val_loss: 0.0365\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 32.7824 - val_loss: 0.0337\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.9356 - val_loss: 0.0334\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.0766 - val_loss: 0.0334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.2299 - val_loss: 0.0329\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.7723 - val_loss: 0.0307\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.8570 - val_loss: 0.0326\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.2477 - val_loss: 0.0321\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1782 - val_loss: 0.0330\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2724 - val_loss: 0.0334\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.4107 - val_loss: 0.0334\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0009 - val_loss: 0.0331\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7194 - val_loss: 0.0335\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4526 - val_loss: 0.0333\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2988 - val_loss: 0.0339\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2205 - val_loss: 0.0330\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.1729 - val_loss: 0.0339\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1665 - val_loss: 0.0326\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2225 - val_loss: 0.0342\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3442 - val_loss: 0.0320\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5231 - val_loss: 0.0355\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED52DB550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_128 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 291ms/step - loss: 1.8137 - val_loss: 0.0316\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.2228 - val_loss: 0.0361\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.1486 - val_loss: 0.0306\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.7558 - val_loss: 0.0335\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 12.5506 - val_loss: 0.0315\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.1139 - val_loss: 0.0332\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.7526 - val_loss: 0.0295\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.3045 - val_loss: 0.0295\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.7890 - val_loss: 0.0281\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.2700 - val_loss: 0.0276\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.8831 - val_loss: 0.0265\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.1243 - val_loss: 0.0267\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7745 - val_loss: 0.0262\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4784 - val_loss: 0.0262\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1688 - val_loss: 0.0262\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8506 - val_loss: 0.0260\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6719 - val_loss: 0.0260\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5391 - val_loss: 0.0259\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3750 - val_loss: 0.0260\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3110 - val_loss: 0.0262\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2689 - val_loss: 0.0262\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2462 - val_loss: 0.0267\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2514 - val_loss: 0.0263\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2898 - val_loss: 0.0277\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3847 - val_loss: 0.0265\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5672 - val_loss: 0.0289\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8704 - val_loss: 0.0269\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0888 - val_loss: 0.0284\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3975 - val_loss: 0.0272\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2809 - val_loss: 0.0279\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0850 - val_loss: 0.0284\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9614 - val_loss: 0.0268\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7931 - val_loss: 0.0313\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F31106C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_129 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 1.6349 - val_loss: 0.0427\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 25.8790 - val_loss: 0.0309\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 14.3511 - val_loss: 0.0337\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.9008 - val_loss: 0.0310\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 10.3193 - val_loss: 0.0343\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.0729 - val_loss: 0.0326\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.5885 - val_loss: 0.0353\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.2007 - val_loss: 0.0328\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3565 - val_loss: 0.0357\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7582 - val_loss: 0.0341\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3750 - val_loss: 0.0372\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6539 - val_loss: 0.0353\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3432 - val_loss: 0.0381\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9611 - val_loss: 0.0369\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7218 - val_loss: 0.0389\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5899 - val_loss: 0.0385\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8090 - val_loss: 0.0394\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFFFCAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_130 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 1.3662 - val_loss: 0.0358\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.4002 - val_loss: 0.0370\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.5557 - val_loss: 0.0333\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 16.3802 - val_loss: 0.0355\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.8708 - val_loss: 0.0342\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9010 - val_loss: 0.0336\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6137 - val_loss: 0.0337\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.1195 - val_loss: 0.0331\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.9364 - val_loss: 0.0341\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2688 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.7638 - val_loss: 0.0352\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.7257 - val_loss: 0.0317\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7910 - val_loss: 0.0347\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.7556 - val_loss: 0.0345\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5881 - val_loss: 0.0337\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0041 - val_loss: 0.0352\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5528 - val_loss: 0.0341\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3036 - val_loss: 0.0353\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1469 - val_loss: 0.0343\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0983 - val_loss: 0.0352\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0872 - val_loss: 0.0344\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1104 - val_loss: 0.0349\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1778 - val_loss: 0.0347\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3005 - val_loss: 0.0342\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4889 - val_loss: 0.0367\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5909 - val_loss: 0.0320\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6420 - val_loss: 0.0408\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E820F4310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_131 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 244ms/step - loss: 2.1318 - val_loss: 0.0459\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.4110 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.4176 - val_loss: 0.0292\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 10.3361 - val_loss: 0.0279\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1865 - val_loss: 0.0287\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.2550 - val_loss: 0.0286\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.0159 - val_loss: 0.0287\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.6643 - val_loss: 0.0279\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.6895 - val_loss: 0.0276\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.7230 - val_loss: 0.0274\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.1967 - val_loss: 0.0270\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0337 - val_loss: 0.0273\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.0703 - val_loss: 0.0265\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.4183 - val_loss: 0.0271\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7542 - val_loss: 0.0271\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1850 - val_loss: 0.0265\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6317 - val_loss: 0.0265\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2921 - val_loss: 0.0264\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2308 - val_loss: 0.0262\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2433 - val_loss: 0.0260\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2836 - val_loss: 0.0258\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3299 - val_loss: 0.0257\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3632 - val_loss: 0.0256\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3896 - val_loss: 0.0255\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4009 - val_loss: 0.0256\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3776 - val_loss: 0.0256\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3461 - val_loss: 0.0256\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3521 - val_loss: 0.0266\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3826 - val_loss: 0.0256\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4538 - val_loss: 0.0280\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6052 - val_loss: 0.0260\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8378 - val_loss: 0.0292\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1824 - val_loss: 0.0260\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1799 - val_loss: 0.0294\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3102 - val_loss: 0.0258\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4007 - val_loss: 0.0283\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0117 - val_loss: 0.0273\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3434 - val_loss: 0.0263\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.8226 - val_loss: 0.0264\n",
      "Epoch 00039: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F39B8A160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_132 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 3.0731 - val_loss: 0.0457\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 11.2398 - val_loss: 0.0333\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6434 - val_loss: 0.0348\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.9538 - val_loss: 0.0347\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.4596 - val_loss: 0.0358\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7928 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0135 - val_loss: 0.0368\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.9713 - val_loss: 0.0359\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.1383 - val_loss: 0.0375\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.4209 - val_loss: 0.0381\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.3575 - val_loss: 0.0382\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.8271 - val_loss: 0.0386\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0973 - val_loss: 0.0368\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.7555 - val_loss: 0.0412\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.9995 - val_loss: 0.0382\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 2.3516 - val_loss: 0.0415\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8599 - val_loss: 0.0386\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E82FB3EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_133 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 950ms/step - loss: 1.6890 - val_loss: 0.0411\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7807 - val_loss: 0.0309\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.5630 - val_loss: 0.0335\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.4245 - val_loss: 0.0292\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 13.0274 - val_loss: 0.0288\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.3097 - val_loss: 0.0299\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.7009 - val_loss: 0.0279\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 4.0994 - val_loss: 0.0289\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.7282 - val_loss: 0.0274\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7463 - val_loss: 0.0276\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7903 - val_loss: 0.0279\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0592 - val_loss: 0.0267\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4529 - val_loss: 0.0278\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1645 - val_loss: 0.0260\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1433 - val_loss: 0.0277\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0481 - val_loss: 0.0260\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9122 - val_loss: 0.0271\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7599 - val_loss: 0.0259\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5918 - val_loss: 0.0263\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3911 - val_loss: 0.0258\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.2734 - val_loss: 0.0258\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2053 - val_loss: 0.0257\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1815 - val_loss: 0.0254\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2009 - val_loss: 0.0257\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2579 - val_loss: 0.0253\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3124 - val_loss: 0.0259\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3577 - val_loss: 0.0254\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3410 - val_loss: 0.0258\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3198 - val_loss: 0.0256\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3528 - val_loss: 0.0260\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3977 - val_loss: 0.0260\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4242 - val_loss: 0.0264\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4472 - val_loss: 0.0278\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5327 - val_loss: 0.0282\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7198 - val_loss: 0.0318\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7867 - val_loss: 0.0270\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8576 - val_loss: 0.0301\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1253 - val_loss: 0.0262\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8297 - val_loss: 0.0260\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5796 - val_loss: 0.0273\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E99D6A550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_134 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.7270 - val_loss: 0.0393\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.5828 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 16.4381 - val_loss: 0.0336\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 10.7593 - val_loss: 0.0332\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.6398 - val_loss: 0.0339\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 7.3947 - val_loss: 0.0307\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.6114 - val_loss: 0.0337\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.8045 - val_loss: 0.0299\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.4805 - val_loss: 0.0332\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.6552 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.7824 - val_loss: 0.0337\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8674 - val_loss: 0.0344\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3611 - val_loss: 0.0337\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3455 - val_loss: 0.0364\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5247 - val_loss: 0.0322\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.9106 - val_loss: 0.0382\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.1705 - val_loss: 0.0336\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8526 - val_loss: 0.0392\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5937 - val_loss: 0.0351\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4524 - val_loss: 0.0427\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3443 - val_loss: 0.0373\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2782 - val_loss: 0.0447\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2465 - val_loss: 0.0399\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F166FB670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_135 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 257ms/step - loss: 1.4535 - val_loss: 0.0468\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.4390 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 16.3384 - val_loss: 0.0290\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.3337 - val_loss: 0.0278\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.4503 - val_loss: 0.0276\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.3254 - val_loss: 0.0271\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.5937 - val_loss: 0.0271\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.1657 - val_loss: 0.0266\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.5053 - val_loss: 0.0266\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1896 - val_loss: 0.0262\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3218 - val_loss: 0.0264\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9959 - val_loss: 0.0261\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9838 - val_loss: 0.0266\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7343 - val_loss: 0.0260\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4480 - val_loss: 0.0265\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2581 - val_loss: 0.0259\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9761 - val_loss: 0.0265\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6959 - val_loss: 0.0259\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3585 - val_loss: 0.0263\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2112 - val_loss: 0.0260\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1965 - val_loss: 0.0262\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2650 - val_loss: 0.0259\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3481 - val_loss: 0.0261\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3642 - val_loss: 0.0259\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3439 - val_loss: 0.0261\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3059 - val_loss: 0.0258\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3076 - val_loss: 0.0260\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2908 - val_loss: 0.0259\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3054 - val_loss: 0.0260\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4970 - val_loss: 0.0261\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8694 - val_loss: 0.0270\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3896 - val_loss: 0.0272\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3267 - val_loss: 0.0280\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7095 - val_loss: 0.0323\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0265 - val_loss: 0.0294\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7454 - val_loss: 0.0313\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8617 - val_loss: 0.0282\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5638 - val_loss: 0.0266\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.6759 - val_loss: 0.0298\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.6012 - val_loss: 0.0313\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F3937F160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_136 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 1.5920 - val_loss: 0.0416\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.2845 - val_loss: 0.0319\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.9046 - val_loss: 0.0300\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.4999 - val_loss: 0.0294\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.4231 - val_loss: 0.0279\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.5470 - val_loss: 0.0271\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.4352 - val_loss: 0.0286\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.8855 - val_loss: 0.0289\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.6041 - val_loss: 0.0291\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.3067 - val_loss: 0.0290\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5767 - val_loss: 0.0301\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.7523 - val_loss: 0.0281\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.9160 - val_loss: 0.0298\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3312 - val_loss: 0.0285\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7451 - val_loss: 0.0290\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6148 - val_loss: 0.0300\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7351 - val_loss: 0.0283\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8267 - val_loss: 0.0312\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8181 - val_loss: 0.0288\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6182 - val_loss: 0.0319\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4183 - val_loss: 0.0290\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECECECB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_137 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.6830 - val_loss: 0.0493\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.6060 - val_loss: 0.0283\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.1201 - val_loss: 0.0305\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.2506 - val_loss: 0.0293\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.0676 - val_loss: 0.0288\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.4953 - val_loss: 0.0288\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.8266 - val_loss: 0.0279\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.1125 - val_loss: 0.0284\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.8428 - val_loss: 0.0279\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.3029 - val_loss: 0.0284\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.5231 - val_loss: 0.0290\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3428 - val_loss: 0.0280\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4715 - val_loss: 0.0289\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.8024 - val_loss: 0.0281\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2973 - val_loss: 0.0290\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9112 - val_loss: 0.0285\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6780 - val_loss: 0.0287\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4574 - val_loss: 0.0293\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3831 - val_loss: 0.0286\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.4036 - val_loss: 0.0298\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4154 - val_loss: 0.0295\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4745 - val_loss: 0.0292\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE0A653A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_138 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 388ms/step - loss: 2.3638 - val_loss: 0.0359\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.9338 - val_loss: 0.0328\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 18.9981 - val_loss: 0.0313\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.8141 - val_loss: 0.0287\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.5187 - val_loss: 0.0287\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.3184 - val_loss: 0.0278\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.5518 - val_loss: 0.0285\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.7343 - val_loss: 0.0274\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.2496 - val_loss: 0.0282\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.5363 - val_loss: 0.0275\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0553 - val_loss: 0.0291\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4971 - val_loss: 0.0276\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4404 - val_loss: 0.0297\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.2787 - val_loss: 0.0277\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4376 - val_loss: 0.0292\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6547 - val_loss: 0.0297\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6785 - val_loss: 0.0276\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2393 - val_loss: 0.0311\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3061 - val_loss: 0.0277\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0182 - val_loss: 0.0309\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6201 - val_loss: 0.0280\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4389 - val_loss: 0.0307\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.570 - 0s 36ms/step - loss: 0.5703 - val_loss: 0.0277\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E9D1F3F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_139 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 343ms/step - loss: 2.4419 - val_loss: 0.0459\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.5505 - val_loss: 0.0369\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.6157 - val_loss: 0.0332\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.7104 - val_loss: 0.0333\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.5323 - val_loss: 0.0315\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 11.2487 - val_loss: 0.0319\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.9383 - val_loss: 0.0320\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.1402 - val_loss: 0.0336\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.1096 - val_loss: 0.0317\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8282 - val_loss: 0.0337\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5898 - val_loss: 0.0332\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5655 - val_loss: 0.0336\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5957 - val_loss: 0.0338\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8477 - val_loss: 0.0335\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5285 - val_loss: 0.0329\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5063 - val_loss: 0.0338\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5819 - val_loss: 0.0321\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6233 - val_loss: 0.0338\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5858 - val_loss: 0.0324\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3975 - val_loss: 0.0333\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F05666940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_140 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 2.0549 - val_loss: 0.0425\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.7909 - val_loss: 0.0405\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 20.0522 - val_loss: 0.0342\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.6228 - val_loss: 0.0338\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.7601 - val_loss: 0.0326\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.0553 - val_loss: 0.0320\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.1113 - val_loss: 0.0322\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7724 - val_loss: 0.0314\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.8464 - val_loss: 0.0311\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.5513 - val_loss: 0.0306\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1279 - val_loss: 0.0302\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2795 - val_loss: 0.0303\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8173 - val_loss: 0.0296\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6379 - val_loss: 0.0297\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6372 - val_loss: 0.0292\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5393 - val_loss: 0.0291\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3870 - val_loss: 0.0289\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2503 - val_loss: 0.0289\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1922 - val_loss: 0.0288\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1794 - val_loss: 0.0286\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2042 - val_loss: 0.0287\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2424 - val_loss: 0.0281\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3090 - val_loss: 0.0284\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3713 - val_loss: 0.0276\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5124 - val_loss: 0.0286\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5985 - val_loss: 0.0268\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6882 - val_loss: 0.0284\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8359 - val_loss: 0.0267\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0765 - val_loss: 0.0287\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1074 - val_loss: 0.0266\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1746 - val_loss: 0.0274\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.3039 - val_loss: 0.0274\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0212 - val_loss: 0.0283\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1529 - val_loss: 0.0265\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8164 - val_loss: 0.0302\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6863 - val_loss: 0.0276\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8322 - val_loss: 0.0267\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0175 - val_loss: 0.0308\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3274 - val_loss: 0.0288\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2129 - val_loss: 0.0297\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2FE25310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_141 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step - loss: 1.8062 - val_loss: 0.0425\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.2026 - val_loss: 0.0327\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.6168 - val_loss: 0.0351\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.8630 - val_loss: 0.0354\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.3651 - val_loss: 0.0334\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.2319 - val_loss: 0.0343\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.8361 - val_loss: 0.0317\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.8414 - val_loss: 0.0323\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.5013 - val_loss: 0.0314\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.2238 - val_loss: 0.0319\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.2521 - val_loss: 0.0308\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.7839 - val_loss: 0.0309\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7159 - val_loss: 0.0309\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.1290 - val_loss: 0.0303\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5104 - val_loss: 0.0309\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4454 - val_loss: 0.0295\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5809 - val_loss: 0.0313\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.7227 - val_loss: 0.0304\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9102 - val_loss: 0.0304\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9442 - val_loss: 0.0315\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8473 - val_loss: 0.0304\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7741 - val_loss: 0.0311\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7022 - val_loss: 0.0327\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7939 - val_loss: 0.0302\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7715 - val_loss: 0.0332\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6976 - val_loss: 0.0311\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4983 - val_loss: 0.0320\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3669 - val_loss: 0.0317\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2809 - val_loss: 0.0304\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2359 - val_loss: 0.0340\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2495 - val_loss: 0.0291\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3110 - val_loss: 0.0370\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4560 - val_loss: 0.0288\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6237 - val_loss: 0.0347\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7398 - val_loss: 0.0313\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7528 - val_loss: 0.0305\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7037 - val_loss: 0.0330\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7299 - val_loss: 0.0279\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9130 - val_loss: 0.0360\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9983 - val_loss: 0.0278\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F05666820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_142 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.9603 - val_loss: 0.0479\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.5259 - val_loss: 0.0371\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22.5123 - val_loss: 0.0350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.6985 - val_loss: 0.0342\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.7136 - val_loss: 0.0332\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.0533 - val_loss: 0.0328\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.3084 - val_loss: 0.0317\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 3.8422 - val_loss: 0.0331\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.5728 - val_loss: 0.0317\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2848 - val_loss: 0.0338\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0397 - val_loss: 0.0314\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3074 - val_loss: 0.0325\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8967 - val_loss: 0.0315\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6914 - val_loss: 0.0320\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5787 - val_loss: 0.0321\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5777 - val_loss: 0.0315\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5581 - val_loss: 0.0325\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4709 - val_loss: 0.0313\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3411 - val_loss: 0.0328\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2962 - val_loss: 0.0312\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2187 - val_loss: 0.0325\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1823 - val_loss: 0.0306\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1885 - val_loss: 0.0323\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2551 - val_loss: 0.0299\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4183 - val_loss: 0.0325\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6187 - val_loss: 0.0295\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6134 - val_loss: 0.0327\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5250 - val_loss: 0.0291\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3830 - val_loss: 0.0332\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3379 - val_loss: 0.0284\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3857 - val_loss: 0.0341\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6190 - val_loss: 0.0270\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1444 - val_loss: 0.0331\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6542 - val_loss: 0.0303\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8419 - val_loss: 0.0296\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2238 - val_loss: 0.0450\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0745 - val_loss: 0.0276\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2997 - val_loss: 0.0368\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9617 - val_loss: 0.0299\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0025 - val_loss: 0.0375\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F16F59550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_143 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 613ms/step - loss: 1.1880 - val_loss: 0.0560\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.5809 - val_loss: 0.0376\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 20.9323 - val_loss: 0.0425\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 17.0339 - val_loss: 0.0404\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.1243 - val_loss: 0.0403\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.3609 - val_loss: 0.0423\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.3154 - val_loss: 0.0425\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.2440 - val_loss: 0.0444\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.4109 - val_loss: 0.0441\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.0037 - val_loss: 0.0465\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.4862 - val_loss: 0.0488\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3325 - val_loss: 0.0491\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.8948 - val_loss: 0.0511\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4764 - val_loss: 0.0529\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1188 - val_loss: 0.0532\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8939 - val_loss: 0.0569\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6030 - val_loss: 0.0574\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EF0D19D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_144 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.0137 - val_loss: 0.0350\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.2358 - val_loss: 0.0408\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 21.8341 - val_loss: 0.0364\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.4527 - val_loss: 0.0371\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.8091 - val_loss: 0.0318\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.5184 - val_loss: 0.0333\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3718 - val_loss: 0.0298\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.5066 - val_loss: 0.0324\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1071 - val_loss: 0.0292\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6927 - val_loss: 0.0309\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4461 - val_loss: 0.0283\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8386 - val_loss: 0.0286\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4865 - val_loss: 0.0269\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3717 - val_loss: 0.0273\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4622 - val_loss: 0.0258\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5889 - val_loss: 0.0261\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6946 - val_loss: 0.0254\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7922 - val_loss: 0.0253\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7798 - val_loss: 0.0253\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7534 - val_loss: 0.0254\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7302 - val_loss: 0.0253\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7428 - val_loss: 0.0263\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5642 - val_loss: 0.0256\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4198 - val_loss: 0.0273\n",
      "Epoch 25/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2969 - val_loss: 0.0263\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2240 - val_loss: 0.0286\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2210 - val_loss: 0.0271\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2373 - val_loss: 0.0302\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2581 - val_loss: 0.0283\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2900 - val_loss: 0.0325\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3242 - val_loss: 0.0301\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3915 - val_loss: 0.0335\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5101 - val_loss: 0.0320\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1BBACCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_145 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 968ms/step - loss: 1.6561 - val_loss: 0.0462\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 16.1035 - val_loss: 0.0473\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.7991 - val_loss: 0.0401\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.9286 - val_loss: 0.0426\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.9584 - val_loss: 0.0414\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.3190 - val_loss: 0.0416\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.8407 - val_loss: 0.0426\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.1443 - val_loss: 0.0448\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3430 - val_loss: 0.0438\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1297 - val_loss: 0.0450\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1519 - val_loss: 0.0430\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6094 - val_loss: 0.0462\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9956 - val_loss: 0.0444\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6744 - val_loss: 0.0474\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5824 - val_loss: 0.0472\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7040 - val_loss: 0.0491\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8294 - val_loss: 0.0483\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7693 - val_loss: 0.0535\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F33C67670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_146 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 1.8528 - val_loss: 0.0447\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.7159 - val_loss: 0.0390\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 19.2127 - val_loss: 0.0413\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.8984 - val_loss: 0.0409\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2329 - val_loss: 0.0394\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.7948 - val_loss: 0.0409\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.1212 - val_loss: 0.0420\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.2430 - val_loss: 0.0401\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.5646 - val_loss: 0.0433\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6508 - val_loss: 0.0425\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 2.0985 - val_loss: 0.0438\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0441 - val_loss: 0.0465\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7662 - val_loss: 0.0457\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7193 - val_loss: 0.0488\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.9083 - val_loss: 0.0476\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0469 - val_loss: 0.0507\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8398 - val_loss: 0.0515\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F142661F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_147 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 250ms/step - loss: 3.4429 - val_loss: 0.0517\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.7915 - val_loss: 0.0362\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.8836 - val_loss: 0.0376\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.5096 - val_loss: 0.0349\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.8091 - val_loss: 0.0340\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.0121 - val_loss: 0.0355\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.2256 - val_loss: 0.0342\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.2789 - val_loss: 0.0382\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.5564 - val_loss: 0.0334\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1670 - val_loss: 0.0385\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.7977 - val_loss: 0.0369\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8375 - val_loss: 0.0415\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.5992 - val_loss: 0.0410\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3584 - val_loss: 0.0432\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0582 - val_loss: 0.0440\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9893 - val_loss: 0.0438\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9628 - val_loss: 0.0454\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7346 - val_loss: 0.0468\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4297 - val_loss: 0.0457\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3053 - val_loss: 0.0487\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2311 - val_loss: 0.0465\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1884 - val_loss: 0.0514\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1742 - val_loss: 0.0468\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1751 - val_loss: 0.0548\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F44D839D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_148 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.9671 - val_loss: 0.0359\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.6132 - val_loss: 0.0447\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 17.9146 - val_loss: 0.0360\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 15.1504 - val_loss: 0.0344\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.8643 - val_loss: 0.0332\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.1450 - val_loss: 0.0338\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.1442 - val_loss: 0.0315\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.6914 - val_loss: 0.0313\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3080 - val_loss: 0.0307\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3916 - val_loss: 0.0311\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.6340 - val_loss: 0.0287\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9877 - val_loss: 0.0305\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3130 - val_loss: 0.0273\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6635 - val_loss: 0.0286\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3918 - val_loss: 0.0271\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2901 - val_loss: 0.0278\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2503 - val_loss: 0.0262\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2537 - val_loss: 0.0276\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2388 - val_loss: 0.0250\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2641 - val_loss: 0.0282\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.351 - 0s 53ms/step - loss: 0.3510 - val_loss: 0.0235\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5795 - val_loss: 0.0289\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8597 - val_loss: 0.0234\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9805 - val_loss: 0.0257\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1335 - val_loss: 0.0255\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2409 - val_loss: 0.0226\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3528 - val_loss: 0.0297\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5711 - val_loss: 0.0218\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.4499 - val_loss: 0.0358\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3047 - val_loss: 0.0221\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1680 - val_loss: 0.0332\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0139 - val_loss: 0.0224\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8255 - val_loss: 0.0289\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9356 - val_loss: 0.0237\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9176 - val_loss: 0.0261\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7037 - val_loss: 0.0263\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6224 - val_loss: 0.0234\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4913 - val_loss: 0.0305\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4207 - val_loss: 0.0218\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.3454 - val_loss: 0.0304\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA24B5DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_149 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 1.3102 - val_loss: 0.0551\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.1024 - val_loss: 0.0415\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 16.1300 - val_loss: 0.0453\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 14.7401 - val_loss: 0.0438\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.8718 - val_loss: 0.0444\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2472 - val_loss: 0.0451\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.1458 - val_loss: 0.0453\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.0688 - val_loss: 0.0457\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.8724 - val_loss: 0.0474\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.6112 - val_loss: 0.0461\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.3432 - val_loss: 0.0525\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5778 - val_loss: 0.0492\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0500 - val_loss: 0.0532\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7317 - val_loss: 0.0541\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5122 - val_loss: 0.0536\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3396 - val_loss: 0.0566\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2905 - val_loss: 0.0565\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2BEF0F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_150 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 1.6778 - val_loss: 0.0523\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.5167 - val_loss: 0.0505\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 21.9470 - val_loss: 0.0404\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.5356 - val_loss: 0.0386\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 9.9790 - val_loss: 0.0413\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3208 - val_loss: 0.0423\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.3654 - val_loss: 0.0443\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 6.1573 - val_loss: 0.0470\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.7390 - val_loss: 0.0468\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7692 - val_loss: 0.0514\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.0676 - val_loss: 0.0512\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3788 - val_loss: 0.0539\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.1325 - val_loss: 0.0541\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.9880 - val_loss: 0.0589\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6852 - val_loss: 0.0566\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5611 - val_loss: 0.0616\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7138 - val_loss: 0.0624\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9657 - val_loss: 0.0631\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3012 - val_loss: 0.0658\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA269F9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_151 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 1.6144 - val_loss: 0.0354\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.2698 - val_loss: 0.0465\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.8198 - val_loss: 0.0370\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.1170 - val_loss: 0.0394\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.1289 - val_loss: 0.0328\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6439 - val_loss: 0.0362\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.1682 - val_loss: 0.0318\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.7232 - val_loss: 0.0339\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.9369 - val_loss: 0.0309\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 3.3730 - val_loss: 0.0317\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.4425 - val_loss: 0.0308\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.5124 - val_loss: 0.0294\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.7291 - val_loss: 0.0302\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3789 - val_loss: 0.0287\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3435 - val_loss: 0.0298\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1192 - val_loss: 0.0260\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7361 - val_loss: 0.0309\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5377 - val_loss: 0.0248\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7056 - val_loss: 0.0332\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0729 - val_loss: 0.0226\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.1439 - val_loss: 0.0313\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8851 - val_loss: 0.0231\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6429 - val_loss: 0.0284\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4862 - val_loss: 0.0238\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3133 - val_loss: 0.0260\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2142 - val_loss: 0.0238\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1432 - val_loss: 0.0244\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1143 - val_loss: 0.0239\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1248 - val_loss: 0.0225\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1646 - val_loss: 0.0245\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2513 - val_loss: 0.0200\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3947 - val_loss: 0.0263\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5457 - val_loss: 0.0182\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5634 - val_loss: 0.0265\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6138 - val_loss: 0.0176\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7351 - val_loss: 0.0269\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0714 - val_loss: 0.0178\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.1527 - val_loss: 0.0237\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3001 - val_loss: 0.0172\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4034 - val_loss: 0.0337\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED061E670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_152 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1.6695 - val_loss: 0.0454\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 22.0666 - val_loss: 0.0433\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.8971 - val_loss: 0.0402\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.2805 - val_loss: 0.0391\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6982 - val_loss: 0.0406\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.3821 - val_loss: 0.0415\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.7904 - val_loss: 0.0400\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.5461 - val_loss: 0.0374\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.2974 - val_loss: 0.0426\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.4666 - val_loss: 0.0359\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4417 - val_loss: 0.0399\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8585 - val_loss: 0.0374\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5292 - val_loss: 0.0386\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4233 - val_loss: 0.0393\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4485 - val_loss: 0.0383\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6009 - val_loss: 0.0430\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8187 - val_loss: 0.0364\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0895 - val_loss: 0.0464\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1326 - val_loss: 0.0370\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9598 - val_loss: 0.0446\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9727 - val_loss: 0.0387\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7008 - val_loss: 0.0456\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4663 - val_loss: 0.0381\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4285 - val_loss: 0.0438\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4306 - val_loss: 0.0397\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_153 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 257ms/step - loss: 1.5739 - val_loss: 0.0589\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.9533 - val_loss: 0.0489\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.9308 - val_loss: 0.0399\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.4441 - val_loss: 0.0396\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.0828 - val_loss: 0.0423\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.8782 - val_loss: 0.0444\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 5.373 - 0s 39ms/step - loss: 5.3739 - val_loss: 0.0446\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.5792 - val_loss: 0.0472\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7537 - val_loss: 0.0468\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8888 - val_loss: 0.0496\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5201 - val_loss: 0.0506\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2533 - val_loss: 0.0525\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9923 - val_loss: 0.0526\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7433 - val_loss: 0.0539\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4592 - val_loss: 0.0559\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3038 - val_loss: 0.0559\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2536 - val_loss: 0.0576\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3550 - val_loss: 0.0571\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5958 - val_loss: 0.0607\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F16F595E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_154 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.0609 - val_loss: 0.0450\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.6228 - val_loss: 0.0531\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.3984 - val_loss: 0.0431\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.6383 - val_loss: 0.0446\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.2717 - val_loss: 0.0406\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.4820 - val_loss: 0.0391\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.3210 - val_loss: 0.0369\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.5694 - val_loss: 0.0413\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.2061 - val_loss: 0.0368\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1730 - val_loss: 0.0408\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6942 - val_loss: 0.0389\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.4446 - val_loss: 0.0418\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7699 - val_loss: 0.0407\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5016 - val_loss: 0.0432\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3699 - val_loss: 0.0426\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3440 - val_loss: 0.0448\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3490 - val_loss: 0.0441\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3511 - val_loss: 0.0453\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3345 - val_loss: 0.0455\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3275 - val_loss: 0.0448\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3001 - val_loss: 0.0478\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2990 - val_loss: 0.0446\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2903 - val_loss: 0.0504\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3161 - val_loss: 0.0427\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F13ED8310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_155 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.5265 - val_loss: 0.0395\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.0539 - val_loss: 0.0503\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.5118 - val_loss: 0.0375\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.6947 - val_loss: 0.0415\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 7.5712 - val_loss: 0.0391\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.9277 - val_loss: 0.0403\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.3136 - val_loss: 0.0391\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.2962 - val_loss: 0.0405\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.3545 - val_loss: 0.0395\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9217 - val_loss: 0.0395\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1383 - val_loss: 0.0377\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7971 - val_loss: 0.0414\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1472 - val_loss: 0.0369\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6661 - val_loss: 0.0415\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3101 - val_loss: 0.0372\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1367 - val_loss: 0.0389\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0048 - val_loss: 0.0353\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8467 - val_loss: 0.0384\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8064 - val_loss: 0.0338\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6443 - val_loss: 0.0364\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5136 - val_loss: 0.0321\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4687 - val_loss: 0.0342\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5532 - val_loss: 0.0321\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6960 - val_loss: 0.0307\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7661 - val_loss: 0.0330\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6327 - val_loss: 0.0280\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5315 - val_loss: 0.0340\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4634 - val_loss: 0.0269\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.4459 - val_loss: 0.0362\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3833 - val_loss: 0.0244\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3425 - val_loss: 0.0368\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3331 - val_loss: 0.0223\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3407 - val_loss: 0.0381\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3958 - val_loss: 0.0208\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5948 - val_loss: 0.0367\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0058 - val_loss: 0.0199\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2460 - val_loss: 0.0332\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4195 - val_loss: 0.0202\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4471 - val_loss: 0.0290\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3606 - val_loss: 0.0225\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F29D91CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_156 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 3.7563 - val_loss: 0.0374\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.5781 - val_loss: 0.0536\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 16.7583 - val_loss: 0.0465\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2319 - val_loss: 0.0450\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.9953 - val_loss: 0.0453\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.7184 - val_loss: 0.0436\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.1683 - val_loss: 0.0382\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.3185 - val_loss: 0.0414\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.7914 - val_loss: 0.0387\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.6780 - val_loss: 0.0353\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9510 - val_loss: 0.0344\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6348 - val_loss: 0.0307\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2142 - val_loss: 0.0309\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0486 - val_loss: 0.0271\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.9931 - val_loss: 0.0295\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8340 - val_loss: 0.0243\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.8302 - val_loss: 0.0281\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6305 - val_loss: 0.0232\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4568 - val_loss: 0.0255\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3305 - val_loss: 0.0225\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2594 - val_loss: 0.0235\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2159 - val_loss: 0.0215\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2082 - val_loss: 0.0224\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2460 - val_loss: 0.0202\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3310 - val_loss: 0.0217\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4432 - val_loss: 0.0193\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5656 - val_loss: 0.0209\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6113 - val_loss: 0.0193\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5852 - val_loss: 0.0190\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5500 - val_loss: 0.0196\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4867 - val_loss: 0.0180\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4771 - val_loss: 0.0197\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5123 - val_loss: 0.0171\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5471 - val_loss: 0.0206\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.6182 - val_loss: 0.0170\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5978 - val_loss: 0.0194\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6592 - val_loss: 0.0165\n",
      "Epoch 38/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0022 - val_loss: 0.0180\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4010 - val_loss: 0.0161\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3653 - val_loss: 0.0223\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2ACE35E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_157 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 1.7346 - val_loss: 0.0419\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.5776 - val_loss: 0.0395\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 17.4370 - val_loss: 0.0390\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.8292 - val_loss: 0.0388\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 10.3779 - val_loss: 0.0383\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.6959 - val_loss: 0.0376\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.1205 - val_loss: 0.0386\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.9946 - val_loss: 0.0369\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.1514 - val_loss: 0.0350\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6895 - val_loss: 0.0362\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.5885 - val_loss: 0.0324\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.7045 - val_loss: 0.0346\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9207 - val_loss: 0.0314\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5083 - val_loss: 0.0325\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2996 - val_loss: 0.0298\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3572 - val_loss: 0.0308\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5338 - val_loss: 0.0275\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8154 - val_loss: 0.0296\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0367 - val_loss: 0.0274\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9018 - val_loss: 0.0284\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7087 - val_loss: 0.0267\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6191 - val_loss: 0.0260\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5141 - val_loss: 0.0264\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4414 - val_loss: 0.0253\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3887 - val_loss: 0.0251\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3651 - val_loss: 0.0239\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4483 - val_loss: 0.0250\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6192 - val_loss: 0.0228\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7937 - val_loss: 0.0232\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7623 - val_loss: 0.0227\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7181 - val_loss: 0.0209\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6310 - val_loss: 0.0246\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5609 - val_loss: 0.0194\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5292 - val_loss: 0.0256\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4775 - val_loss: 0.0180\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4634 - val_loss: 0.0275\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6893 - val_loss: 0.0168\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9085 - val_loss: 0.0279\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.0123 - val_loss: 0.0175\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0682 - val_loss: 0.0283\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E820FF0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_158 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 245ms/step - loss: 1.6633 - val_loss: 0.0625\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.4763 - val_loss: 0.0424\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 18.3740 - val_loss: 0.0409\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.7249 - val_loss: 0.0436\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.3332 - val_loss: 0.0437\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.0404 - val_loss: 0.0479\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.5233 - val_loss: 0.0486\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.4681 - val_loss: 0.0476\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3526 - val_loss: 0.0506\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2961 - val_loss: 0.0503\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8536 - val_loss: 0.0524\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7424 - val_loss: 0.0533\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.2544 - val_loss: 0.0541\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8229 - val_loss: 0.0552\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4385 - val_loss: 0.0565\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2856 - val_loss: 0.0591\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2364 - val_loss: 0.0592\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2467 - val_loss: 0.0648\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2ACE33A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_159 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 1.8285 - val_loss: 0.0366\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 16.0048 - val_loss: 0.0486\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 17.3611 - val_loss: 0.0394\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 9.3506 - val_loss: 0.0417\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.3964 - val_loss: 0.0353\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.8860 - val_loss: 0.0377\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 4.9355 - val_loss: 0.0345\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.9192 - val_loss: 0.0352\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.6734 - val_loss: 0.0378\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.4732 - val_loss: 0.0332\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8398 - val_loss: 0.0349\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9299 - val_loss: 0.0311\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7441 - val_loss: 0.0340\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3561 - val_loss: 0.0291\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3571 - val_loss: 0.0331\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1300 - val_loss: 0.0274\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1144 - val_loss: 0.0308\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8082 - val_loss: 0.0273\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4645 - val_loss: 0.0271\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2948 - val_loss: 0.0275\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2116 - val_loss: 0.0256\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1485 - val_loss: 0.0268\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1147 - val_loss: 0.0245\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0966 - val_loss: 0.0261\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0940 - val_loss: 0.0233\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1031 - val_loss: 0.0258\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1344 - val_loss: 0.0218\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2006 - val_loss: 0.0267\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3431 - val_loss: 0.0200\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5978 - val_loss: 0.0280\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0300 - val_loss: 0.0183\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4828 - val_loss: 0.0271\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.5795 - val_loss: 0.0207\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.2603 - val_loss: 0.0216\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1663 - val_loss: 0.0192\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.8303 - val_loss: 0.0182\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7930 - val_loss: 0.0207\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3667 - val_loss: 0.0185\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1840 - val_loss: 0.0211\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9682 - val_loss: 0.0178\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2F9345E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_160 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 2.1368 - val_loss: 0.0546\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 18.7203 - val_loss: 0.0359\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17.4878 - val_loss: 0.0395\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 12.1572 - val_loss: 0.0378\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.3978 - val_loss: 0.0371\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 5.9911 - val_loss: 0.0404\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.9142 - val_loss: 0.0371\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9625 - val_loss: 0.0406\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.7811 - val_loss: 0.0380\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.4664 - val_loss: 0.0449\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.5711 - val_loss: 0.0383\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7283 - val_loss: 0.0452\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2396 - val_loss: 0.0417\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6773 - val_loss: 0.0454\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4356 - val_loss: 0.0456\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3300 - val_loss: 0.0457\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2828 - val_loss: 0.0496\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F51D46160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_161 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 238ms/step - loss: 1.5876 - val_loss: 0.0370\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.1144 - val_loss: 0.0432\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 17.7869 - val_loss: 0.0310\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.3610 - val_loss: 0.0352\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.0070 - val_loss: 0.0312\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.4242 - val_loss: 0.0304\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.2017 - val_loss: 0.0306\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.9287 - val_loss: 0.0277\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3951 - val_loss: 0.0281\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5473 - val_loss: 0.0269\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0016 - val_loss: 0.0252\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.4444 - val_loss: 0.0269\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3055 - val_loss: 0.0231\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0848 - val_loss: 0.0256\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0092 - val_loss: 0.0216\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9380 - val_loss: 0.0244\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8068 - val_loss: 0.0203\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6583 - val_loss: 0.0226\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4186 - val_loss: 0.0198\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2284 - val_loss: 0.0210\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1259 - val_loss: 0.0194\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0788 - val_loss: 0.0197\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0637 - val_loss: 0.0191\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0746 - val_loss: 0.0188\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1310 - val_loss: 0.0191\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.2867 - val_loss: 0.0184\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5721 - val_loss: 0.0199\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7857 - val_loss: 0.0191\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7618 - val_loss: 0.0195\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6303 - val_loss: 0.0187\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6681 - val_loss: 0.0187\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6722 - val_loss: 0.0183\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8062 - val_loss: 0.0185\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9196 - val_loss: 0.0184\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9323 - val_loss: 0.0187\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9506 - val_loss: 0.0199\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5701 - val_loss: 0.0201\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5573 - val_loss: 0.0202\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5310 - val_loss: 0.0197\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6086 - val_loss: 0.0198\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_162 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 1.9472 - val_loss: 0.0438\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 10.3912 - val_loss: 0.0317\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 16.3347 - val_loss: 0.0330\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.0448 - val_loss: 0.0319\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.3696 - val_loss: 0.0338\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.1357 - val_loss: 0.0319\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.8087 - val_loss: 0.0324\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5766 - val_loss: 0.0330\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.0283 - val_loss: 0.0302\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3135 - val_loss: 0.0329\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3025 - val_loss: 0.0306\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0995 - val_loss: 0.0317\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5825 - val_loss: 0.0331\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0653 - val_loss: 0.0317\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5143 - val_loss: 0.0351\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.3704 - val_loss: 0.0312\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4454 - val_loss: 0.0378\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5821 - val_loss: 0.0299\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6582 - val_loss: 0.0396\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.5121 - val_loss: 0.0306\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3983 - val_loss: 0.0389\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2864 - val_loss: 0.0318\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2669 - val_loss: 0.0380\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3746 - val_loss: 0.0336\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6486 - val_loss: 0.0355\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0576 - val_loss: 0.0380\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2275 - val_loss: 0.0311\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1048 - val_loss: 0.0421\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9224 - val_loss: 0.0283\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6983 - val_loss: 0.0448\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6113 - val_loss: 0.0272\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5615 - val_loss: 0.0487\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5069 - val_loss: 0.0257\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5604 - val_loss: 0.0507\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5575 - val_loss: 0.0259\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6556 - val_loss: 0.0474\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7082 - val_loss: 0.0278\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6961 - val_loss: 0.0435\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6553 - val_loss: 0.0320\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5491 - val_loss: 0.0359\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE2318D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_163 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 2.4020 - val_loss: 0.0448\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.0342 - val_loss: 0.0280\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14.4946 - val_loss: 0.0348\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.7297 - val_loss: 0.0326\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.3743 - val_loss: 0.0317\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.3646 - val_loss: 0.0338\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.5362 - val_loss: 0.0306\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.9637 - val_loss: 0.0330\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.6332 - val_loss: 0.0343\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9367 - val_loss: 0.0326\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1390 - val_loss: 0.0359\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.6829 - val_loss: 0.0318\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2034 - val_loss: 0.0375\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7039 - val_loss: 0.0325\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4822 - val_loss: 0.0385\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4419 - val_loss: 0.0324\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5543 - val_loss: 0.0403\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EF0C83D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_164 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 254ms/step - loss: 1.2980 - val_loss: 0.0409\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 21.8341 - val_loss: 0.0320\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.7778 - val_loss: 0.0264\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 11.5960 - val_loss: 0.0272\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0690 - val_loss: 0.0252\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.2847 - val_loss: 0.0263\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.7479 - val_loss: 0.0277\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.3360 - val_loss: 0.0270\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8124 - val_loss: 0.0305\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.1765 - val_loss: 0.0307\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3635 - val_loss: 0.0324\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.7025 - val_loss: 0.0328\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3847 - val_loss: 0.0339\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3208 - val_loss: 0.0354\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2487 - val_loss: 0.0359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1844 - val_loss: 0.0379\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1355 - val_loss: 0.0384\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1083 - val_loss: 0.0409\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1137 - val_loss: 0.0405\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1599 - val_loss: 0.0451\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F139F38B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_165 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 1.8213 - val_loss: 0.0498\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 18.7482 - val_loss: 0.0281\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.1659 - val_loss: 0.0297\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.2393 - val_loss: 0.0283\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.8897 - val_loss: 0.0267\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.7792 - val_loss: 0.0303\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.2661 - val_loss: 0.0266\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1613 - val_loss: 0.0299\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1527 - val_loss: 0.0275\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8409 - val_loss: 0.0282\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2611 - val_loss: 0.0294\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0428 - val_loss: 0.0270\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8961 - val_loss: 0.0319\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7282 - val_loss: 0.0276\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4640 - val_loss: 0.0332\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2907 - val_loss: 0.0298\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1972 - val_loss: 0.0343\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1881 - val_loss: 0.0327\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2506 - val_loss: 0.0345\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4144 - val_loss: 0.0357\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6498 - val_loss: 0.0337\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8099 - val_loss: 0.0377\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F44CFB280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_166 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 239ms/step - loss: 1.7726 - val_loss: 0.0392\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.4397 - val_loss: 0.0271\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.8988 - val_loss: 0.0268\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8698 - val_loss: 0.0256\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.3767 - val_loss: 0.0229\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.1875 - val_loss: 0.0249\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.9955 - val_loss: 0.0229\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9198 - val_loss: 0.0255\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2032 - val_loss: 0.0228\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2763 - val_loss: 0.0274\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.2083 - val_loss: 0.0223\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.2081 - val_loss: 0.0271\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9086 - val_loss: 0.0249\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6805 - val_loss: 0.0247\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1276 - val_loss: 0.0266\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9110 - val_loss: 0.0247\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5128 - val_loss: 0.0269\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2716 - val_loss: 0.0256\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1586 - val_loss: 0.0273\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1176 - val_loss: 0.0266\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.1180 - val_loss: 0.0282\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1428 - val_loss: 0.0269\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.1944 - val_loss: 0.0292\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2922 - val_loss: 0.0260\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4363 - val_loss: 0.0316\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5544 - val_loss: 0.0245\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F17437DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_167 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 2.1860 - val_loss: 0.0310\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.0381 - val_loss: 0.0272\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 21.7332 - val_loss: 0.0259\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.0153 - val_loss: 0.0251\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.4275 - val_loss: 0.0241\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.4429 - val_loss: 0.0242\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.8028 - val_loss: 0.0224\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6694 - val_loss: 0.0237\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1524 - val_loss: 0.0231\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.9576 - val_loss: 0.0240\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2228 - val_loss: 0.0236\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8904 - val_loss: 0.0239\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6792 - val_loss: 0.0237\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6863 - val_loss: 0.0236\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7772 - val_loss: 0.0246\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7364 - val_loss: 0.0228\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5563 - val_loss: 0.0258\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4384 - val_loss: 0.0225\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4017 - val_loss: 0.0269\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4107 - val_loss: 0.0214\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4371 - val_loss: 0.0283\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5309 - val_loss: 0.0200\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6079 - val_loss: 0.0299\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7396 - val_loss: 0.0186\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0381 - val_loss: 0.0355\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2587 - val_loss: 0.0185\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3246 - val_loss: 0.0358\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.1935 - val_loss: 0.0189\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8548 - val_loss: 0.0285\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7792 - val_loss: 0.0213\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7439 - val_loss: 0.0237\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7350 - val_loss: 0.0249\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6077 - val_loss: 0.0204\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4959 - val_loss: 0.0281\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5775 - val_loss: 0.0200\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.7760 - val_loss: 0.0315\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8076 - val_loss: 0.0186\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6635 - val_loss: 0.0290\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6287 - val_loss: 0.0205\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6195 - val_loss: 0.0225\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1550B670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_168 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 294ms/step - loss: 1.8394 - val_loss: 0.0291\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 16.7457 - val_loss: 0.0296\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 16.2744 - val_loss: 0.0242\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.2985 - val_loss: 0.0241\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8539 - val_loss: 0.0206\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.6521 - val_loss: 0.0224\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.3579 - val_loss: 0.0197\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3472 - val_loss: 0.0215\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9202 - val_loss: 0.0198\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5193 - val_loss: 0.0208\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0658 - val_loss: 0.0204\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0528 - val_loss: 0.0200\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8796 - val_loss: 0.0214\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7219 - val_loss: 0.0198\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5283 - val_loss: 0.0217\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3247 - val_loss: 0.0199\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2029 - val_loss: 0.0216\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1398 - val_loss: 0.0199\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1031 - val_loss: 0.0216\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.0968 - val_loss: 0.0198\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1156 - val_loss: 0.0218\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1855 - val_loss: 0.0197\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3495 - val_loss: 0.0222\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6428 - val_loss: 0.0198\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9554 - val_loss: 0.0219\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2589 - val_loss: 0.0193\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2986 - val_loss: 0.0206\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0548 - val_loss: 0.0203\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8628 - val_loss: 0.0215\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8439 - val_loss: 0.0192\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7729 - val_loss: 0.0206\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6651 - val_loss: 0.0200\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5879 - val_loss: 0.0191\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5175 - val_loss: 0.0212\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4842 - val_loss: 0.0186\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4822 - val_loss: 0.0218\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4434 - val_loss: 0.0186\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4278 - val_loss: 0.0198\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3715 - val_loss: 0.0196\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3162 - val_loss: 0.0183\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F500EB430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_169 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.8657 - val_loss: 0.0367\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.5392 - val_loss: 0.0272\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 13.6192 - val_loss: 0.0272\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.4682 - val_loss: 0.0228\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.0559 - val_loss: 0.0252\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.4801 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.2890 - val_loss: 0.0237\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1179 - val_loss: 0.0217\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3921 - val_loss: 0.0238\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0597 - val_loss: 0.0213\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3471 - val_loss: 0.0238\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8272 - val_loss: 0.0217\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4341 - val_loss: 0.0230\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2320 - val_loss: 0.0225\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1572 - val_loss: 0.0227\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1443 - val_loss: 0.0231\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1788 - val_loss: 0.0225\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2464 - val_loss: 0.0235\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3899 - val_loss: 0.0223\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5967 - val_loss: 0.0232\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9189 - val_loss: 0.0229\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9962 - val_loss: 0.0235\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8201 - val_loss: 0.0229\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.5516 - val_loss: 0.0233\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3825 - val_loss: 0.0238\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F550EBE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_170 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 242ms/step - loss: 2.4250 - val_loss: 0.0339\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.9718 - val_loss: 0.0289\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.4475 - val_loss: 0.0276\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 15.7923 - val_loss: 0.0246\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.3761 - val_loss: 0.0229\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.5593 - val_loss: 0.0240\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3192 - val_loss: 0.0216\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.8292 - val_loss: 0.0225\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6900 - val_loss: 0.0213\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4094 - val_loss: 0.0208\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3328 - val_loss: 0.0217\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2227 - val_loss: 0.0199\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8945 - val_loss: 0.0213\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0288 - val_loss: 0.0196\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.8393 - val_loss: 0.0206\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6605 - val_loss: 0.0194\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4527 - val_loss: 0.0204\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3248 - val_loss: 0.0190\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2857 - val_loss: 0.0207\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2988 - val_loss: 0.0185\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3617 - val_loss: 0.0214\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4462 - val_loss: 0.0183\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5122 - val_loss: 0.0215\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5452 - val_loss: 0.0182\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5101 - val_loss: 0.0201\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5014 - val_loss: 0.0182\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5144 - val_loss: 0.0189\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5638 - val_loss: 0.0183\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.6735 - val_loss: 0.0183\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5887 - val_loss: 0.0189\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5669 - val_loss: 0.0181\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5589 - val_loss: 0.0193\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5738 - val_loss: 0.0185\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7413 - val_loss: 0.0202\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.9886 - val_loss: 0.0199\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1469 - val_loss: 0.0204\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2462 - val_loss: 0.0192\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0720 - val_loss: 0.0185\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6336 - val_loss: 0.0183\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8366 - val_loss: 0.0196\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F622ABCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_171 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 2.2595 - val_loss: 0.0286\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.8376 - val_loss: 0.0229\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 16.4803 - val_loss: 0.0239\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.7887 - val_loss: 0.0215\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2567 - val_loss: 0.0195\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.0810 - val_loss: 0.0188\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.7505 - val_loss: 0.0181\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 3.7163 - val_loss: 0.0185\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8483 - val_loss: 0.0180\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2724 - val_loss: 0.0181\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7837 - val_loss: 0.0181\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3371 - val_loss: 0.0181\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.1411 - val_loss: 0.0182\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0717 - val_loss: 0.0186\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8307 - val_loss: 0.0184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5318 - val_loss: 0.0192\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.4000 - val_loss: 0.0185\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3696 - val_loss: 0.0201\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3575 - val_loss: 0.0190\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3416 - val_loss: 0.0208\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3311 - val_loss: 0.0200\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3182 - val_loss: 0.0208\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3254 - val_loss: 0.0221\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3491 - val_loss: 0.0205\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F009CE820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_172 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 259ms/step - loss: 2.2236 - val_loss: 0.0276\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.9309 - val_loss: 0.0225\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 24.2012 - val_loss: 0.0209\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.1467 - val_loss: 0.0212\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.3529 - val_loss: 0.0206\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.1758 - val_loss: 0.0206\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.8308 - val_loss: 0.0198\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.8861 - val_loss: 0.0207\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1992 - val_loss: 0.0198\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0525 - val_loss: 0.0200\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1264 - val_loss: 0.0198\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7358 - val_loss: 0.0199\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4574 - val_loss: 0.0197\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3156 - val_loss: 0.0196\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2233 - val_loss: 0.0195\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1739 - val_loss: 0.0195\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1643 - val_loss: 0.0193\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1764 - val_loss: 0.0192\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.2493 - val_loss: 0.0194\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4278 - val_loss: 0.0187\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7794 - val_loss: 0.0197\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0644 - val_loss: 0.0183\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0041 - val_loss: 0.0191\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7832 - val_loss: 0.0186\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5835 - val_loss: 0.0185\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4682 - val_loss: 0.0192\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4811 - val_loss: 0.0183\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6129 - val_loss: 0.0202\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6401 - val_loss: 0.0184\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5489 - val_loss: 0.0195\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4254 - val_loss: 0.0182\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3963 - val_loss: 0.0198\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4956 - val_loss: 0.0184\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6452 - val_loss: 0.0190\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0508 - val_loss: 0.0185\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.4579 - val_loss: 0.0185\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9206 - val_loss: 0.0182\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6991 - val_loss: 0.0181\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5163 - val_loss: 0.0181\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.4695 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8D5920D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_173 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 2.2818 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 24.0288 - val_loss: 0.0249\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.8200 - val_loss: 0.0254\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 120s 120s/step - loss: 11.4397 - val_loss: 0.0212\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.4271 - val_loss: 0.0212\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.0257 - val_loss: 0.0199\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.7663 - val_loss: 0.0203\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8435 - val_loss: 0.0202\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4396 - val_loss: 0.0201\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6087 - val_loss: 0.0200\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2974 - val_loss: 0.0200\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2011 - val_loss: 0.0201\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1825 - val_loss: 0.0202\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1805 - val_loss: 0.0200\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1964 - val_loss: 0.0206\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2093 - val_loss: 0.0197\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.2642 - val_loss: 0.0213\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 204ms/step - loss: 0.3822 - val_loss: 0.0190\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 0.5817 - val_loss: 0.0232\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 433ms/step - loss: 0.8065 - val_loss: 0.0182\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 0.9294 - val_loss: 0.0245\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 180ms/step - loss: 0.9300 - val_loss: 0.0184\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 240ms/step - loss: 0.7089 - val_loss: 0.0239\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 82ms/step - loss: 0.5235 - val_loss: 0.0186\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 0.4849 - val_loss: 0.0224\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.6344 - val_loss: 0.0187\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.8733 - val_loss: 0.0231\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 87ms/step - loss: 1.1065 - val_loss: 0.0184\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 164ms/step - loss: 1.2121 - val_loss: 0.0267\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 157ms/step - loss: 1.1324 - val_loss: 0.0187\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.9749 - val_loss: 0.0259\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 90ms/step - loss: 0.7977 - val_loss: 0.0199\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6239 - val_loss: 0.0237\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.6311 - val_loss: 0.0216\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 0.5967 - val_loss: 0.0230\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336DE820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_174 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 1.7392 - val_loss: 0.0298\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.0411 - val_loss: 0.0268\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.9618 - val_loss: 0.0289\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.4347 - val_loss: 0.0269\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.6051 - val_loss: 0.0281\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.4484 - val_loss: 0.0248\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.2224 - val_loss: 0.0258\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9523 - val_loss: 0.0236\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8027 - val_loss: 0.0238\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2619 - val_loss: 0.0239\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7965 - val_loss: 0.0235\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6468 - val_loss: 0.0252\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5523 - val_loss: 0.0235\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4775 - val_loss: 0.0270\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3373 - val_loss: 0.0239\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2831 - val_loss: 0.0284\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2584 - val_loss: 0.0246\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2367 - val_loss: 0.0292\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2356 - val_loss: 0.0251\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2489 - val_loss: 0.0296\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3425 - val_loss: 0.0253\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5266 - val_loss: 0.0301\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7222 - val_loss: 0.0262\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7675 - val_loss: 0.0300\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6624 - val_loss: 0.0264\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5868 - val_loss: 0.0308\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5780 - val_loss: 0.0267\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5674 - val_loss: 0.0325\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1F1B38B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_175 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 513ms/step - loss: 1.9356 - val_loss: 0.0396\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.4858 - val_loss: 0.0276\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.7041 - val_loss: 0.0258\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.0156 - val_loss: 0.0235\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.7396 - val_loss: 0.0213\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.1375 - val_loss: 0.0226\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.2655 - val_loss: 0.0214\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7314 - val_loss: 0.0236\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4555 - val_loss: 0.0228\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5282 - val_loss: 0.0250\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2125 - val_loss: 0.0241\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6990 - val_loss: 0.0261\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4254 - val_loss: 0.0256\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2790 - val_loss: 0.0271\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2199 - val_loss: 0.0276\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1831 - val_loss: 0.0285\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1646 - val_loss: 0.0293\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1533 - val_loss: 0.0297\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1559 - val_loss: 0.0309\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1739 - val_loss: 0.0315\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA86B0160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_176 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 1.2340 - val_loss: 0.0363\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.3614 - val_loss: 0.0296\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.5125 - val_loss: 0.0259\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 10.6023 - val_loss: 0.0252\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1800 - val_loss: 0.0246\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.9376 - val_loss: 0.0216\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.6205 - val_loss: 0.0222\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.0632 - val_loss: 0.0204\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.4446 - val_loss: 0.0201\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.5299 - val_loss: 0.0220\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.0355 - val_loss: 0.0193\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.7342 - val_loss: 0.0228\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1188 - val_loss: 0.0205\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9621 - val_loss: 0.0221\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7373 - val_loss: 0.0210\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4962 - val_loss: 0.0212\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2651 - val_loss: 0.0207\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1507 - val_loss: 0.0202\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1027 - val_loss: 0.0203\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.0989 - val_loss: 0.0192\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1188 - val_loss: 0.0198\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1770 - val_loss: 0.0186\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3145 - val_loss: 0.0196\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6019 - val_loss: 0.0183\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0180 - val_loss: 0.0188\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2652 - val_loss: 0.0192\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3397 - val_loss: 0.0169\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5226 - val_loss: 0.0182\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4250 - val_loss: 0.0175\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3326 - val_loss: 0.0184\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3535 - val_loss: 0.0187\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0951 - val_loss: 0.0167\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9971 - val_loss: 0.0191\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7288 - val_loss: 0.0171\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5366 - val_loss: 0.0185\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3954 - val_loss: 0.0176\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3253 - val_loss: 0.0175\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2624 - val_loss: 0.0184\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2078 - val_loss: 0.0169\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1856 - val_loss: 0.0199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F64DAB940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_177 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 332ms/step - loss: 1.9703 - val_loss: 0.0419\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 15.2574 - val_loss: 0.0264\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.1758 - val_loss: 0.0286\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.2845 - val_loss: 0.0244\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.2588 - val_loss: 0.0246\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.6553 - val_loss: 0.0232\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.2798 - val_loss: 0.0220\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.5469 - val_loss: 0.0219\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.1480 - val_loss: 0.0210\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2532 - val_loss: 0.0206\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4156 - val_loss: 0.0208\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7285 - val_loss: 0.0201\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3597 - val_loss: 0.0197\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3250 - val_loss: 0.0198\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4560 - val_loss: 0.0188\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6357 - val_loss: 0.0188\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5800 - val_loss: 0.0186\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4486 - val_loss: 0.0181\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3247 - val_loss: 0.0183\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2520 - val_loss: 0.0173\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2513 - val_loss: 0.0179\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3587 - val_loss: 0.0167\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5644 - val_loss: 0.0177\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7370 - val_loss: 0.0164\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7419 - val_loss: 0.0178\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8183 - val_loss: 0.0164\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8663 - val_loss: 0.0184\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8940 - val_loss: 0.0168\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8170 - val_loss: 0.0190\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7422 - val_loss: 0.0170\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7362 - val_loss: 0.0197\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6557 - val_loss: 0.0174\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6696 - val_loss: 0.0180\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7343 - val_loss: 0.0175\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6368 - val_loss: 0.0175\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6007 - val_loss: 0.0180\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6079 - val_loss: 0.0172\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5946 - val_loss: 0.0186\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.753 - 0s 24ms/step - loss: 0.7538 - val_loss: 0.0167\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0126 - val_loss: 0.0185\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F139F3B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_178 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 2.3834 - val_loss: 0.0333\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.1317 - val_loss: 0.0283\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 21.6921 - val_loss: 0.0267\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.9323 - val_loss: 0.0224\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.9678 - val_loss: 0.0239\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.1199 - val_loss: 0.0201\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.2230 - val_loss: 0.0227\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.0479 - val_loss: 0.0185\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.1993 - val_loss: 0.0202\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6725 - val_loss: 0.0208\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3487 - val_loss: 0.0187\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4533 - val_loss: 0.0203\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7887 - val_loss: 0.0189\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4709 - val_loss: 0.0206\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3623 - val_loss: 0.0188\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2836 - val_loss: 0.0209\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2436 - val_loss: 0.0186\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2269 - val_loss: 0.0211\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2341 - val_loss: 0.0184\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2581 - val_loss: 0.0212\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3100 - val_loss: 0.0181\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3584 - val_loss: 0.0205\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4036 - val_loss: 0.0183\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4688 - val_loss: 0.0200\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4693 - val_loss: 0.0190\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5047 - val_loss: 0.0190\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5072 - val_loss: 0.0196\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4918 - val_loss: 0.0181\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5536 - val_loss: 0.0186\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7106 - val_loss: 0.0194\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9473 - val_loss: 0.0168\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5237 - val_loss: 0.0201\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.0178 - val_loss: 0.0171\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.8403 - val_loss: 0.0198\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7014 - val_loss: 0.0188\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6791 - val_loss: 0.0187\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9913 - val_loss: 0.0166\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0274 - val_loss: 0.0188\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8767 - val_loss: 0.0167\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0266 - val_loss: 0.0181\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EF0CA5A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_179 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.8522 - val_loss: 0.0272\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.4450 - val_loss: 0.0324\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 18.0405 - val_loss: 0.0245\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.1359 - val_loss: 0.0222\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8.6799 - val_loss: 0.0209\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.3256 - val_loss: 0.0191\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.5265 - val_loss: 0.0190\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0372 - val_loss: 0.0190\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5512 - val_loss: 0.0181\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3274 - val_loss: 0.0183\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4426 - val_loss: 0.0177\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6956 - val_loss: 0.0176\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3408 - val_loss: 0.0178\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2385 - val_loss: 0.0170\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2380 - val_loss: 0.0183\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5140 - val_loss: 0.0164\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1421 - val_loss: 0.0193\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0736 - val_loss: 0.0164\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4665 - val_loss: 0.0195\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2566 - val_loss: 0.0165\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8933 - val_loss: 0.0190\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6584 - val_loss: 0.0164\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5004 - val_loss: 0.0182\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4188 - val_loss: 0.0165\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4589 - val_loss: 0.0181\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4912 - val_loss: 0.0165\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4954 - val_loss: 0.0177\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4447 - val_loss: 0.0167\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4729 - val_loss: 0.0172\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.506 - 0s 24ms/step - loss: 0.5064 - val_loss: 0.0174\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5081 - val_loss: 0.0168\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4565 - val_loss: 0.0189\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4245 - val_loss: 0.0167\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4621 - val_loss: 0.0190\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5108 - val_loss: 0.0167\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5787 - val_loss: 0.0175\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6769 - val_loss: 0.0182\n",
      "Epoch 00037: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F311B1AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_180 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 229ms/step - loss: 1.6793 - val_loss: 0.0396\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.3957 - val_loss: 0.0298\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.1987 - val_loss: 0.0281\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.3237 - val_loss: 0.0258\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.7909 - val_loss: 0.0238\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.8694 - val_loss: 0.0220\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.1114 - val_loss: 0.0224\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.4799 - val_loss: 0.0225\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.9916 - val_loss: 0.0222\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.0329 - val_loss: 0.0209\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.4770 - val_loss: 0.0227\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.0595 - val_loss: 0.0200\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6610 - val_loss: 0.0226\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0659 - val_loss: 0.0209\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4954 - val_loss: 0.0214\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2660 - val_loss: 0.0217\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2363 - val_loss: 0.0209\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3038 - val_loss: 0.0227\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3982 - val_loss: 0.0207\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4559 - val_loss: 0.0235\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4589 - val_loss: 0.0204\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4506 - val_loss: 0.0246\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4584 - val_loss: 0.0200\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4115 - val_loss: 0.0250\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3567 - val_loss: 0.0202\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2750 - val_loss: 0.0244\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2165 - val_loss: 0.0205\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2084 - val_loss: 0.0243\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2240 - val_loss: 0.0208\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2479 - val_loss: 0.0237\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4014 - val_loss: 0.0213\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6374 - val_loss: 0.0218\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8995 - val_loss: 0.0247\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1285 - val_loss: 0.0184\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1524 - val_loss: 0.0229\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3274 - val_loss: 0.0192\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6974 - val_loss: 0.0252\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9791 - val_loss: 0.0201\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.513 - 0s 16ms/step - loss: 1.5138 - val_loss: 0.0243\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.7496 - val_loss: 0.0169\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EDA61A3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_181 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 300ms/step - loss: 2.2689 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.3419 - val_loss: 0.0318\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 19.2346 - val_loss: 0.0249\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.4897 - val_loss: 0.0220\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.3689 - val_loss: 0.0197\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.1939 - val_loss: 0.0173\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.2669 - val_loss: 0.0178\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.0911 - val_loss: 0.0162\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0128 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.1458 - val_loss: 0.0159\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4098 - val_loss: 0.0159\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4635 - val_loss: 0.0159\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2843 - val_loss: 0.0157\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.5265 - val_loss: 0.0156\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9917 - val_loss: 0.0158\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4884 - val_loss: 0.0158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2578 - val_loss: 0.0160\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1319 - val_loss: 0.0162\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.0641 - val_loss: 0.0162\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0540 - val_loss: 0.0168\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0794 - val_loss: 0.0163\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1679 - val_loss: 0.0183\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3945 - val_loss: 0.0161\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7861 - val_loss: 0.0208\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1628 - val_loss: 0.0163\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2018 - val_loss: 0.0197\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9037 - val_loss: 0.0167\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5483 - val_loss: 0.0198\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3868 - val_loss: 0.0178\n",
      "Epoch 00029: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F550EBE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_182 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.4270 - val_loss: 0.0486\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.2374 - val_loss: 0.0291\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.7629 - val_loss: 0.0297\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.3674 - val_loss: 0.0258\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.0573 - val_loss: 0.0237\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.5089 - val_loss: 0.0240\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.6025 - val_loss: 0.0228\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.3917 - val_loss: 0.0236\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4170 - val_loss: 0.0234\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4630 - val_loss: 0.0221\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3064 - val_loss: 0.0268\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0651 - val_loss: 0.0225\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6966 - val_loss: 0.0287\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5070 - val_loss: 0.0241\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3715 - val_loss: 0.0292\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2751 - val_loss: 0.0263\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2076 - val_loss: 0.0300\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2138 - val_loss: 0.0283\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2602 - val_loss: 0.0310\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3169 - val_loss: 0.0293\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3838 - val_loss: 0.0328\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4738 - val_loss: 0.0295\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5478 - val_loss: 0.0342\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5219 - val_loss: 0.0306\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4699 - val_loss: 0.0335\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F64A510D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_183 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 1.6846 - val_loss: 0.0396\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.7826 - val_loss: 0.0291\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 17.1074 - val_loss: 0.0330\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.7946 - val_loss: 0.0285\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.1545 - val_loss: 0.0274\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.2703 - val_loss: 0.0264\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.9774 - val_loss: 0.0233\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.0818 - val_loss: 0.0258\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.4319 - val_loss: 0.0226\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8471 - val_loss: 0.0246\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3038 - val_loss: 0.0230\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7288 - val_loss: 0.0244\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3543 - val_loss: 0.0234\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2455 - val_loss: 0.0247\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3152 - val_loss: 0.0246\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4235 - val_loss: 0.0247\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.4168 - val_loss: 0.0266\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3457 - val_loss: 0.0249\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2826 - val_loss: 0.0281\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2342 - val_loss: 0.0252\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2150 - val_loss: 0.0292\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2535 - val_loss: 0.0257\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.3536 - val_loss: 0.0291\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5355 - val_loss: 0.0279\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F139F3550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_184 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 298ms/step - loss: 1.5412 - val_loss: 0.0344\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.1511 - val_loss: 0.0291\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 20.9171 - val_loss: 0.0349\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 8.2044 - val_loss: 0.0293\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.7507 - val_loss: 0.0276\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.1046 - val_loss: 0.0273\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.0077 - val_loss: 0.0275\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.0477 - val_loss: 0.0276\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.5866 - val_loss: 0.0300\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2238 - val_loss: 0.0300\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1962 - val_loss: 0.0334\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5319 - val_loss: 0.0335\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3819 - val_loss: 0.0383\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2966 - val_loss: 0.0371\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2394 - val_loss: 0.0421\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2272 - val_loss: 0.0418\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2346 - val_loss: 0.0459\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2768 - val_loss: 0.0462\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3390 - val_loss: 0.0503\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4442 - val_loss: 0.0483\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5006 - val_loss: 0.0573\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED48C1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_185 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.3209 - val_loss: 0.0405\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.2324 - val_loss: 0.0354\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 16.6264 - val_loss: 0.0300\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.6199 - val_loss: 0.0271\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.6229 - val_loss: 0.0292\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.6554 - val_loss: 0.0244\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.7904 - val_loss: 0.0252\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.7247 - val_loss: 0.0235\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7489 - val_loss: 0.0237\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.8289 - val_loss: 0.0243\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4572 - val_loss: 0.0231\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8828 - val_loss: 0.0253\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5634 - val_loss: 0.0233\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3500 - val_loss: 0.0264\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2137 - val_loss: 0.0237\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1569 - val_loss: 0.0276\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1327 - val_loss: 0.0240\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1715 - val_loss: 0.0304\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2513 - val_loss: 0.0229\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4022 - val_loss: 0.0354\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6053 - val_loss: 0.0214\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6986 - val_loss: 0.0396\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7402 - val_loss: 0.0222\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8742 - val_loss: 0.0406\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0541 - val_loss: 0.0234\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8986 - val_loss: 0.0393\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8000 - val_loss: 0.0274\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6559 - val_loss: 0.0386\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5915 - val_loss: 0.0344\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5863 - val_loss: 0.0392\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6511 - val_loss: 0.0388\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7653 - val_loss: 0.0406\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8956 - val_loss: 0.0425\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7333 - val_loss: 0.0438\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5412 - val_loss: 0.0501\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4276 - val_loss: 0.0425\n",
      "Epoch 00036: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F4D32AF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_186 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 283ms/step - loss: 1.9912 - val_loss: 0.0478\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.0546 - val_loss: 0.0248\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.9743 - val_loss: 0.0282\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 13.5651 - val_loss: 0.0266\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.0118 - val_loss: 0.0226\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.3598 - val_loss: 0.0233\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.1847 - val_loss: 0.0217\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.8708 - val_loss: 0.0218\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3.3918 - val_loss: 0.0200\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1.4488 - val_loss: 0.0216\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4676 - val_loss: 0.0214\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1442 - val_loss: 0.0213\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0145 - val_loss: 0.0219\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7148 - val_loss: 0.0211\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5405 - val_loss: 0.0213\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4424 - val_loss: 0.0212\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3953 - val_loss: 0.0207\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3687 - val_loss: 0.0216\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3827 - val_loss: 0.0211\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4095 - val_loss: 0.0213\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4693 - val_loss: 0.0231\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.5844 - val_loss: 0.0204\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6888 - val_loss: 0.0256\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8864 - val_loss: 0.0210\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2BA63B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_187 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.8750 - val_loss: 0.0481\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.6292 - val_loss: 0.0362\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 20.2581 - val_loss: 0.0373\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.4475 - val_loss: 0.0330\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.9449 - val_loss: 0.0301\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.1473 - val_loss: 0.0302\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.7276 - val_loss: 0.0266\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.5018 - val_loss: 0.0273\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.3393 - val_loss: 0.0263\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9153 - val_loss: 0.0257\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8596 - val_loss: 0.0256\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5510 - val_loss: 0.0262\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4249 - val_loss: 0.0252\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4269 - val_loss: 0.0264\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5448 - val_loss: 0.0244\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6821 - val_loss: 0.0269\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7777 - val_loss: 0.0241\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.9212 - val_loss: 0.0272\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8808 - val_loss: 0.0252\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7777 - val_loss: 0.0278\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5846 - val_loss: 0.0257\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4736 - val_loss: 0.0306\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4859 - val_loss: 0.0240\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7350 - val_loss: 0.0362\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9185 - val_loss: 0.0230\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8332 - val_loss: 0.0388\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6294 - val_loss: 0.0248\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5175 - val_loss: 0.0384\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4686 - val_loss: 0.0275\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4182 - val_loss: 0.0347\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4775 - val_loss: 0.0345\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7660 - val_loss: 0.0299\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0659 - val_loss: 0.0458\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9682 - val_loss: 0.0264\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9832 - val_loss: 0.0471\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9566 - val_loss: 0.0271\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8449 - val_loss: 0.0426\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8567 - val_loss: 0.0368\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0548 - val_loss: 0.0325\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.0757 - val_loss: 0.0532\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2D770D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_188 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 2.0965 - val_loss: 0.0415\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.3151 - val_loss: 0.0256\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 27.3790 - val_loss: 0.0214\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.4282 - val_loss: 0.0192\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.6732 - val_loss: 0.0175\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.3505 - val_loss: 0.0172\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.4547 - val_loss: 0.0168\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.3561 - val_loss: 0.0167\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0429 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2396 - val_loss: 0.0167\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8913 - val_loss: 0.0168\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6362 - val_loss: 0.0167\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4450 - val_loss: 0.0169\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3264 - val_loss: 0.0168\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2671 - val_loss: 0.0171\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2755 - val_loss: 0.0173\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3443 - val_loss: 0.0171\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4567 - val_loss: 0.0181\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6264 - val_loss: 0.0167\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7790 - val_loss: 0.0200\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8267 - val_loss: 0.0166\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7377 - val_loss: 0.0211\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6515 - val_loss: 0.0168\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6508 - val_loss: 0.0210\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7476 - val_loss: 0.0168\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8031 - val_loss: 0.0216\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8579 - val_loss: 0.0169\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9105 - val_loss: 0.0209\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9267 - val_loss: 0.0170\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0997 - val_loss: 0.0213\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0743 - val_loss: 0.0166\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8742 - val_loss: 0.0194\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7122 - val_loss: 0.0168\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5568 - val_loss: 0.0178\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4775 - val_loss: 0.0170\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4520 - val_loss: 0.0178\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4953 - val_loss: 0.0181\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5457 - val_loss: 0.0171\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7705 - val_loss: 0.0186\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8835 - val_loss: 0.0165\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F4A0265E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_189 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.6007 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 13.5911 - val_loss: 0.0291\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.7462 - val_loss: 0.0265\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 14.8128 - val_loss: 0.0250\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.1233 - val_loss: 0.0239\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.7211 - val_loss: 0.0230\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.2936 - val_loss: 0.0201\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.4750 - val_loss: 0.0209\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.9458 - val_loss: 0.0208\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5696 - val_loss: 0.0192\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.9708 - val_loss: 0.0213\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9278 - val_loss: 0.0186\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0781 - val_loss: 0.0209\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6235 - val_loss: 0.0184\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.3429 - val_loss: 0.0201\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.1762 - val_loss: 0.0183\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1229 - val_loss: 0.0197\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1243 - val_loss: 0.0180\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.1707 - val_loss: 0.0200\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2656 - val_loss: 0.0176\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4076 - val_loss: 0.0204\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 72ms/step - loss: 0.5173 - val_loss: 0.0176\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4845 - val_loss: 0.0200\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4656 - val_loss: 0.0177\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4006 - val_loss: 0.0187\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3998 - val_loss: 0.0177\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5209 - val_loss: 0.0180\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7182 - val_loss: 0.0172\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0181 - val_loss: 0.0174\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.0681 - val_loss: 0.0179\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9172 - val_loss: 0.0186\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7514 - val_loss: 0.0182\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.7369 - val_loss: 0.0201\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6891 - val_loss: 0.0187\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7073 - val_loss: 0.0233\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7281 - val_loss: 0.0180\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7921 - val_loss: 0.0258\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6920 - val_loss: 0.0187\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7649 - val_loss: 0.0300\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9462 - val_loss: 0.0199\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EE69A7790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_190 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.4875 - val_loss: 0.0389\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.8509 - val_loss: 0.0276\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 19.4375 - val_loss: 0.0259\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.7959 - val_loss: 0.0233\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.2400 - val_loss: 0.0211\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.1926 - val_loss: 0.0215\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.4386 - val_loss: 0.0205\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5837 - val_loss: 0.0204\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.5651 - val_loss: 0.0201\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0889 - val_loss: 0.0200\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9980 - val_loss: 0.0200\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1668 - val_loss: 0.0201\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0146 - val_loss: 0.0197\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7741 - val_loss: 0.0202\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5518 - val_loss: 0.0197\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4011 - val_loss: 0.0203\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2806 - val_loss: 0.0198\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1882 - val_loss: 0.0202\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1429 - val_loss: 0.0197\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1177 - val_loss: 0.0203\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1192 - val_loss: 0.0195\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1456 - val_loss: 0.0209\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2305 - val_loss: 0.0191\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4048 - val_loss: 0.0226\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7013 - val_loss: 0.0189\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9861 - val_loss: 0.0224\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1212 - val_loss: 0.0196\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2036 - val_loss: 0.0209\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1468 - val_loss: 0.0199\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0278 - val_loss: 0.0206\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7613 - val_loss: 0.0198\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6590 - val_loss: 0.0203\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7154 - val_loss: 0.0193\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0199 - val_loss: 0.0211\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5707 - val_loss: 0.0198\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7512 - val_loss: 0.0215\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6230 - val_loss: 0.0200\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1827 - val_loss: 0.0211\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8453 - val_loss: 0.0201\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5465 - val_loss: 0.0233\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F760980D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_191 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step - loss: 1.6537 - val_loss: 0.0303\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.8293 - val_loss: 0.0326\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.9633 - val_loss: 0.0258\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.9559 - val_loss: 0.0259\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.3943 - val_loss: 0.0266\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.5313 - val_loss: 0.0246\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.7644 - val_loss: 0.0257\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.2136 - val_loss: 0.0270\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.8343 - val_loss: 0.0271\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0677 - val_loss: 0.0292\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.0283 - val_loss: 0.0280\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2207 - val_loss: 0.0331\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8228 - val_loss: 0.0283\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6284 - val_loss: 0.0343\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4498 - val_loss: 0.0303\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3428 - val_loss: 0.0350\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2447 - val_loss: 0.0335\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2423 - val_loss: 0.0370\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3334 - val_loss: 0.0365\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5631 - val_loss: 0.0389\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9752 - val_loss: 0.0391\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F11844A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_192 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.1563 - val_loss: 0.0366\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.8802 - val_loss: 0.0308\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 22.6852 - val_loss: 0.0302\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.9728 - val_loss: 0.0249\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.2743 - val_loss: 0.0261\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.7689 - val_loss: 0.0257\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.5457 - val_loss: 0.0256\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.1950 - val_loss: 0.0245\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4311 - val_loss: 0.0248\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7543 - val_loss: 0.0245\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5022 - val_loss: 0.0263\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1693 - val_loss: 0.0249\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7458 - val_loss: 0.0275\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5203 - val_loss: 0.0262\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4008 - val_loss: 0.0286\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3329 - val_loss: 0.0274\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2651 - val_loss: 0.0298\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2137 - val_loss: 0.0281\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1930 - val_loss: 0.0305\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2132 - val_loss: 0.0286\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2649 - val_loss: 0.0313\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3386 - val_loss: 0.0284\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3924 - val_loss: 0.0330\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4980 - val_loss: 0.0263\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6499 - val_loss: 0.0373\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ED48C1AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_193 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 2.0738 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.1844 - val_loss: 0.0283\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.0359 - val_loss: 0.0252\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.1385 - val_loss: 0.0251\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.6654 - val_loss: 0.0222\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.5948 - val_loss: 0.0223\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.9420 - val_loss: 0.0219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.1842 - val_loss: 0.0215\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0024 - val_loss: 0.0217\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8317 - val_loss: 0.0212\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9441 - val_loss: 0.0207\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5622 - val_loss: 0.0217\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4708 - val_loss: 0.0203\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1741 - val_loss: 0.0218\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7799 - val_loss: 0.0203\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4473 - val_loss: 0.0213\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2579 - val_loss: 0.0203\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1864 - val_loss: 0.0207\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1865 - val_loss: 0.0206\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2180 - val_loss: 0.0203\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2663 - val_loss: 0.0209\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3449 - val_loss: 0.0198\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3536 - val_loss: 0.0213\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3819 - val_loss: 0.0195\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.4133 - val_loss: 0.0216\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.5003 - val_loss: 0.0194\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6511 - val_loss: 0.0227\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9574 - val_loss: 0.0192\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1831 - val_loss: 0.0241\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3455 - val_loss: 0.0195\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4281 - val_loss: 0.0249\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4592 - val_loss: 0.0192\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.5986 - val_loss: 0.0257\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1392 - val_loss: 0.0219\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1226 - val_loss: 0.0227\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9223 - val_loss: 0.0255\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7980 - val_loss: 0.0207\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8869 - val_loss: 0.0369\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9315 - val_loss: 0.0194\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7886 - val_loss: 0.0351\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2D770A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_194 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 1.8751 - val_loss: 0.0350\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.1054 - val_loss: 0.0235\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 20.4630 - val_loss: 0.0234\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.9178 - val_loss: 0.0226\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.1140 - val_loss: 0.0213\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.8274 - val_loss: 0.0208\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.4155 - val_loss: 0.0201\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9633 - val_loss: 0.0201\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2759 - val_loss: 0.0198\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1201 - val_loss: 0.0197\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8930 - val_loss: 0.0198\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0220 - val_loss: 0.0198\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0203 - val_loss: 0.0201\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7975 - val_loss: 0.0204\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6548 - val_loss: 0.0207\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5559 - val_loss: 0.0208\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5333 - val_loss: 0.0221\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4998 - val_loss: 0.0210\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4424 - val_loss: 0.0241\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4034 - val_loss: 0.0211\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3866 - val_loss: 0.0263\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3981 - val_loss: 0.0212\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4402 - val_loss: 0.0281\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4634 - val_loss: 0.0217\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5363 - val_loss: 0.0294\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F80C64CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_195 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 268ms/step - loss: 2.0961 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 22.0900 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 20.0817 - val_loss: 0.0264\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.4008 - val_loss: 0.0249\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.3260 - val_loss: 0.0242\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.9666 - val_loss: 0.0228\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.1571 - val_loss: 0.0228\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.0406 - val_loss: 0.0210\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.9370 - val_loss: 0.0218\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8747 - val_loss: 0.0207\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4628 - val_loss: 0.0210\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9301 - val_loss: 0.0206\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5316 - val_loss: 0.0208\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3535 - val_loss: 0.0209\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3350 - val_loss: 0.0207\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3529 - val_loss: 0.0208\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3847 - val_loss: 0.0208\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4239 - val_loss: 0.0208\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4525 - val_loss: 0.0213\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4406 - val_loss: 0.0207\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3813 - val_loss: 0.0215\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3488 - val_loss: 0.0212\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3849 - val_loss: 0.0208\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6073 - val_loss: 0.0237\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.0805 - val_loss: 0.0194\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3942 - val_loss: 0.0280\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4083 - val_loss: 0.0194\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2077 - val_loss: 0.0274\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9516 - val_loss: 0.0193\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6605 - val_loss: 0.0271\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5487 - val_loss: 0.0203\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4852 - val_loss: 0.0247\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5409 - val_loss: 0.0217\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7061 - val_loss: 0.0233\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8061 - val_loss: 0.0243\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7606 - val_loss: 0.0236\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6166 - val_loss: 0.0229\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5314 - val_loss: 0.0244\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6112 - val_loss: 0.0235\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7348 - val_loss: 0.0250\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F6FC2D430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_196 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 2.0055 - val_loss: 0.0360\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.2690 - val_loss: 0.0286\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.9954 - val_loss: 0.0221\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.9815 - val_loss: 0.0207\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.7085 - val_loss: 0.0204\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.8390 - val_loss: 0.0199\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.9637 - val_loss: 0.0194\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.4473 - val_loss: 0.0198\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.3723 - val_loss: 0.0191\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.6412 - val_loss: 0.0199\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.1931 - val_loss: 0.0189\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4572 - val_loss: 0.0194\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8526 - val_loss: 0.0190\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3938 - val_loss: 0.0192\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1538 - val_loss: 0.0191\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0733 - val_loss: 0.0191\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0452 - val_loss: 0.0193\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0370 - val_loss: 0.0192\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0369 - val_loss: 0.0194\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.0480 - val_loss: 0.0192\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.0757 - val_loss: 0.0200\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1474 - val_loss: 0.0190\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2917 - val_loss: 0.0217\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5130 - val_loss: 0.0188\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.6859 - val_loss: 0.0255\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7150 - val_loss: 0.0192\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6893 - val_loss: 0.0264\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7058 - val_loss: 0.0188\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8703 - val_loss: 0.0242\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9541 - val_loss: 0.0190\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9728 - val_loss: 0.0209\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9753 - val_loss: 0.0199\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9040 - val_loss: 0.0191\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9337 - val_loss: 0.0205\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9206 - val_loss: 0.0191\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8349 - val_loss: 0.0210\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7708 - val_loss: 0.0207\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6845 - val_loss: 0.0195\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5597 - val_loss: 0.0225\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4040 - val_loss: 0.0190\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F139F34C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_197 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6870 - val_loss: 0.0344\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.6218 - val_loss: 0.0287\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6309 - val_loss: 0.0225\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.3913 - val_loss: 0.0234\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.6456 - val_loss: 0.0224\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.5802 - val_loss: 0.0234\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6890 - val_loss: 0.0231\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.5674 - val_loss: 0.0230\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.0036 - val_loss: 0.0231\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 1.5817 - val_loss: 0.0241\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5718 - val_loss: 0.0249\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2038 - val_loss: 0.0261\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1581 - val_loss: 0.0279\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2189 - val_loss: 0.0286\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3447 - val_loss: 0.0322\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5330 - val_loss: 0.0301\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6832 - val_loss: 0.0378\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6684 - val_loss: 0.0330\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5507 - val_loss: 0.0407\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4455 - val_loss: 0.0381\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F44D83430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_198 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.2821 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.5473 - val_loss: 0.0268\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 18.5324 - val_loss: 0.0242\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.0149 - val_loss: 0.0230\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.4715 - val_loss: 0.0225\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.5955 - val_loss: 0.0210\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5639 - val_loss: 0.0219\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9727 - val_loss: 0.0206\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.2216 - val_loss: 0.0220\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.0286 - val_loss: 0.0206\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6728 - val_loss: 0.0219\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8929 - val_loss: 0.0202\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4269 - val_loss: 0.0217\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2227 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1718 - val_loss: 0.0221\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1926 - val_loss: 0.0208\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2546 - val_loss: 0.0230\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3640 - val_loss: 0.0208\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5244 - val_loss: 0.0242\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7053 - val_loss: 0.0203\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8852 - val_loss: 0.0258\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8107 - val_loss: 0.0203\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5790 - val_loss: 0.0264\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5011 - val_loss: 0.0206\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4682 - val_loss: 0.0271\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4669 - val_loss: 0.0219\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5128 - val_loss: 0.0259\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F85FD9280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_199 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 326ms/step - loss: 1.9743 - val_loss: 0.0233\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 13.2481 - val_loss: 0.0222\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 18.6824 - val_loss: 0.0212\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.7482 - val_loss: 0.0189\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.6326 - val_loss: 0.0188\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.3388 - val_loss: 0.0184\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.8829 - val_loss: 0.0182\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.7814 - val_loss: 0.0184\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7324 - val_loss: 0.0179\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3836 - val_loss: 0.0184\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9629 - val_loss: 0.0180\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7251 - val_loss: 0.0187\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5399 - val_loss: 0.0181\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3857 - val_loss: 0.0188\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3774 - val_loss: 0.0181\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3959 - val_loss: 0.0188\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3889 - val_loss: 0.0182\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3263 - val_loss: 0.0188\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3082 - val_loss: 0.0185\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2768 - val_loss: 0.0187\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2305 - val_loss: 0.0188\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1905 - val_loss: 0.0188\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1635 - val_loss: 0.0189\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1842 - val_loss: 0.0191\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F64AB8040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_200 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 317ms/step - loss: 1.5534 - val_loss: 0.0361\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.5834 - val_loss: 0.0217\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.9688 - val_loss: 0.0210\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.8972 - val_loss: 0.0191\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.6495 - val_loss: 0.0197\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.3109 - val_loss: 0.0183\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.0887 - val_loss: 0.0183\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.4885 - val_loss: 0.0178\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.9869 - val_loss: 0.0179\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.389 - 0s 24ms/step - loss: 1.3899 - val_loss: 0.0176\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7922 - val_loss: 0.0178\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4425 - val_loss: 0.0176\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2510 - val_loss: 0.0176\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1998 - val_loss: 0.0177\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2065 - val_loss: 0.0176\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2139 - val_loss: 0.0178\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2120 - val_loss: 0.0177\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2048 - val_loss: 0.0178\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2030 - val_loss: 0.0177\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2092 - val_loss: 0.0178\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2337 - val_loss: 0.0178\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3030 - val_loss: 0.0177\n",
      "Epoch 23/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4320 - val_loss: 0.0177\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6560 - val_loss: 0.0178\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8351 - val_loss: 0.0176\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8BB3A1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_201 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 1.8843 - val_loss: 0.0324\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.3670 - val_loss: 0.0216\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 17.3033 - val_loss: 0.0213\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.3283 - val_loss: 0.0183\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.2827 - val_loss: 0.0182\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.8437 - val_loss: 0.0173\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6986 - val_loss: 0.0171\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.6430 - val_loss: 0.0172\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4220 - val_loss: 0.0173\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4212 - val_loss: 0.0178\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6821 - val_loss: 0.0178\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3486 - val_loss: 0.0186\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2408 - val_loss: 0.0184\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2002 - val_loss: 0.0192\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2241 - val_loss: 0.0191\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2834 - val_loss: 0.0194\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4153 - val_loss: 0.0205\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5078 - val_loss: 0.0191\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5273 - val_loss: 0.0226\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4653 - val_loss: 0.0191\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4776 - val_loss: 0.0249\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6558 - val_loss: 0.0182\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E888823A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_202 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 1.5555 - val_loss: 0.0245\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.7163 - val_loss: 0.0262\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 15.2752 - val_loss: 0.0208\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.2532 - val_loss: 0.0206\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.4118 - val_loss: 0.0200\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.3888 - val_loss: 0.0189\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.4930 - val_loss: 0.0186\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.7967 - val_loss: 0.0189\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9890 - val_loss: 0.0188\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5706 - val_loss: 0.0186\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9891 - val_loss: 0.0190\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8212 - val_loss: 0.0187\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8230 - val_loss: 0.0188\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7472 - val_loss: 0.0188\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6180 - val_loss: 0.0190\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4858 - val_loss: 0.0191\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4083 - val_loss: 0.0196\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3963 - val_loss: 0.0192\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3942 - val_loss: 0.0209\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3826 - val_loss: 0.0190\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3831 - val_loss: 0.0229\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4734 - val_loss: 0.0187\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6057 - val_loss: 0.0254\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7969 - val_loss: 0.0187\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8523 - val_loss: 0.0262\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F5224EF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_203 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 1.6609 - val_loss: 0.0363\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.2952 - val_loss: 0.0201\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 15.7509 - val_loss: 0.0207\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.5449 - val_loss: 0.0182\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.1604 - val_loss: 0.0177\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.8736 - val_loss: 0.0179\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.4090 - val_loss: 0.0174\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.8808 - val_loss: 0.0183\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.5000 - val_loss: 0.0169\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.5696 - val_loss: 0.0180\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.2232 - val_loss: 0.0169\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4030 - val_loss: 0.0177\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8263 - val_loss: 0.0169\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6200 - val_loss: 0.0173\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6284 - val_loss: 0.0169\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6026 - val_loss: 0.0171\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4883 - val_loss: 0.0170\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4223 - val_loss: 0.0170\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4364 - val_loss: 0.0173\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4621 - val_loss: 0.0169\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4563 - val_loss: 0.0179\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4080 - val_loss: 0.0169\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3092 - val_loss: 0.0182\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2175 - val_loss: 0.0170\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1535 - val_loss: 0.0183\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1286 - val_loss: 0.0170\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F419688B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_204 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 1.8356 - val_loss: 0.0314\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.1956 - val_loss: 0.0256\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 19.1698 - val_loss: 0.0216\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 18.0678 - val_loss: 0.0206\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.5573 - val_loss: 0.0204\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.7050 - val_loss: 0.0199\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6368 - val_loss: 0.0195\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3308 - val_loss: 0.0191\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4567 - val_loss: 0.0196\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3736 - val_loss: 0.0190\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7935 - val_loss: 0.0194\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4803 - val_loss: 0.0189\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3263 - val_loss: 0.0196\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2346 - val_loss: 0.0195\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2031 - val_loss: 0.0196\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2002 - val_loss: 0.0201\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2127 - val_loss: 0.0195\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2405 - val_loss: 0.0212\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2890 - val_loss: 0.0193\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3817 - val_loss: 0.0228\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4362 - val_loss: 0.0202\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4584 - val_loss: 0.0229\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4858 - val_loss: 0.0217\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5227 - val_loss: 0.0226\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4561 - val_loss: 0.0235\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3800 - val_loss: 0.0218\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4141 - val_loss: 0.0266\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F17437EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_205 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 1.7626 - val_loss: 0.0301\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.4537 - val_loss: 0.0241\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 22.4350 - val_loss: 0.0227\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.4179 - val_loss: 0.0208\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.1025 - val_loss: 0.0229\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.3559 - val_loss: 0.0198\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.4704 - val_loss: 0.0214\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5534 - val_loss: 0.0191\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.6372 - val_loss: 0.0206\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.0483 - val_loss: 0.0199\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4000 - val_loss: 0.0215\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6267 - val_loss: 0.0214\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2934 - val_loss: 0.0225\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2179 - val_loss: 0.0235\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2680 - val_loss: 0.0239\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3710 - val_loss: 0.0263\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4584 - val_loss: 0.0258\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4979 - val_loss: 0.0290\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5318 - val_loss: 0.0283\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6100 - val_loss: 0.0323\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7004 - val_loss: 0.0296\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6622 - val_loss: 0.0377\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6333 - val_loss: 0.0326\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8BB3ADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_206 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 1.6955 - val_loss: 0.0244\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.6274 - val_loss: 0.0214\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.6411 - val_loss: 0.0198\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.4969 - val_loss: 0.0193\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.8329 - val_loss: 0.0176\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.8634 - val_loss: 0.0177\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.9025 - val_loss: 0.0168\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.6732 - val_loss: 0.0172\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.8135 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4115 - val_loss: 0.0168\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1762 - val_loss: 0.0168\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3431 - val_loss: 0.0167\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8320 - val_loss: 0.0167\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3037 - val_loss: 0.0168\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1523 - val_loss: 0.0168\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.1398 - val_loss: 0.0167\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.1960 - val_loss: 0.0168\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3012 - val_loss: 0.0167\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4575 - val_loss: 0.0170\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5918 - val_loss: 0.0167\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7007 - val_loss: 0.0172\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7428 - val_loss: 0.0167\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7761 - val_loss: 0.0181\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7641 - val_loss: 0.0168\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7684 - val_loss: 0.0196\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6267 - val_loss: 0.0170\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4935 - val_loss: 0.0189\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4834 - val_loss: 0.0169\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4937 - val_loss: 0.0186\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5473 - val_loss: 0.0169\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5951 - val_loss: 0.0186\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5690 - val_loss: 0.0169\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5287 - val_loss: 0.0178\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5798 - val_loss: 0.0174\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6747 - val_loss: 0.0176\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8181 - val_loss: 0.0179\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9560 - val_loss: 0.0168\n",
      "Epoch 00037: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F24C89A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_207 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 2.3556 - val_loss: 0.0242\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 12.7085 - val_loss: 0.0202\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 18.6608 - val_loss: 0.0196\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.5056 - val_loss: 0.0175\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.4163 - val_loss: 0.0175\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6645 - val_loss: 0.0166\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7759 - val_loss: 0.0167\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2243 - val_loss: 0.0166\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5490 - val_loss: 0.0163\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1021 - val_loss: 0.0163\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3213 - val_loss: 0.0163\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7151 - val_loss: 0.0162\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5428 - val_loss: 0.0163\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5215 - val_loss: 0.0163\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5667 - val_loss: 0.0163\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5424 - val_loss: 0.0163\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5112 - val_loss: 0.0164\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5461 - val_loss: 0.0163\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5931 - val_loss: 0.0164\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5308 - val_loss: 0.0163\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4038 - val_loss: 0.0164\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3029 - val_loss: 0.0163\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2616 - val_loss: 0.0163\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2715 - val_loss: 0.0163\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3326 - val_loss: 0.0163\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4183 - val_loss: 0.0163\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4193 - val_loss: 0.0163\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F64DAB820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_208 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 281ms/step - loss: 1.2845 - val_loss: 0.0289\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.2234 - val_loss: 0.0181\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 24.7582 - val_loss: 0.0191\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.9472 - val_loss: 0.0194\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1090 - val_loss: 0.0183\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8618 - val_loss: 0.0168\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.4930 - val_loss: 0.0171\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9155 - val_loss: 0.0166\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4582 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3961 - val_loss: 0.0167\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5950 - val_loss: 0.0169\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3019 - val_loss: 0.0168\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2078 - val_loss: 0.0169\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2052 - val_loss: 0.0170\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2384 - val_loss: 0.0169\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2885 - val_loss: 0.0172\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3532 - val_loss: 0.0171\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4219 - val_loss: 0.0172\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5133 - val_loss: 0.0178\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6170 - val_loss: 0.0165\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7610 - val_loss: 0.0203\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9589 - val_loss: 0.0161\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0099 - val_loss: 0.0224\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7563 - val_loss: 0.0162\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7705 - val_loss: 0.0239\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6276 - val_loss: 0.0173\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5061 - val_loss: 0.0213\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4499 - val_loss: 0.0190\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3701 - val_loss: 0.0193\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3066 - val_loss: 0.0219\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2666 - val_loss: 0.0174\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2896 - val_loss: 0.0277\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4123 - val_loss: 0.0162\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6397 - val_loss: 0.0378\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8877 - val_loss: 0.0165\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8963 - val_loss: 0.0322\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9490 - val_loss: 0.0167\n",
      "Epoch 00037: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA86B09D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_209 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.4786 - val_loss: 0.0250\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.9298 - val_loss: 0.0173\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 18.6512 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 11.6056 - val_loss: 0.0160\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.3844 - val_loss: 0.0161\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.3798 - val_loss: 0.0160\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.0406 - val_loss: 0.0160\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0427 - val_loss: 0.0162\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.0177 - val_loss: 0.0160\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9496 - val_loss: 0.0170\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0852 - val_loss: 0.0162\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5496 - val_loss: 0.0173\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9678 - val_loss: 0.0164\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7692 - val_loss: 0.0170\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7753 - val_loss: 0.0166\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8227 - val_loss: 0.0166\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6417 - val_loss: 0.0175\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4292 - val_loss: 0.0168\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2973 - val_loss: 0.0183\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2719 - val_loss: 0.0169\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3831 - val_loss: 0.0194\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5640 - val_loss: 0.0173\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F32899C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_210 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 235ms/step - loss: 2.5769 - val_loss: 0.0166\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.1046 - val_loss: 0.0197\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.2633 - val_loss: 0.0172\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.8415 - val_loss: 0.0164\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.7692 - val_loss: 0.0164\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.7247 - val_loss: 0.0159\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6911 - val_loss: 0.0158\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.7994 - val_loss: 0.0159\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4816 - val_loss: 0.0160\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.4320 - val_loss: 0.0162\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.5585 - val_loss: 0.0164\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7056 - val_loss: 0.0165\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4234 - val_loss: 0.0166\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3185 - val_loss: 0.0168\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3171 - val_loss: 0.0167\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3521 - val_loss: 0.0169\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3683 - val_loss: 0.0168\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3530 - val_loss: 0.0168\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.3507 - val_loss: 0.0169\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3321 - val_loss: 0.0169\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2913 - val_loss: 0.0167\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2374 - val_loss: 0.0171\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8C7E8160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_211 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 1.2977 - val_loss: 0.0201\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.2633 - val_loss: 0.0218\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.7935 - val_loss: 0.0169\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.5973 - val_loss: 0.0159\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.9159 - val_loss: 0.0158\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.8487 - val_loss: 0.0156\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.7605 - val_loss: 0.0157\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.6675 - val_loss: 0.0157\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3475 - val_loss: 0.0156\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.5703 - val_loss: 0.0160\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8635 - val_loss: 0.0158\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0041 - val_loss: 0.0160\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5734 - val_loss: 0.0159\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3518 - val_loss: 0.0161\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2921 - val_loss: 0.0158\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3728 - val_loss: 0.0161\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4885 - val_loss: 0.0157\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4849 - val_loss: 0.0161\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4098 - val_loss: 0.0157\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4500 - val_loss: 0.0162\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7464 - val_loss: 0.0156\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0818 - val_loss: 0.0157\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0797 - val_loss: 0.0159\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9628 - val_loss: 0.0156\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8034 - val_loss: 0.0165\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6846 - val_loss: 0.0162\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5584 - val_loss: 0.0168\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4851 - val_loss: 0.0163\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4416 - val_loss: 0.0164\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4533 - val_loss: 0.0163\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4234 - val_loss: 0.0158\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4284 - val_loss: 0.0157\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4131 - val_loss: 0.0156\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4333 - val_loss: 0.0157\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4430 - val_loss: 0.0156\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4512 - val_loss: 0.0160\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5908 - val_loss: 0.0157\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5825 - val_loss: 0.0159\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5108 - val_loss: 0.0159\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4556 - val_loss: 0.0166\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E88B49550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_212 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 284ms/step - loss: 1.9952 - val_loss: 0.0188\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.2927 - val_loss: 0.0172\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.6537 - val_loss: 0.0155\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 10.24 - 0s 26ms/step - loss: 10.2495 - val_loss: 0.0154\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.8553 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.6204 - val_loss: 0.0148\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.3209 - val_loss: 0.0149\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.3502 - val_loss: 0.0150\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9267 - val_loss: 0.0149\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.9262 - val_loss: 0.0151\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.1550 - val_loss: 0.0152\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.7341 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7212 - val_loss: 0.0154\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9927 - val_loss: 0.0155\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6424 - val_loss: 0.0156\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3649 - val_loss: 0.0156\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1934 - val_loss: 0.0161\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1508 - val_loss: 0.0155\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1678 - val_loss: 0.0166\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2211 - val_loss: 0.0153\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2928 - val_loss: 0.0172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EFEEB0040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_213 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 2.7643 - val_loss: 0.0256\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.9404 - val_loss: 0.0176\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.0430 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.8074 - val_loss: 0.0168\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.2985 - val_loss: 0.0163\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 7.0621 - val_loss: 0.0179\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.9266 - val_loss: 0.0154\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.7676 - val_loss: 0.0169\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8947 - val_loss: 0.0151\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6943 - val_loss: 0.0164\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0608 - val_loss: 0.0150\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7323 - val_loss: 0.0170\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9649 - val_loss: 0.0149\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.2178 - val_loss: 0.0174\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2219 - val_loss: 0.0154\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9330 - val_loss: 0.0171\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7356 - val_loss: 0.0171\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5226 - val_loss: 0.0174\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3497 - val_loss: 0.0185\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2600 - val_loss: 0.0186\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2678 - val_loss: 0.0192\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3438 - val_loss: 0.0211\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4606 - val_loss: 0.0193\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5184 - val_loss: 0.0252\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5116 - val_loss: 0.0181\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5453 - val_loss: 0.0321\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6500 - val_loss: 0.0164\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8619 - val_loss: 0.0364\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8D5920D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_214 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.8900 - val_loss: 0.0184\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.1751 - val_loss: 0.0153\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.9751 - val_loss: 0.0136\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.3176 - val_loss: 0.0134\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.4090 - val_loss: 0.0136\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.1806 - val_loss: 0.0143\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.1709 - val_loss: 0.0148\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7499 - val_loss: 0.0156\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8198 - val_loss: 0.0176\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.6583 - val_loss: 0.0181\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3708 - val_loss: 0.0216\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9318 - val_loss: 0.0203\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5529 - val_loss: 0.0231\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3227 - val_loss: 0.0229\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1770 - val_loss: 0.0250\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.118 - 0s 33ms/step - loss: 0.1182 - val_loss: 0.0259\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1106 - val_loss: 0.0262\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1379 - val_loss: 0.0295\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2016 - val_loss: 0.0260\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F547BE310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_215 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.1199 - val_loss: 0.0199\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9781 - val_loss: 0.0149\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 17.8963 - val_loss: 0.0148\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.8005 - val_loss: 0.0133\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.2834 - val_loss: 0.0132\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.6691 - val_loss: 0.0136\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.0642 - val_loss: 0.0136\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.8682 - val_loss: 0.0143\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.0750 - val_loss: 0.0141\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7997 - val_loss: 0.0157\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1104 - val_loss: 0.0138\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3502 - val_loss: 0.0182\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.5370 - val_loss: 0.0147\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0111 - val_loss: 0.0189\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7133 - val_loss: 0.0157\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6432 - val_loss: 0.0184\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6491 - val_loss: 0.0173\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6100 - val_loss: 0.0184\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4968 - val_loss: 0.0188\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3515 - val_loss: 0.0178\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F455B5040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_216 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.8015 - val_loss: 0.0207\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.5477 - val_loss: 0.0208\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 18.1940 - val_loss: 0.0142\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.4687 - val_loss: 0.0137\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6996 - val_loss: 0.0131\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7923 - val_loss: 0.0131\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.1020 - val_loss: 0.0130\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.0642 - val_loss: 0.0130\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0314 - val_loss: 0.0131\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.8416 - val_loss: 0.0131\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.9526 - val_loss: 0.0133\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4615 - val_loss: 0.0133\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5839 - val_loss: 0.0135\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1774 - val_loss: 0.0135\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9599 - val_loss: 0.0140\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7100 - val_loss: 0.0139\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4485 - val_loss: 0.0140\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3552 - val_loss: 0.0143\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3315 - val_loss: 0.0137\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3543 - val_loss: 0.0152\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3748 - val_loss: 0.0134\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4067 - val_loss: 0.0163\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E8D5920D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_217 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 258ms/step - loss: 2.4045 - val_loss: 0.0154\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.3751 - val_loss: 0.0166\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 17.2680 - val_loss: 0.0143\n",
      "Epoch 4/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 7.4981 - val_loss: 0.0136\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.3071 - val_loss: 0.0132\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 7.1022 - val_loss: 0.0131\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6824 - val_loss: 0.0132\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.3816 - val_loss: 0.0133\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.7364 - val_loss: 0.0138\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9950 - val_loss: 0.0137\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5654 - val_loss: 0.0144\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.0776 - val_loss: 0.0142\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.7945 - val_loss: 0.0149\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0004 - val_loss: 0.0149\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7076 - val_loss: 0.0157\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5037 - val_loss: 0.0152\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3088 - val_loss: 0.0160\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2715 - val_loss: 0.0157\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2419 - val_loss: 0.0167\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2843 - val_loss: 0.0162\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3306 - val_loss: 0.0173\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1DC7BAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_218 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 301ms/step - loss: 1.7582 - val_loss: 0.0159\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.3468 - val_loss: 0.0171\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.8796 - val_loss: 0.0137\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.3235 - val_loss: 0.0132\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.0679 - val_loss: 0.0130\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.7748 - val_loss: 0.0133\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.8855 - val_loss: 0.0131\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.3426 - val_loss: 0.0140\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.7143 - val_loss: 0.0133\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.6275 - val_loss: 0.0154\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5723 - val_loss: 0.0142\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8470 - val_loss: 0.0162\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5043 - val_loss: 0.0161\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2808 - val_loss: 0.0175\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1959 - val_loss: 0.0183\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1775 - val_loss: 0.0184\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1986 - val_loss: 0.0200\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2352 - val_loss: 0.0194\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3540 - val_loss: 0.0200\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5817 - val_loss: 0.0219\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F89B55280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_219 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 420ms/step - loss: 1.3447 - val_loss: 0.0278\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.7591 - val_loss: 0.0155\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.2486 - val_loss: 0.0168\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 17.4207 - val_loss: 0.0145\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.1921 - val_loss: 0.0154\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.3001 - val_loss: 0.0135\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.4917 - val_loss: 0.0150\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6889 - val_loss: 0.0139\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1958 - val_loss: 0.0150\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1241 - val_loss: 0.0141\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6119 - val_loss: 0.0158\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4902 - val_loss: 0.0148\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3653 - val_loss: 0.0161\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2386 - val_loss: 0.0159\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.158 - 0s 25ms/step - loss: 0.1588 - val_loss: 0.0166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1292 - val_loss: 0.0179\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1408 - val_loss: 0.0178\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1772 - val_loss: 0.0204\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2725 - val_loss: 0.0195\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4266 - val_loss: 0.0225\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5824 - val_loss: 0.0224\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EBAB3F940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_220 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.1639 - val_loss: 0.0182\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.3626 - val_loss: 0.0197\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.9159 - val_loss: 0.0125\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.4864 - val_loss: 0.0124\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.8630 - val_loss: 0.0133\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.0218 - val_loss: 0.0143\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.5045 - val_loss: 0.0151\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.5207 - val_loss: 0.0185\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.0656 - val_loss: 0.0163\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.6910 - val_loss: 0.0195\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5257 - val_loss: 0.0185\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9312 - val_loss: 0.0195\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3876 - val_loss: 0.0187\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1575 - val_loss: 0.0187\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0977 - val_loss: 0.0187\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7654 - val_loss: 0.0192\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5276 - val_loss: 0.0190\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3618 - val_loss: 0.0182\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3029 - val_loss: 0.0200\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EFA98B5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_221 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 494ms/step - loss: 1.5772 - val_loss: 0.0173\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.0183 - val_loss: 0.0151\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 14.5649 - val_loss: 0.0144\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14.4017 - val_loss: 0.0143\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.5904 - val_loss: 0.0123\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.4611 - val_loss: 0.0121\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.1686 - val_loss: 0.0118\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.1887 - val_loss: 0.0119\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.8472 - val_loss: 0.0125\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.2542 - val_loss: 0.0124\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.0396 - val_loss: 0.0134\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4409 - val_loss: 0.0132\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9959 - val_loss: 0.0138\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6278 - val_loss: 0.0141\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3855 - val_loss: 0.0143\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2944 - val_loss: 0.0150\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2625 - val_loss: 0.0148\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2645 - val_loss: 0.0158\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3565 - val_loss: 0.0155\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4257 - val_loss: 0.0167\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4219 - val_loss: 0.0161\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4495 - val_loss: 0.0174\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F50114310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_222 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 303ms/step - loss: 2.4049 - val_loss: 0.0249\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.8588 - val_loss: 0.0149\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 19.6162 - val_loss: 0.0132\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.8638 - val_loss: 0.0116\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.4258 - val_loss: 0.0119\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.9145 - val_loss: 0.0114\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.9154 - val_loss: 0.0119\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.4326 - val_loss: 0.0113\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2212 - val_loss: 0.0116\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.4784 - val_loss: 0.0114\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7302 - val_loss: 0.0116\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3979 - val_loss: 0.0114\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1805 - val_loss: 0.0117\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8303 - val_loss: 0.0114\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5161 - val_loss: 0.0115\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4223 - val_loss: 0.0113\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3720 - val_loss: 0.0115\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3489 - val_loss: 0.0113\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3339 - val_loss: 0.0114\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3289 - val_loss: 0.0114\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4141 - val_loss: 0.0113\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8024 - val_loss: 0.0123\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5919 - val_loss: 0.0113\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.6325 - val_loss: 0.0120\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3893 - val_loss: 0.0116\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.0625 - val_loss: 0.0121\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0603 - val_loss: 0.0124\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7901 - val_loss: 0.0117\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7049 - val_loss: 0.0143\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5953 - val_loss: 0.0116\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4616 - val_loss: 0.0157\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3551 - val_loss: 0.0118\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3167 - val_loss: 0.0168\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA13C4DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_223 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 2.6667 - val_loss: 0.0151\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.6233 - val_loss: 0.0167\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 15.5380 - val_loss: 0.0121\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.7848 - val_loss: 0.0116\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.9814 - val_loss: 0.0110\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.8795 - val_loss: 0.0111\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.1790 - val_loss: 0.0115\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.3646 - val_loss: 0.0110\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.8112 - val_loss: 0.0118\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.6991 - val_loss: 0.0109\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4550 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.4117 - val_loss: 0.0109\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.2875 - val_loss: 0.0112\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5830 - val_loss: 0.0109\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6562 - val_loss: 0.0113\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5089 - val_loss: 0.0110\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9889 - val_loss: 0.0114\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6940 - val_loss: 0.0114\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4287 - val_loss: 0.0112\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3247 - val_loss: 0.0121\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3844 - val_loss: 0.0111\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4774 - val_loss: 0.0131\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5367 - val_loss: 0.0110\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5105 - val_loss: 0.0145\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5335 - val_loss: 0.0110\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1F7E1550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_224 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 367ms/step - loss: 1.9353 - val_loss: 0.0189\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 6.0505 - val_loss: 0.0145\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 13.7237 - val_loss: 0.0151\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0059 - val_loss: 0.0121\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.9460 - val_loss: 0.0111\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.4706 - val_loss: 0.0117\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.7934 - val_loss: 0.0106\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.8090 - val_loss: 0.0103\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.4451 - val_loss: 0.0101\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.1424 - val_loss: 0.0101\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3212 - val_loss: 0.0102\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2658 - val_loss: 0.0101\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7391 - val_loss: 0.0102\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2136 - val_loss: 0.0101\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6448 - val_loss: 0.0102\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3391 - val_loss: 0.0101\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2071 - val_loss: 0.0102\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.1455 - val_loss: 0.0101\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1179 - val_loss: 0.0103\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1133 - val_loss: 0.0101\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1217 - val_loss: 0.0105\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1499 - val_loss: 0.0102\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2075 - val_loss: 0.0109\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3250 - val_loss: 0.0104\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4981 - val_loss: 0.0120\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6717 - val_loss: 0.0107\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7223 - val_loss: 0.0114\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA28F0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_225 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 1.9534 - val_loss: 0.0211\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.2897 - val_loss: 0.0129\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 22.1139 - val_loss: 0.0128\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8.2440 - val_loss: 0.0100\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.9330 - val_loss: 0.0091\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 8.6952 - val_loss: 0.0091\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.7915 - val_loss: 0.0087\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.3802 - val_loss: 0.0087\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.3066 - val_loss: 0.0088\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9697 - val_loss: 0.0088\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.1899 - val_loss: 0.0088\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2212 - val_loss: 0.0092\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7868 - val_loss: 0.0091\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2664 - val_loss: 0.0092\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9587 - val_loss: 0.0094\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6349 - val_loss: 0.0092\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3359 - val_loss: 0.0098\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3369 - val_loss: 0.0090\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4493 - val_loss: 0.0105\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5075 - val_loss: 0.0088\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4406 - val_loss: 0.0102\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4241 - val_loss: 0.0088\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4218 - val_loss: 0.0104\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8E011670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_226 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step - loss: 1.6608 - val_loss: 0.0194\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.6701 - val_loss: 0.0198\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.1162 - val_loss: 0.0109\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.2351 - val_loss: 0.0122\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.7379 - val_loss: 0.0098\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 5.8412 - val_loss: 0.0090\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.6570 - val_loss: 0.0090\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.7229 - val_loss: 0.0084\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.8929 - val_loss: 0.0087\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.9553 - val_loss: 0.0081\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.6386 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5319 - val_loss: 0.0082\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1271 - val_loss: 0.0079\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0230 - val_loss: 0.0085\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6628 - val_loss: 0.0083\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3695 - val_loss: 0.0085\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3077 - val_loss: 0.0085\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4310 - val_loss: 0.0084\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5771 - val_loss: 0.0088\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6711 - val_loss: 0.0083\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6899 - val_loss: 0.0088\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6339 - val_loss: 0.0086\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4757 - val_loss: 0.0090\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4215 - val_loss: 0.0094\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5341 - val_loss: 0.0084\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6948 - val_loss: 0.0100\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7772 - val_loss: 0.0090\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8892 - val_loss: 0.0096\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F76098F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_227 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 2.5142 - val_loss: 0.0160\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.1066 - val_loss: 0.0133\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 15.2130 - val_loss: 0.0106\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.7988 - val_loss: 0.0105\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.2963 - val_loss: 0.0093\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.1256 - val_loss: 0.0082\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.9797 - val_loss: 0.0079\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.1342 - val_loss: 0.0079\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.7713 - val_loss: 0.0079\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.0262 - val_loss: 0.0079\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5106 - val_loss: 0.0078\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2997 - val_loss: 0.0077\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2905 - val_loss: 0.0079\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0768 - val_loss: 0.0077\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8855 - val_loss: 0.0077\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5581 - val_loss: 0.0078\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5348 - val_loss: 0.0077\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4592 - val_loss: 0.0078\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4003 - val_loss: 0.0078\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4035 - val_loss: 0.0078\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4951 - val_loss: 0.0078\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6344 - val_loss: 0.0079\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5989 - val_loss: 0.0078\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5845 - val_loss: 0.0079\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5394 - val_loss: 0.0078\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4267 - val_loss: 0.0078\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3257 - val_loss: 0.0077\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3001 - val_loss: 0.0077\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3562 - val_loss: 0.0078\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5758 - val_loss: 0.0079\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F026381F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_228 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 266ms/step - loss: 1.7290 - val_loss: 0.0163\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.9436 - val_loss: 0.0154\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.4108 - val_loss: 0.0131\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.6259 - val_loss: 0.0116\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.1943 - val_loss: 0.0108\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.3732 - val_loss: 0.0100\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.3925 - val_loss: 0.0104\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.8588 - val_loss: 0.0092\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.7489 - val_loss: 0.0094\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.6873 - val_loss: 0.0091\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7808 - val_loss: 0.0096\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7297 - val_loss: 0.0098\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.3976 - val_loss: 0.0088\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7367 - val_loss: 0.0094\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7498 - val_loss: 0.0089\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.0770 - val_loss: 0.0097\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6872 - val_loss: 0.0095\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3447 - val_loss: 0.0111\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9521 - val_loss: 0.0107\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5978 - val_loss: 0.0120\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3680 - val_loss: 0.0115\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2429 - val_loss: 0.0138\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2554 - val_loss: 0.0111\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3336 - val_loss: 0.0166\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4630 - val_loss: 0.0104\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5636 - val_loss: 0.0193\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6173 - val_loss: 0.0123\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5714 - val_loss: 0.0198\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F85CEB820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_229 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.4493 - val_loss: 0.0291\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.3871 - val_loss: 0.0093\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 14.8822 - val_loss: 0.0106\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.9443 - val_loss: 0.0086\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.5156 - val_loss: 0.0080\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.2882 - val_loss: 0.0077\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.7435 - val_loss: 0.0076\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2688 - val_loss: 0.0080\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.4871 - val_loss: 0.0083\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2715 - val_loss: 0.0085\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2690 - val_loss: 0.0099\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.3442 - val_loss: 0.0098\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0610 - val_loss: 0.0108\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1838 - val_loss: 0.0108\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7533 - val_loss: 0.0125\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4128 - val_loss: 0.0126\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2529 - val_loss: 0.0130\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1992 - val_loss: 0.0146\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1828 - val_loss: 0.0135\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1718 - val_loss: 0.0170\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1772 - val_loss: 0.0147\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1920 - val_loss: 0.0199\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F9B48D550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_230 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 1.9377 - val_loss: 0.0126\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.8896 - val_loss: 0.0150\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.3882 - val_loss: 0.0107\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.1244 - val_loss: 0.0094\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.1756 - val_loss: 0.0087\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.0671 - val_loss: 0.0078\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.5825 - val_loss: 0.0077\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.0486 - val_loss: 0.0076\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.3104 - val_loss: 0.0083\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.0069 - val_loss: 0.0078\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8259 - val_loss: 0.0090\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.8546 - val_loss: 0.0090\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1446 - val_loss: 0.0099\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6769 - val_loss: 0.0105\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3373 - val_loss: 0.0111\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1635 - val_loss: 0.0120\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0811 - val_loss: 0.0124\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0741 - val_loss: 0.0139\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1240 - val_loss: 0.0132\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2754 - val_loss: 0.0170\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6341 - val_loss: 0.0122\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0470 - val_loss: 0.0227\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0698 - val_loss: 0.0115\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1550B820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_231 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 1.9643 - val_loss: 0.0222\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.0469 - val_loss: 0.0130\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.3431 - val_loss: 0.0134\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.5832 - val_loss: 0.0090\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.8751 - val_loss: 0.0090\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.5846 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.9237 - val_loss: 0.0082\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.2331 - val_loss: 0.0080\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.4589 - val_loss: 0.0081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.4929 - val_loss: 0.0084\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2466 - val_loss: 0.0091\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7061 - val_loss: 0.0086\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 1.0389 - val_loss: 0.0092\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5592 - val_loss: 0.0087\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3263 - val_loss: 0.0089\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2086 - val_loss: 0.0086\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1954 - val_loss: 0.0087\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2627 - val_loss: 0.0086\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4346 - val_loss: 0.0084\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6545 - val_loss: 0.0087\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7529 - val_loss: 0.0081\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6499 - val_loss: 0.0092\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5456 - val_loss: 0.0079\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5528 - val_loss: 0.0092\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5934 - val_loss: 0.0081\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7370 - val_loss: 0.0107\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8299 - val_loss: 0.0081\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7855 - val_loss: 0.0097\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8678 - val_loss: 0.0081\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7995 - val_loss: 0.0087\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6562 - val_loss: 0.0080\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5490 - val_loss: 0.0081\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4436 - val_loss: 0.0080\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4019 - val_loss: 0.0081\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4392 - val_loss: 0.0090\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5720 - val_loss: 0.0091\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8294 - val_loss: 0.0119\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9264 - val_loss: 0.0103\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F28AFA9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_232 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 315ms/step - loss: 1.9286 - val_loss: 0.0317\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.8527 - val_loss: 0.0104\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 15.8868 - val_loss: 0.0115\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.9573 - val_loss: 0.0083\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1465 - val_loss: 0.0088\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.9689 - val_loss: 0.0081\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.0491 - val_loss: 0.0082\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.4425 - val_loss: 0.0081\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.3059 - val_loss: 0.0080\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.9347 - val_loss: 0.0090\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2533 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6198 - val_loss: 0.0093\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.2239 - val_loss: 0.0084\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2523 - val_loss: 0.0093\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8939 - val_loss: 0.0092\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5658 - val_loss: 0.0093\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5084 - val_loss: 0.0095\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4783 - val_loss: 0.0096\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3908 - val_loss: 0.0096\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2882 - val_loss: 0.0099\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2328 - val_loss: 0.0097\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1950 - val_loss: 0.0102\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2079 - val_loss: 0.0099\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2534 - val_loss: 0.0105\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAE804280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_233 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 519ms/step - loss: 1.6192 - val_loss: 0.0166\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.7798 - val_loss: 0.0126\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 25.2277 - val_loss: 0.0101\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.0268 - val_loss: 0.0100\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.1240 - val_loss: 0.0105\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.9033 - val_loss: 0.0090\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.3557 - val_loss: 0.0105\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.7347 - val_loss: 0.0095\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.4021 - val_loss: 0.0095\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.3065 - val_loss: 0.0096\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8336 - val_loss: 0.0094\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2345 - val_loss: 0.0095\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8976 - val_loss: 0.0097\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8003 - val_loss: 0.0095\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6525 - val_loss: 0.0099\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4526 - val_loss: 0.0098\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2798 - val_loss: 0.0102\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1930 - val_loss: 0.0105\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1796 - val_loss: 0.0103\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2358 - val_loss: 0.0119\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3746 - val_loss: 0.0100\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F10DB04C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_234 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 1.9394 - val_loss: 0.0141\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 23.0189 - val_loss: 0.0091\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 23.9565 - val_loss: 0.0094\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.6702 - val_loss: 0.0083\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3221 - val_loss: 0.0082\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.6311 - val_loss: 0.0085\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.3968 - val_loss: 0.0090\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.4745 - val_loss: 0.0088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.2507 - val_loss: 0.0102\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0054 - val_loss: 0.0097\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5594 - val_loss: 0.0118\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9189 - val_loss: 0.0108\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5123 - val_loss: 0.0124\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3139 - val_loss: 0.0119\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2301 - val_loss: 0.0133\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2258 - val_loss: 0.0133\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3001 - val_loss: 0.0137\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4024 - val_loss: 0.0147\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5415 - val_loss: 0.0138\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5685 - val_loss: 0.0170\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F5ED67700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_235 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 439ms/step - loss: 3.1750 - val_loss: 0.0134\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 18.6605 - val_loss: 0.0113\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 22.0249 - val_loss: 0.0088\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.0500 - val_loss: 0.0083\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 8.6165 - val_loss: 0.0082\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.7456 - val_loss: 0.0086\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.5640 - val_loss: 0.0087\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.9580 - val_loss: 0.0099\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8974 - val_loss: 0.0101\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4959 - val_loss: 0.0121\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7919 - val_loss: 0.0123\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4149 - val_loss: 0.0142\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2960 - val_loss: 0.0146\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2779 - val_loss: 0.0163\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2688 - val_loss: 0.0172\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2939 - val_loss: 0.0183\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3399 - val_loss: 0.0201\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4539 - val_loss: 0.0209\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5761 - val_loss: 0.0226\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6289 - val_loss: 0.0233\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E88B49C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_236 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 2.1629 - val_loss: 0.0208\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.9674 - val_loss: 0.0123\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.9630 - val_loss: 0.0102\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.6806 - val_loss: 0.0085\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.8090 - val_loss: 0.0081\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.5101 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.0348 - val_loss: 0.0079\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.0755 - val_loss: 0.0081\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.311 - 0s 33ms/step - loss: 3.3119 - val_loss: 0.0083\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.1878 - val_loss: 0.0087\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.6039 - val_loss: 0.0091\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2831 - val_loss: 0.0094\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9601 - val_loss: 0.0096\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9135 - val_loss: 0.0093\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7759 - val_loss: 0.0099\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6279 - val_loss: 0.0095\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4433 - val_loss: 0.0098\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2966 - val_loss: 0.0096\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2154 - val_loss: 0.0098\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1846 - val_loss: 0.0098\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1870 - val_loss: 0.0096\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2089 - val_loss: 0.0101\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F38CCEDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_237 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 1.4796 - val_loss: 0.0098\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.7471 - val_loss: 0.0115\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 27.1671 - val_loss: 0.0084\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.4510 - val_loss: 0.0078\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.5242 - val_loss: 0.0072\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.1127 - val_loss: 0.0073\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.3355 - val_loss: 0.0074\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7121 - val_loss: 0.0074\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4772 - val_loss: 0.0077\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.6563 - val_loss: 0.0080\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7876 - val_loss: 0.0080\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8238 - val_loss: 0.0085\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0535 - val_loss: 0.0083\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5929 - val_loss: 0.0088\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4101 - val_loss: 0.0081\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4987 - val_loss: 0.0094\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6779 - val_loss: 0.0077\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7377 - val_loss: 0.0098\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7597 - val_loss: 0.0075\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7252 - val_loss: 0.0102\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F10DB0940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_238 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.2458 - val_loss: 0.0088\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.3129 - val_loss: 0.0130\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 20.3505 - val_loss: 0.0079\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.5812 - val_loss: 0.0078\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 6.333 - 0s 23ms/step - loss: 6.3335 - val_loss: 0.0070\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.3518 - val_loss: 0.0070\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.0751 - val_loss: 0.0070\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.0476 - val_loss: 0.0069\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.2529 - val_loss: 0.0069\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7739 - val_loss: 0.0069\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0136 - val_loss: 0.0068\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0160 - val_loss: 0.0068\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4260 - val_loss: 0.0069\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1082 - val_loss: 0.0069\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6575 - val_loss: 0.0069\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5544 - val_loss: 0.0069\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0961 - val_loss: 0.0069\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7908 - val_loss: 0.0070\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4392 - val_loss: 0.0070\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2523 - val_loss: 0.0072\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2162 - val_loss: 0.0070\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2598 - val_loss: 0.0075\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3636 - val_loss: 0.0071\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4738 - val_loss: 0.0079\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5110 - val_loss: 0.0073\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4635 - val_loss: 0.0083\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3909 - val_loss: 0.0074\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAE817DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_239 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 312ms/step - loss: 2.7775 - val_loss: 0.0114\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.9677 - val_loss: 0.0113\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 18.5442 - val_loss: 0.0080\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.5227 - val_loss: 0.0071\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.2886 - val_loss: 0.0068\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.8662 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8909 - val_loss: 0.0076\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.6915 - val_loss: 0.0078\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5349 - val_loss: 0.0085\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3823 - val_loss: 0.0097\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4789 - val_loss: 0.0098\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4512 - val_loss: 0.0103\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7648 - val_loss: 0.0111\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5587 - val_loss: 0.0111\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5502 - val_loss: 0.0129\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5510 - val_loss: 0.0115\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4691 - val_loss: 0.0145\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4163 - val_loss: 0.0118\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 0.3573 - val_loss: 0.0154\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2876 - val_loss: 0.0125\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB165F700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_240 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 2.3929 - val_loss: 0.0107\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 13.9856 - val_loss: 0.0080\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 20.4319 - val_loss: 0.0066\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 14.4140 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.0858 - val_loss: 0.0068\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.8929 - val_loss: 0.0077\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.5162 - val_loss: 0.0091\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.9780 - val_loss: 0.0100\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4497 - val_loss: 0.0124\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.0237 - val_loss: 0.0128\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0329 - val_loss: 0.0152\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3088 - val_loss: 0.0146\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9878 - val_loss: 0.0179\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.828 - 0s 27ms/step - loss: 0.8282 - val_loss: 0.0167\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8501 - val_loss: 0.0203\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8259 - val_loss: 0.0190\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6844 - val_loss: 0.0228\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4761 - val_loss: 0.0209\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F774C1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_241 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 2.0847 - val_loss: 0.0078\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.8762 - val_loss: 0.0082\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 22.2479 - val_loss: 0.0061\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.1109 - val_loss: 0.0061\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.0678 - val_loss: 0.0068\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.6761 - val_loss: 0.0066\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 5.7004 - val_loss: 0.0086\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 4.2904 - val_loss: 0.0086\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 4.3583 - val_loss: 0.0107\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.5048 - val_loss: 0.0094\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.4045 - val_loss: 0.0130\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8438 - val_loss: 0.0104\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.7485 - val_loss: 0.0129\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2189 - val_loss: 0.0116\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7426 - val_loss: 0.0130\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5143 - val_loss: 0.0132\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5143 - val_loss: 0.0125\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5936 - val_loss: 0.0137\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5901 - val_loss: 0.0128\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8E011DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_242 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.9794 - val_loss: 0.0094\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 16.7754 - val_loss: 0.0087\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.4942 - val_loss: 0.0062\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.3838 - val_loss: 0.0062\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.8399 - val_loss: 0.0075\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.7630 - val_loss: 0.0078\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.9672 - val_loss: 0.0086\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.2008 - val_loss: 0.0113\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.1083 - val_loss: 0.0110\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8146 - val_loss: 0.0132\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3864 - val_loss: 0.0135\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4336 - val_loss: 0.0165\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.5629 - val_loss: 0.0141\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8670 - val_loss: 0.0186\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0900 - val_loss: 0.0162\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5503 - val_loss: 0.0188\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2256 - val_loss: 0.0178\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1281 - val_loss: 0.0190\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70EA6670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_243 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 296ms/step - loss: 1.2322 - val_loss: 0.0095\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.1835 - val_loss: 0.0063\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.3305 - val_loss: 0.0062\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.8100 - val_loss: 0.0061\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.9363 - val_loss: 0.0062\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.9865 - val_loss: 0.0077\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.4826 - val_loss: 0.0071\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.2854 - val_loss: 0.0104\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6003 - val_loss: 0.0107\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 3.9867 - val_loss: 0.0142\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0144 - val_loss: 0.0136\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.6948 - val_loss: 0.0187\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0714 - val_loss: 0.0160\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1879 - val_loss: 0.0222\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6343 - val_loss: 0.0184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.2906 - val_loss: 0.0237\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2015 - val_loss: 0.0213\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1644 - val_loss: 0.0260\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1451 - val_loss: 0.0238\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8C15E940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_244 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 2.1212 - val_loss: 0.0116\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.7296 - val_loss: 0.0071\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 15.4341 - val_loss: 0.0061\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 17.0946 - val_loss: 0.0063\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.6253 - val_loss: 0.0064\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.9997 - val_loss: 0.0075\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.8022 - val_loss: 0.0069\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8309 - val_loss: 0.0072\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.0474 - val_loss: 0.0074\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.1435 - val_loss: 0.0085\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4880 - val_loss: 0.0085\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4617 - val_loss: 0.0094\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2641 - val_loss: 0.0094\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8424 - val_loss: 0.0105\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4975 - val_loss: 0.0107\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3126 - val_loss: 0.0112\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2210 - val_loss: 0.0115\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1748 - val_loss: 0.0124\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2ACE39D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_245 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 294ms/step - loss: 3.2226 - val_loss: 0.0142\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.7866 - val_loss: 0.0062\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 26.3524 - val_loss: 0.0063\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.7307 - val_loss: 0.0066\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.1273 - val_loss: 0.0100\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.7455 - val_loss: 0.0099\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8858 - val_loss: 0.0145\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.6694 - val_loss: 0.0145\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.5528 - val_loss: 0.0199\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0219 - val_loss: 0.0201\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.8837 - val_loss: 0.0237\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4639 - val_loss: 0.0248\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6820 - val_loss: 0.0284\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3438 - val_loss: 0.0311\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2623 - val_loss: 0.0325\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3222 - val_loss: 0.0382\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4689 - val_loss: 0.0372\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F74214700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_246 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.4037 - val_loss: 0.0138\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.9101 - val_loss: 0.0069\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17.0058 - val_loss: 0.0064\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.2223 - val_loss: 0.0068\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.6284 - val_loss: 0.0073\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.9556 - val_loss: 0.0082\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.4831 - val_loss: 0.0087\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.6552 - val_loss: 0.0109\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3736 - val_loss: 0.0130\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.8029 - val_loss: 0.0127\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3597 - val_loss: 0.0157\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.7525 - val_loss: 0.0133\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.7056 - val_loss: 0.0175\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8999 - val_loss: 0.0142\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5622 - val_loss: 0.0195\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3547 - val_loss: 0.0158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2417 - val_loss: 0.0211\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1595 - val_loss: 0.0180\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA24D3A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_247 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.3286 - val_loss: 0.0086\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.6787 - val_loss: 0.0092\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 19.6138 - val_loss: 0.0067\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.3455 - val_loss: 0.0066\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.2898 - val_loss: 0.0066\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.3421 - val_loss: 0.0067\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.0274 - val_loss: 0.0069\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.8710 - val_loss: 0.0074\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.8104 - val_loss: 0.0076\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3607 - val_loss: 0.0078\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.7811 - val_loss: 0.0095\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3912 - val_loss: 0.0082\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8769 - val_loss: 0.0106\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5214 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3128 - val_loss: 0.0112\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1922 - val_loss: 0.0100\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1353 - val_loss: 0.0117\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1158 - val_loss: 0.0107\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1148 - val_loss: 0.0120\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F139F3A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_248 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.8421 - val_loss: 0.0084\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.3253 - val_loss: 0.0094\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 21.0390 - val_loss: 0.0067\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.8840 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.6495 - val_loss: 0.0071\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.5493 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.7039 - val_loss: 0.0084\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.7145 - val_loss: 0.0092\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 5.3061 - val_loss: 0.0109\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.1000 - val_loss: 0.0109\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.0446 - val_loss: 0.0131\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8796 - val_loss: 0.0127\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2668 - val_loss: 0.0156\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7238 - val_loss: 0.0143\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4265 - val_loss: 0.0183\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3359 - val_loss: 0.0161\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3041 - val_loss: 0.0212\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2746 - val_loss: 0.0181\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F6D6FD1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_249 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 526ms/step - loss: 1.5358 - val_loss: 0.0115\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 11.5712 - val_loss: 0.0072\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 17.1908 - val_loss: 0.0076\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 14.6883 - val_loss: 0.0072\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9475 - val_loss: 0.0068\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.9590 - val_loss: 0.0068\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.0465 - val_loss: 0.0074\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.8542 - val_loss: 0.0070\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.7901 - val_loss: 0.0076\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.3309 - val_loss: 0.0074\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.3141 - val_loss: 0.0079\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1777 - val_loss: 0.0075\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2222 - val_loss: 0.0077\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0172 - val_loss: 0.0080\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7747 - val_loss: 0.0076\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5691 - val_loss: 0.0083\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3812 - val_loss: 0.0077\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2363 - val_loss: 0.0083\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1736 - val_loss: 0.0077\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1560 - val_loss: 0.0084\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1672 - val_loss: 0.0074\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1E67D670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_250 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.6996 - val_loss: 0.0117\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.9628 - val_loss: 0.0090\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.1826 - val_loss: 0.0077\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 11.1753 - val_loss: 0.0068\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.8115 - val_loss: 0.0068\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.3765 - val_loss: 0.0064\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.7770 - val_loss: 0.0063\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8620 - val_loss: 0.0063\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.2552 - val_loss: 0.0065\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5312 - val_loss: 0.0067\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.9852 - val_loss: 0.0067\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4221 - val_loss: 0.0070\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3889 - val_loss: 0.0067\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1227 - val_loss: 0.0070\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7219 - val_loss: 0.0068\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3558 - val_loss: 0.0068\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.1718 - val_loss: 0.0068\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0919 - val_loss: 0.0067\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0542 - val_loss: 0.0068\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0406 - val_loss: 0.0065\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.0361 - val_loss: 0.0067\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0411 - val_loss: 0.0064\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBEA58DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_251 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 1.9738 - val_loss: 0.0092\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.8889 - val_loss: 0.0091\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 25.3460 - val_loss: 0.0064\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.8786 - val_loss: 0.0064\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.5594 - val_loss: 0.0063\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.9345 - val_loss: 0.0065\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.9699 - val_loss: 0.0065\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.9676 - val_loss: 0.0072\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.3330 - val_loss: 0.0067\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.7948 - val_loss: 0.0083\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.0500 - val_loss: 0.0075\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.3407 - val_loss: 0.0090\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6372 - val_loss: 0.0088\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0943 - val_loss: 0.0107\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.7186 - val_loss: 0.0096\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7611 - val_loss: 0.0129\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8934 - val_loss: 0.0106\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7586 - val_loss: 0.0134\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5588 - val_loss: 0.0128\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4717 - val_loss: 0.0135\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8B6F55E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_252 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 1.9100 - val_loss: 0.0101\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 12.0718 - val_loss: 0.0120\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 19.4995 - val_loss: 0.0081\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.9904 - val_loss: 0.0074\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.5636 - val_loss: 0.0070\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.8289 - val_loss: 0.0065\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.3143 - val_loss: 0.0066\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5483 - val_loss: 0.0067\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7923 - val_loss: 0.0067\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.7019 - val_loss: 0.0069\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9635 - val_loss: 0.0072\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.2108 - val_loss: 0.0074\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7383 - val_loss: 0.0080\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4380 - val_loss: 0.0081\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2365 - val_loss: 0.0088\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1637 - val_loss: 0.0088\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1572 - val_loss: 0.0096\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1550 - val_loss: 0.0093\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1789 - val_loss: 0.0101\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2177 - val_loss: 0.0098\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3063 - val_loss: 0.0104\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EB97A4550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_253 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2.6521 - val_loss: 0.0160\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.3030 - val_loss: 0.0104\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 14.7243 - val_loss: 0.0086\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.6194 - val_loss: 0.0080\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.1273 - val_loss: 0.0069\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 8.0115 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.1338 - val_loss: 0.0070\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.7897 - val_loss: 0.0071\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.8555 - val_loss: 0.0073\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3857 - val_loss: 0.0076\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6879 - val_loss: 0.0074\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9618 - val_loss: 0.0100\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3210 - val_loss: 0.0083\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6000 - val_loss: 0.0105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9894 - val_loss: 0.0095\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5410 - val_loss: 0.0115\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2560 - val_loss: 0.0108\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.0960 - val_loss: 0.0124\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0597 - val_loss: 0.0122\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0665 - val_loss: 0.0137\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F996C8700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_254 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 631ms/step - loss: 1.4301 - val_loss: 0.0163\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.5430 - val_loss: 0.0192\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 19.1057 - val_loss: 0.0113\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.8509 - val_loss: 0.0120\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.6284 - val_loss: 0.0104\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 8.9793 - val_loss: 0.0091\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.5949 - val_loss: 0.0089\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.3983 - val_loss: 0.0081\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3828 - val_loss: 0.0082\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.4106 - val_loss: 0.0077\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5769 - val_loss: 0.0080\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 0.8667 - val_loss: 0.0077\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5634 - val_loss: 0.0078\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5266 - val_loss: 0.0076\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6169 - val_loss: 0.0077\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6568 - val_loss: 0.0077\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6331 - val_loss: 0.0074\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5979 - val_loss: 0.0083\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6055 - val_loss: 0.0074\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6730 - val_loss: 0.0085\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7201 - val_loss: 0.0074\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8515 - val_loss: 0.0084\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7865 - val_loss: 0.0076\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6152 - val_loss: 0.0083\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5520 - val_loss: 0.0076\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5353 - val_loss: 0.0087\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6996 - val_loss: 0.0076\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8321 - val_loss: 0.0082\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8262 - val_loss: 0.0077\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7225 - val_loss: 0.0088\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5707 - val_loss: 0.0079\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4942 - val_loss: 0.0080\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4027 - val_loss: 0.0081\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3768 - val_loss: 0.0078\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F5AEE4940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_255 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 1.6711 - val_loss: 0.0201\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.1244 - val_loss: 0.0154\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 13.9337 - val_loss: 0.0117\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.6430 - val_loss: 0.0097\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4363 - val_loss: 0.0079\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.099 - 0s 38ms/step - loss: 7.0999 - val_loss: 0.0078\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.7159 - val_loss: 0.0074\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.8241 - val_loss: 0.0074\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.0285 - val_loss: 0.0079\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.9892 - val_loss: 0.0077\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8460 - val_loss: 0.0087\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.3310 - val_loss: 0.0084\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3291 - val_loss: 0.0096\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.0579 - val_loss: 0.0092\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6660 - val_loss: 0.0106\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5223 - val_loss: 0.0098\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5482 - val_loss: 0.0114\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6369 - val_loss: 0.0107\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7233 - val_loss: 0.0124\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6385 - val_loss: 0.0111\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5495 - val_loss: 0.0137\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4755 - val_loss: 0.0126\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F625A4040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_256 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 2.1237 - val_loss: 0.0134\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.7563 - val_loss: 0.0151\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 21.0040 - val_loss: 0.0091\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.1075 - val_loss: 0.0087\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.5524 - val_loss: 0.0080\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.7607 - val_loss: 0.0077\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.8291 - val_loss: 0.0076\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.9462 - val_loss: 0.0077\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0390 - val_loss: 0.0078\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.9969 - val_loss: 0.0085\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.0861 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6897 - val_loss: 0.0101\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7250 - val_loss: 0.0084\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.4152 - val_loss: 0.0116\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0553 - val_loss: 0.0090\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8314 - val_loss: 0.0132\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8155 - val_loss: 0.0091\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8269 - val_loss: 0.0142\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7049 - val_loss: 0.0101\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4892 - val_loss: 0.0135\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3710 - val_loss: 0.0115\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3541 - val_loss: 0.0129\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAB30B790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_257 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.0767 - val_loss: 0.0222\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.9282 - val_loss: 0.0110\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 19.1008 - val_loss: 0.0133\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 18.5179 - val_loss: 0.0105\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.7225 - val_loss: 0.0099\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.8360 - val_loss: 0.0093\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.2322 - val_loss: 0.0092\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.3552 - val_loss: 0.0085\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.2195 - val_loss: 0.0088\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.8752 - val_loss: 0.0082\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.2157 - val_loss: 0.0093\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3950 - val_loss: 0.0083\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6737 - val_loss: 0.0091\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2638 - val_loss: 0.0085\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7441 - val_loss: 0.0093\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4675 - val_loss: 0.0086\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3674 - val_loss: 0.0095\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.340 - 0s 23ms/step - loss: 0.3407 - val_loss: 0.0086\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3676 - val_loss: 0.0094\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4521 - val_loss: 0.0089\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6533 - val_loss: 0.0093\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7758 - val_loss: 0.0095\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.6971 - val_loss: 0.0095\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5429 - val_loss: 0.0094\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4779 - val_loss: 0.0100\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC4B7D310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_258 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 342ms/step - loss: 1.4150 - val_loss: 0.0163\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.9848 - val_loss: 0.0196\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 21.6056 - val_loss: 0.0138\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.1756 - val_loss: 0.0119\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 8.8546 - val_loss: 0.0121\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.4803 - val_loss: 0.0109\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.9372 - val_loss: 0.0109\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.0380 - val_loss: 0.0102\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.3027 - val_loss: 0.0098\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.9272 - val_loss: 0.0101\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3360 - val_loss: 0.0090\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9836 - val_loss: 0.0098\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.732 - 0s 23ms/step - loss: 0.7328 - val_loss: 0.0086\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4894 - val_loss: 0.0097\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3214 - val_loss: 0.0087\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2889 - val_loss: 0.0096\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2574 - val_loss: 0.0090\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2468 - val_loss: 0.0095\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2619 - val_loss: 0.0097\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2945 - val_loss: 0.0093\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3458 - val_loss: 0.0110\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4045 - val_loss: 0.0090\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4663 - val_loss: 0.0127\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5793 - val_loss: 0.0090\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1158 - val_loss: 0.0154\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7075 - val_loss: 0.0109\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7937 - val_loss: 0.0118\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2399 - val_loss: 0.0123\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1DD158B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_259 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 548ms/step - loss: 1.8701 - val_loss: 0.0158\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.8981 - val_loss: 0.0137\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 16.8513 - val_loss: 0.0122\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.7347 - val_loss: 0.0098\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.2123 - val_loss: 0.0087\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0045 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.3343 - val_loss: 0.0082\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.6014 - val_loss: 0.0079\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7854 - val_loss: 0.0080\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.6266 - val_loss: 0.0079\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3023 - val_loss: 0.0082\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.1882 - val_loss: 0.0079\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7795 - val_loss: 0.0087\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6374 - val_loss: 0.0080\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5228 - val_loss: 0.0089\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4238 - val_loss: 0.0082\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3458 - val_loss: 0.0088\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3363 - val_loss: 0.0084\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3029 - val_loss: 0.0087\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2927 - val_loss: 0.0086\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2603 - val_loss: 0.0086\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.2419 - val_loss: 0.0090\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2601 - val_loss: 0.0085\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3613 - val_loss: 0.0096\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5792 - val_loss: 0.0082\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_260 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.2648 - val_loss: 0.0207\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 15.5971 - val_loss: 0.0165\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18.9394 - val_loss: 0.0103\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 11.0440 - val_loss: 0.0094\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.9455 - val_loss: 0.0082\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 8.5821 - val_loss: 0.0082\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.6367 - val_loss: 0.0080\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.0160 - val_loss: 0.0084\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.0928 - val_loss: 0.0090\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.8049 - val_loss: 0.0101\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.1761 - val_loss: 0.0109\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.9441 - val_loss: 0.0120\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3257 - val_loss: 0.0141\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0328 - val_loss: 0.0139\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7797 - val_loss: 0.0175\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5829 - val_loss: 0.0161\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5344 - val_loss: 0.0224\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4862 - val_loss: 0.0182\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4509 - val_loss: 0.0257\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4315 - val_loss: 0.0218\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4078 - val_loss: 0.0281\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3508 - val_loss: 0.0257\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70EA64C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_261 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 2.4190 - val_loss: 0.0222\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.8797 - val_loss: 0.0110\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 17.3192 - val_loss: 0.0125\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 10.4233 - val_loss: 0.0087\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.2759 - val_loss: 0.0082\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.7218 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 4.9486 - val_loss: 0.0079\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.8545 - val_loss: 0.0079\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.1338 - val_loss: 0.0080\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.7267 - val_loss: 0.0079\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6798 - val_loss: 0.0080\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4625 - val_loss: 0.0078\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8943 - val_loss: 0.0079\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7287 - val_loss: 0.0078\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0847 - val_loss: 0.0079\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1841 - val_loss: 0.0078\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9991 - val_loss: 0.0077\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7954 - val_loss: 0.0081\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7559 - val_loss: 0.0077\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7310 - val_loss: 0.0088\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7791 - val_loss: 0.0078\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7037 - val_loss: 0.0087\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4850 - val_loss: 0.0078\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3250 - val_loss: 0.0087\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2002 - val_loss: 0.0078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1878 - val_loss: 0.0087\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2582 - val_loss: 0.0079\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3744 - val_loss: 0.0090\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4207 - val_loss: 0.0081\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.3716 - val_loss: 0.0091\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3515 - val_loss: 0.0087\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4415 - val_loss: 0.0086\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5508 - val_loss: 0.0098\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.6052 - val_loss: 0.0091\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAFB8D670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_262 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 1.7829 - val_loss: 0.0168\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.3547 - val_loss: 0.0191\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 20.2813 - val_loss: 0.0122\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.1115 - val_loss: 0.0112\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.8039 - val_loss: 0.0095\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.8184 - val_loss: 0.0113\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6164 - val_loss: 0.0088\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.0479 - val_loss: 0.0104\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.6332 - val_loss: 0.0085\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6182 - val_loss: 0.0096\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7390 - val_loss: 0.0083\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0463 - val_loss: 0.0094\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5518 - val_loss: 0.0086\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2714 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1843 - val_loss: 0.0090\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2129 - val_loss: 0.0094\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2557 - val_loss: 0.0092\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2832 - val_loss: 0.0097\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2823 - val_loss: 0.0094\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2790 - val_loss: 0.0103\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3351 - val_loss: 0.0092\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4075 - val_loss: 0.0115\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4641 - val_loss: 0.0085\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4177 - val_loss: 0.0128\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3770 - val_loss: 0.0083\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3680 - val_loss: 0.0148\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70EA6700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_263 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 365ms/step - loss: 1.7394 - val_loss: 0.0153\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 15.8412 - val_loss: 0.0138\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 23.0973 - val_loss: 0.0142\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.4592 - val_loss: 0.0114\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7686 - val_loss: 0.0125\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.0285 - val_loss: 0.0096\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.6282 - val_loss: 0.0097\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.8520 - val_loss: 0.0089\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5495 - val_loss: 0.0087\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6037 - val_loss: 0.0090\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4478 - val_loss: 0.0084\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8131 - val_loss: 0.0087\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6076 - val_loss: 0.0080\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4953 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4510 - val_loss: 0.0077\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4313 - val_loss: 0.0092\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4033 - val_loss: 0.0076\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4048 - val_loss: 0.0091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4542 - val_loss: 0.0077\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5676 - val_loss: 0.0086\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6901 - val_loss: 0.0079\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7630 - val_loss: 0.0077\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7925 - val_loss: 0.0081\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9451 - val_loss: 0.0077\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0627 - val_loss: 0.0080\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0689 - val_loss: 0.0084\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9559 - val_loss: 0.0081\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8167 - val_loss: 0.0087\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5864 - val_loss: 0.0081\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3720 - val_loss: 0.0089\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3159 - val_loss: 0.0081\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3589 - val_loss: 0.0100\n",
      "Epoch 00032: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8C7E84C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_264 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 2.0525 - val_loss: 0.0156\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.9441 - val_loss: 0.0139\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.6770 - val_loss: 0.0117\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.1671 - val_loss: 0.0104\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.2844 - val_loss: 0.0094\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.2791 - val_loss: 0.0096\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.8844 - val_loss: 0.0088\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.7224 - val_loss: 0.0090\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.7235 - val_loss: 0.0083\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.1301 - val_loss: 0.0084\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.2252 - val_loss: 0.0087\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0871 - val_loss: 0.0084\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0073 - val_loss: 0.0084\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4828 - val_loss: 0.0087\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2002 - val_loss: 0.0088\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1760 - val_loss: 0.0092\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2860 - val_loss: 0.0094\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5416 - val_loss: 0.0099\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8205 - val_loss: 0.0101\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7765 - val_loss: 0.0103\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5405 - val_loss: 0.0108\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3443 - val_loss: 0.0115\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2218 - val_loss: 0.0119\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1712 - val_loss: 0.0120\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FCC068040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_265 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.1982 - val_loss: 0.0182\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 9.2980 - val_loss: 0.0140\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 16.3521 - val_loss: 0.0114\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.0982 - val_loss: 0.0109\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.3650 - val_loss: 0.0096\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.3245 - val_loss: 0.0082\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.7231 - val_loss: 0.0084\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2942 - val_loss: 0.0073\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.3631 - val_loss: 0.0078\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0461 - val_loss: 0.0072\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3385 - val_loss: 0.0073\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.3296 - val_loss: 0.0072\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6968 - val_loss: 0.0072\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2714 - val_loss: 0.0073\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1666 - val_loss: 0.0073\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1551 - val_loss: 0.0075\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1702 - val_loss: 0.0075\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1958 - val_loss: 0.0078\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2461 - val_loss: 0.0076\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3295 - val_loss: 0.0088\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4394 - val_loss: 0.0077\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4931 - val_loss: 0.0101\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4923 - val_loss: 0.0079\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4540 - val_loss: 0.0115\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4454 - val_loss: 0.0081\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4916 - val_loss: 0.0125\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6556 - val_loss: 0.0086\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1A891F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_266 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 402ms/step - loss: 1.4570 - val_loss: 0.0154\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.3703 - val_loss: 0.0139\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 16.3272 - val_loss: 0.0091\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 12.9103 - val_loss: 0.0077\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.6791 - val_loss: 0.0074\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.1953 - val_loss: 0.0075\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.7440 - val_loss: 0.0088\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.5683 - val_loss: 0.0090\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.3858 - val_loss: 0.0101\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 3.2793 - val_loss: 0.0097\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 2.7444 - val_loss: 0.0110\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9154 - val_loss: 0.0111\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1749 - val_loss: 0.0124\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6036 - val_loss: 0.0118\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3934 - val_loss: 0.0139\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4309 - val_loss: 0.0119\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6663 - val_loss: 0.0155\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8440 - val_loss: 0.0120\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8104 - val_loss: 0.0163\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5855 - val_loss: 0.0126\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336B2670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_267 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 428ms/step - loss: 1.3847 - val_loss: 0.0175\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.4470 - val_loss: 0.0120\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 19.1614 - val_loss: 0.0130\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.5310 - val_loss: 0.0091\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.8204 - val_loss: 0.0079\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.8434 - val_loss: 0.0080\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.6346 - val_loss: 0.0074\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.7349 - val_loss: 0.0076\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.6120 - val_loss: 0.0081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.8838 - val_loss: 0.0076\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.8230 - val_loss: 0.0088\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2974 - val_loss: 0.0078\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6591 - val_loss: 0.0088\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6101 - val_loss: 0.0084\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8381 - val_loss: 0.0084\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7922 - val_loss: 0.0089\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9395 - val_loss: 0.0082\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8405 - val_loss: 0.0093\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6193 - val_loss: 0.0082\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6050 - val_loss: 0.0100\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6080 - val_loss: 0.0081\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5479 - val_loss: 0.0099\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB66DFB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_268 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 343ms/step - loss: 1.9212 - val_loss: 0.0231\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 10.0471 - val_loss: 0.0123\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.2553 - val_loss: 0.0126\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17.4807 - val_loss: 0.0136\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.9923 - val_loss: 0.0097\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4549 - val_loss: 0.0109\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.7302 - val_loss: 0.0084\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.6036 - val_loss: 0.0092\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8421 - val_loss: 0.0079\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9169 - val_loss: 0.0089\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3482 - val_loss: 0.0078\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2534 - val_loss: 0.0085\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6266 - val_loss: 0.0079\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3633 - val_loss: 0.0082\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9252 - val_loss: 0.0079\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6844 - val_loss: 0.0080\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6598 - val_loss: 0.0081\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5766 - val_loss: 0.0078\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4926 - val_loss: 0.0080\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3634 - val_loss: 0.0081\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2690 - val_loss: 0.0081\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2068 - val_loss: 0.0085\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1812 - val_loss: 0.0080\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1854 - val_loss: 0.0093\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2405 - val_loss: 0.0078\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3619 - val_loss: 0.0106\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5239 - val_loss: 0.0075\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5973 - val_loss: 0.0120\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5388 - val_loss: 0.0075\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5077 - val_loss: 0.0110\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4417 - val_loss: 0.0079\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3500 - val_loss: 0.0103\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3608 - val_loss: 0.0095\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3717 - val_loss: 0.0082\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5052 - val_loss: 0.0131\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6983 - val_loss: 0.0077\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9185 - val_loss: 0.0170\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0109 - val_loss: 0.0077\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1730 - val_loss: 0.0152\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2443 - val_loss: 0.0078\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD5A1DC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_269 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 316ms/step - loss: 1.8296 - val_loss: 0.0184\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.4460 - val_loss: 0.0168\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 18.1270 - val_loss: 0.0124\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.3320 - val_loss: 0.0116\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.7097 - val_loss: 0.0094\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.9301 - val_loss: 0.0097\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 9.6062 - val_loss: 0.0089\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6509 - val_loss: 0.0084\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.3970 - val_loss: 0.0081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.1564 - val_loss: 0.0080\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4347 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2231 - val_loss: 0.0085\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 1.8161 - val_loss: 0.0090\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2652 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8371 - val_loss: 0.0100\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7562 - val_loss: 0.0100\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7624 - val_loss: 0.0109\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5893 - val_loss: 0.0112\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3829 - val_loss: 0.0114\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2298 - val_loss: 0.0126\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1701 - val_loss: 0.0119\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1807 - val_loss: 0.0148\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2612 - val_loss: 0.0115\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4361 - val_loss: 0.0178\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7399 - val_loss: 0.0108\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1673A9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_270 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.2493 - val_loss: 0.0262\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.7678 - val_loss: 0.0137\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.8178 - val_loss: 0.0118\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.7166 - val_loss: 0.0120\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.5405 - val_loss: 0.0091\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 6.7180 - val_loss: 0.0090\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.2311 - val_loss: 0.0085\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.2554 - val_loss: 0.0085\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.8914 - val_loss: 0.0081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.3437 - val_loss: 0.0085\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.5774 - val_loss: 0.0080\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7259 - val_loss: 0.0083\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5088 - val_loss: 0.0079\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0537 - val_loss: 0.0083\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8631 - val_loss: 0.0080\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7493 - val_loss: 0.0083\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6119 - val_loss: 0.0080\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4242 - val_loss: 0.0082\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2990 - val_loss: 0.0084\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3689 - val_loss: 0.0083\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5525 - val_loss: 0.0094\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6268 - val_loss: 0.0084\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7650 - val_loss: 0.0106\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7588 - val_loss: 0.0091\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8318 - val_loss: 0.0117\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7659 - val_loss: 0.0104\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 84ms/step - loss: 0.4415 - val_loss: 0.0112\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3575 - val_loss: 0.0117\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E81C9BF70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_271 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 271ms/step - loss: 2.2364 - val_loss: 0.0253\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.2954 - val_loss: 0.0171\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 27.5235 - val_loss: 0.0139\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.4116 - val_loss: 0.0132\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 11.62 - 0s 25ms/step - loss: 11.6238 - val_loss: 0.0128\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.5846 - val_loss: 0.0115\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.5240 - val_loss: 0.0118\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.6302 - val_loss: 0.0118\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.3335 - val_loss: 0.0120\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.0020 - val_loss: 0.0120\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.3474 - val_loss: 0.0120\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5581 - val_loss: 0.0114\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0883 - val_loss: 0.0126\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1854 - val_loss: 0.0114\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8892 - val_loss: 0.0128\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5042 - val_loss: 0.0115\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2925 - val_loss: 0.0134\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1866 - val_loss: 0.0119\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1507 - val_loss: 0.0143\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1457 - val_loss: 0.0119\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1788 - val_loss: 0.0157\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2643 - val_loss: 0.0116\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4375 - val_loss: 0.0176\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6868 - val_loss: 0.0112\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8620 - val_loss: 0.0174\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8153 - val_loss: 0.0116\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6728 - val_loss: 0.0179\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5501 - val_loss: 0.0121\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5267 - val_loss: 0.0189\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5843 - val_loss: 0.0115\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6264 - val_loss: 0.0248\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7760 - val_loss: 0.0101\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0222 - val_loss: 0.0273\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0313 - val_loss: 0.0126\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8190 - val_loss: 0.0176\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9942 - val_loss: 0.0292\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1034 - val_loss: 0.0100\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3453 - val_loss: 0.0448\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4372 - val_loss: 0.0097\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1461 - val_loss: 0.0380\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA70C28B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_272 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 1.4304 - val_loss: 0.0368\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.7603 - val_loss: 0.0159\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.5966 - val_loss: 0.0164\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.9535 - val_loss: 0.0163\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.2251 - val_loss: 0.0131\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.2547 - val_loss: 0.0116\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.4078 - val_loss: 0.0107\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.7597 - val_loss: 0.0101\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.0551 - val_loss: 0.0100\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9626 - val_loss: 0.0102\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4045 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7939 - val_loss: 0.0116\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5333 - val_loss: 0.0127\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5657 - val_loss: 0.0135\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0169 - val_loss: 0.0155\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2230 - val_loss: 0.0149\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8880 - val_loss: 0.0190\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5243 - val_loss: 0.0152\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3468 - val_loss: 0.0209\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2345 - val_loss: 0.0164\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1864 - val_loss: 0.0220\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1885 - val_loss: 0.0178\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2159 - val_loss: 0.0232\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2645 - val_loss: 0.0194\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC34A2CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_273 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 386ms/step - loss: 2.3498 - val_loss: 0.0292\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 12.2279 - val_loss: 0.0192\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 30.8451 - val_loss: 0.0168\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.1553 - val_loss: 0.0137\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3320 - val_loss: 0.0126\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.4589 - val_loss: 0.0110\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.3040 - val_loss: 0.0110\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7407 - val_loss: 0.0106\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.6402 - val_loss: 0.0107\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.2166 - val_loss: 0.0107\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0531 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.6747 - val_loss: 0.0111\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4632 - val_loss: 0.0124\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3744 - val_loss: 0.0116\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3941 - val_loss: 0.0142\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4074 - val_loss: 0.0119\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4709 - val_loss: 0.0161\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5045 - val_loss: 0.0126\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4714 - val_loss: 0.0177\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3943 - val_loss: 0.0141\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4236 - val_loss: 0.0182\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6096 - val_loss: 0.0160\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7603 - val_loss: 0.0177\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAFB8DC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_274 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 246ms/step - loss: 2.2398 - val_loss: 0.0271\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.1031 - val_loss: 0.0210\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 26.3092 - val_loss: 0.0190\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 15.2304 - val_loss: 0.0142\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.2245 - val_loss: 0.0123\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.7331 - val_loss: 0.0133\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 5.6508 - val_loss: 0.0114\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.9017 - val_loss: 0.0115\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6077 - val_loss: 0.0109\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.7895 - val_loss: 0.0109\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6954 - val_loss: 0.0108\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0193 - val_loss: 0.0109\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6733 - val_loss: 0.0109\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3983 - val_loss: 0.0108\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4140 - val_loss: 0.0110\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4928 - val_loss: 0.0109\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5098 - val_loss: 0.0113\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4661 - val_loss: 0.0110\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4155 - val_loss: 0.0114\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4275 - val_loss: 0.0111\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5616 - val_loss: 0.0113\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9024 - val_loss: 0.0116\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2593 - val_loss: 0.0110\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1642 - val_loss: 0.0125\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9052 - val_loss: 0.0110\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6588 - val_loss: 0.0129\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5036 - val_loss: 0.0109\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4434 - val_loss: 0.0140\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4722 - val_loss: 0.0109\n",
      "Epoch 00029: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC1255790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_275 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 3.1414 - val_loss: 0.0265\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.6469 - val_loss: 0.0232\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.3205 - val_loss: 0.0184\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.7099 - val_loss: 0.0133\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 6.0721 - val_loss: 0.0135\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.1152 - val_loss: 0.0109\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.5745 - val_loss: 0.0112\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.7072 - val_loss: 0.0107\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0627 - val_loss: 0.0107\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.4271 - val_loss: 0.0107\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6872 - val_loss: 0.0106\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.3337 - val_loss: 0.0107\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9999 - val_loss: 0.0109\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0297 - val_loss: 0.0107\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9318 - val_loss: 0.0114\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.7843 - val_loss: 0.0108\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5745 - val_loss: 0.0119\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3968 - val_loss: 0.0109\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2818 - val_loss: 0.0125\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2401 - val_loss: 0.0108\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2567 - val_loss: 0.0133\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3212 - val_loss: 0.0107\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4139 - val_loss: 0.0143\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5173 - val_loss: 0.0107\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5496 - val_loss: 0.0147\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5051 - val_loss: 0.0106\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC3E80280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_276 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 260ms/step - loss: 2.1416 - val_loss: 0.0317\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.0206 - val_loss: 0.0225\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 21.7515 - val_loss: 0.0152\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.8648 - val_loss: 0.0140\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.9147 - val_loss: 0.0114\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.8512 - val_loss: 0.0121\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.8019 - val_loss: 0.0112\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5566 - val_loss: 0.0111\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5162 - val_loss: 0.0115\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 2.289 - 0s 30ms/step - loss: 2.2893 - val_loss: 0.0115\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9058 - val_loss: 0.0121\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4123 - val_loss: 0.0123\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2254 - val_loss: 0.0129\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.1670 - val_loss: 0.0130\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1839 - val_loss: 0.0137\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2935 - val_loss: 0.0133\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4902 - val_loss: 0.0154\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7377 - val_loss: 0.0137\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8417 - val_loss: 0.0170\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7316 - val_loss: 0.0144\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7466 - val_loss: 0.0195\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8565 - val_loss: 0.0140\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1453 - val_loss: 0.0241\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD22740D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_277 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.2551 - val_loss: 0.0195\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22.3049 - val_loss: 0.0231\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17.1392 - val_loss: 0.0186\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.3989 - val_loss: 0.0163\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7351 - val_loss: 0.0137\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.3456 - val_loss: 0.0123\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.6320 - val_loss: 0.0110\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7363 - val_loss: 0.0110\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4969 - val_loss: 0.0111\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.4950 - val_loss: 0.0109\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5004 - val_loss: 0.0118\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.016 - 0s 32ms/step - loss: 1.0168 - val_loss: 0.0114\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8068 - val_loss: 0.0131\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5595 - val_loss: 0.0123\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5213 - val_loss: 0.0140\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3268 - val_loss: 0.0131\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2388 - val_loss: 0.0147\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2042 - val_loss: 0.0138\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2249 - val_loss: 0.0159\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2632 - val_loss: 0.0144\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3419 - val_loss: 0.0170\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4108 - val_loss: 0.0148\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4555 - val_loss: 0.0173\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4159 - val_loss: 0.0158\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3533 - val_loss: 0.0168\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2ACE3D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_278 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 344ms/step - loss: 3.1549 - val_loss: 0.0277\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 24.2166 - val_loss: 0.0265\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 26.2203 - val_loss: 0.0219\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.3827 - val_loss: 0.0180\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.2103 - val_loss: 0.0166\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.3979 - val_loss: 0.0167\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.3391 - val_loss: 0.0148\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7943 - val_loss: 0.0157\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2024 - val_loss: 0.0137\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6094 - val_loss: 0.0150\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.3818 - val_loss: 0.0148\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9475 - val_loss: 0.0147\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5220 - val_loss: 0.0164\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3251 - val_loss: 0.0147\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2562 - val_loss: 0.0174\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2562 - val_loss: 0.0144\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2844 - val_loss: 0.0183\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3253 - val_loss: 0.0144\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3587 - val_loss: 0.0185\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4100 - val_loss: 0.0147\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5086 - val_loss: 0.0180\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6047 - val_loss: 0.0153\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6904 - val_loss: 0.0185\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7619 - val_loss: 0.0149\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F13A03F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_279 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 1.6750 - val_loss: 0.0234\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 14.3325 - val_loss: 0.0217\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 18.0932 - val_loss: 0.0165\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.9347 - val_loss: 0.0157\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.5386 - val_loss: 0.0150\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.5451 - val_loss: 0.0131\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.5233 - val_loss: 0.0125\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 4.2993 - val_loss: 0.0122\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.0273 - val_loss: 0.0120\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2768 - val_loss: 0.0120\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8332 - val_loss: 0.0122\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9980 - val_loss: 0.0121\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4775 - val_loss: 0.0127\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3223 - val_loss: 0.0123\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3164 - val_loss: 0.0136\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3428 - val_loss: 0.0127\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3282 - val_loss: 0.0142\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2887 - val_loss: 0.0131\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2277 - val_loss: 0.0145\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1849 - val_loss: 0.0134\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1587 - val_loss: 0.0146\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1605 - val_loss: 0.0140\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2172 - val_loss: 0.0143\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3721 - val_loss: 0.0155\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAB9D2DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_280 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 1.6442 - val_loss: 0.0373\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.7873 - val_loss: 0.0250\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.8627 - val_loss: 0.0228\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.7018 - val_loss: 0.0210\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.1619 - val_loss: 0.0175\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.8900 - val_loss: 0.0166\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7146 - val_loss: 0.0145\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.0920 - val_loss: 0.0138\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.5812 - val_loss: 0.0136\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.7552 - val_loss: 0.0128\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.3419 - val_loss: 0.0126\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.5391 - val_loss: 0.0123\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1320 - val_loss: 0.0124\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6724 - val_loss: 0.0123\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4956 - val_loss: 0.0122\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3591 - val_loss: 0.0122\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3522 - val_loss: 0.0123\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4043 - val_loss: 0.0123\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4671 - val_loss: 0.0127\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4976 - val_loss: 0.0123\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4328 - val_loss: 0.0132\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3557 - val_loss: 0.0124\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2927 - val_loss: 0.0132\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2504 - val_loss: 0.0126\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2156 - val_loss: 0.0135\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2106 - val_loss: 0.0128\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2269 - val_loss: 0.0138\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3025 - val_loss: 0.0130\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4073 - val_loss: 0.0140\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5335 - val_loss: 0.0141\n",
      "Epoch 00030: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA75C20D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_281 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 413ms/step - loss: 1.7966 - val_loss: 0.0348\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.1620 - val_loss: 0.0235\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17.9732 - val_loss: 0.0212\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.3522 - val_loss: 0.0225\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.9061 - val_loss: 0.0153\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.3641 - val_loss: 0.0165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.5095 - val_loss: 0.0137\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.8508 - val_loss: 0.0142\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.2892 - val_loss: 0.0127\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.6329 - val_loss: 0.0126\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1674 - val_loss: 0.0125\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7165 - val_loss: 0.0122\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4268 - val_loss: 0.0122\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3006 - val_loss: 0.0122\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2756 - val_loss: 0.0125\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3053 - val_loss: 0.0123\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3370 - val_loss: 0.0129\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3641 - val_loss: 0.0122\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4086 - val_loss: 0.0136\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4643 - val_loss: 0.0122\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5287 - val_loss: 0.0142\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5197 - val_loss: 0.0121\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4654 - val_loss: 0.0146\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4377 - val_loss: 0.0123\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4452 - val_loss: 0.0148\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4890 - val_loss: 0.0124\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4926 - val_loss: 0.0143\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4633 - val_loss: 0.0129\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4090 - val_loss: 0.0133\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4157 - val_loss: 0.0138\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4745 - val_loss: 0.0125\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6364 - val_loss: 0.0153\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7524 - val_loss: 0.0121\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8281 - val_loss: 0.0205\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3372 - val_loss: 0.0119\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3921 - val_loss: 0.0207\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5601 - val_loss: 0.0113\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6753 - val_loss: 0.0178\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1568 - val_loss: 0.0116\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8525 - val_loss: 0.0164\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F626E5550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_282 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 1.4919 - val_loss: 0.0277\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.8860 - val_loss: 0.0264\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 26.6707 - val_loss: 0.0231\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.8952 - val_loss: 0.0178\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.1951 - val_loss: 0.0173\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.7529 - val_loss: 0.0162\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.4388 - val_loss: 0.0172\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5006 - val_loss: 0.0169\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.9379 - val_loss: 0.0164\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.1222 - val_loss: 0.0155\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4115 - val_loss: 0.0173\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.7496 - val_loss: 0.0155\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.3689 - val_loss: 0.0177\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9149 - val_loss: 0.0153\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5726 - val_loss: 0.0169\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4848 - val_loss: 0.0145\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4286 - val_loss: 0.0163\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3967 - val_loss: 0.0141\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3169 - val_loss: 0.0159\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2459 - val_loss: 0.0139\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2105 - val_loss: 0.0153\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2562 - val_loss: 0.0139\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4871 - val_loss: 0.0150\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0210 - val_loss: 0.0138\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.3786 - val_loss: 0.0139\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2408 - val_loss: 0.0150\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9387 - val_loss: 0.0130\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7516 - val_loss: 0.0160\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6040 - val_loss: 0.0127\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4380 - val_loss: 0.0158\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4359 - val_loss: 0.0124\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4511 - val_loss: 0.0152\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5714 - val_loss: 0.0124\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7469 - val_loss: 0.0141\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8739 - val_loss: 0.0121\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7112 - val_loss: 0.0122\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 85ms/step - loss: 0.5838 - val_loss: 0.0121\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4360 - val_loss: 0.0122\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2926 - val_loss: 0.0122\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2555 - val_loss: 0.0121\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA1BE4F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_283 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 2.3759 - val_loss: 0.0315\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.5421 - val_loss: 0.0368\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 14.7467 - val_loss: 0.0258\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.0998 - val_loss: 0.0230\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 11.3422 - val_loss: 0.0240\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.8179 - val_loss: 0.0203\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.5170 - val_loss: 0.0231\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0150 - val_loss: 0.0193\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.1051 - val_loss: 0.0217\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9405 - val_loss: 0.0183\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6906 - val_loss: 0.0199\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2114 - val_loss: 0.0210\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.4151 - val_loss: 0.0175\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.8088 - val_loss: 0.0212\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.3596 - val_loss: 0.0182\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2893 - val_loss: 0.0207\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.1109 - val_loss: 0.0195\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7368 - val_loss: 0.0215\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4250 - val_loss: 0.0202\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2232 - val_loss: 0.0210\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1280 - val_loss: 0.0211\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1078 - val_loss: 0.0216\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1304 - val_loss: 0.0221\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2040 - val_loss: 0.0230\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3479 - val_loss: 0.0228\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4627 - val_loss: 0.0237\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4956 - val_loss: 0.0258\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.4919 - val_loss: 0.0221\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD2274160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_284 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 2.0558 - val_loss: 0.0412\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.2851 - val_loss: 0.0259\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.3696 - val_loss: 0.0227\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 12.6213 - val_loss: 0.0221\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.2873 - val_loss: 0.0186\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.1219 - val_loss: 0.0190\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.7639 - val_loss: 0.0159\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.1280 - val_loss: 0.0168\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 3.277 - 0s 37ms/step - loss: 3.2771 - val_loss: 0.0140\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3714 - val_loss: 0.0144\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9500 - val_loss: 0.0135\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5479 - val_loss: 0.0130\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3146 - val_loss: 0.0131\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1623 - val_loss: 0.0125\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1045 - val_loss: 0.0128\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0860 - val_loss: 0.0123\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1051 - val_loss: 0.0128\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1928 - val_loss: 0.0122\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3645 - val_loss: 0.0128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6442 - val_loss: 0.0122\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9845 - val_loss: 0.0125\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1679 - val_loss: 0.0123\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0560 - val_loss: 0.0124\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8591 - val_loss: 0.0122\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6508 - val_loss: 0.0122\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5314 - val_loss: 0.0124\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4262 - val_loss: 0.0124\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4138 - val_loss: 0.0130\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4260 - val_loss: 0.0135\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5160 - val_loss: 0.0136\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6131 - val_loss: 0.0153\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6940 - val_loss: 0.0136\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6635 - val_loss: 0.0165\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6380 - val_loss: 0.0145\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.4733 - val_loss: 0.0139\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4445 - val_loss: 0.0143\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4479 - val_loss: 0.0137\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3929 - val_loss: 0.0129\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3348 - val_loss: 0.0127\n",
      "Epoch 00039: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE3CB63A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_285 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 2s 2s/step - loss: 1.3651 - val_loss: 0.0441\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.0619 - val_loss: 0.0288\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 18.1303 - val_loss: 0.0227\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.3623 - val_loss: 0.0208\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.5530 - val_loss: 0.0178\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.0775 - val_loss: 0.0179\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.3244 - val_loss: 0.0156\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.4519 - val_loss: 0.0157\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.6737 - val_loss: 0.0150\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.8025 - val_loss: 0.0151\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6961 - val_loss: 0.0143\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8987 - val_loss: 0.0145\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4777 - val_loss: 0.0147\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3869 - val_loss: 0.0141\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4340 - val_loss: 0.0152\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4442 - val_loss: 0.0140\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4047 - val_loss: 0.0154\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3052 - val_loss: 0.0146\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3247 - val_loss: 0.0153\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4389 - val_loss: 0.0152\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5863 - val_loss: 0.0157\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7176 - val_loss: 0.0148\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7048 - val_loss: 0.0176\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6913 - val_loss: 0.0136\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6844 - val_loss: 0.0198\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6809 - val_loss: 0.0126\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6411 - val_loss: 0.0215\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5481 - val_loss: 0.0125\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4834 - val_loss: 0.0217\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4179 - val_loss: 0.0128\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4044 - val_loss: 0.0209\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5265 - val_loss: 0.0129\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8336 - val_loss: 0.0186\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0883 - val_loss: 0.0169\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3173 - val_loss: 0.0155\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4359 - val_loss: 0.0177\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6768 - val_loss: 0.0141\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7475 - val_loss: 0.0173\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3333 - val_loss: 0.0139\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2010 - val_loss: 0.0180\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F774C1A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_286 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 595ms/step - loss: 2.5582 - val_loss: 0.0324\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.9289 - val_loss: 0.0308\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 20.7398 - val_loss: 0.0208\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.7280 - val_loss: 0.0205\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.5296 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.0470 - val_loss: 0.0170\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.2866 - val_loss: 0.0149\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9225 - val_loss: 0.0156\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.2669 - val_loss: 0.0134\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.5723 - val_loss: 0.0155\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3754 - val_loss: 0.0136\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7288 - val_loss: 0.0143\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5413 - val_loss: 0.0131\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5894 - val_loss: 0.0147\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6573 - val_loss: 0.0129\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6729 - val_loss: 0.0159\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5884 - val_loss: 0.0128\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5195 - val_loss: 0.0165\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3832 - val_loss: 0.0131\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3556 - val_loss: 0.0172\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3385 - val_loss: 0.0134\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4674 - val_loss: 0.0179\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6404 - val_loss: 0.0133\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7729 - val_loss: 0.0170\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9172 - val_loss: 0.0143\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8644 - val_loss: 0.0159\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6621 - val_loss: 0.0166\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5095 - val_loss: 0.0145\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4422 - val_loss: 0.0188\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3738 - val_loss: 0.0134\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4015 - val_loss: 0.0222\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5529 - val_loss: 0.0127\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7760 - val_loss: 0.0261\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9119 - val_loss: 0.0127\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8943 - val_loss: 0.0250\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6887 - val_loss: 0.0154\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6641 - val_loss: 0.0199\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7110 - val_loss: 0.0155\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5213 - val_loss: 0.0182\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5017 - val_loss: 0.0195\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F626E5670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_287 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 372ms/step - loss: 2.1981 - val_loss: 0.0308\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.8584 - val_loss: 0.0304\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 22.1338 - val_loss: 0.0256\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.0588 - val_loss: 0.0246\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.9938 - val_loss: 0.0246\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.7991 - val_loss: 0.0195\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.7574 - val_loss: 0.0202\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5349 - val_loss: 0.0172\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.1171 - val_loss: 0.0182\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7745 - val_loss: 0.0164\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6073 - val_loss: 0.0168\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3638 - val_loss: 0.0158\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4288 - val_loss: 0.0156\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9112 - val_loss: 0.0155\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5008 - val_loss: 0.0152\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2066 - val_loss: 0.0153\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1088 - val_loss: 0.0153\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0878 - val_loss: 0.0153\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1021 - val_loss: 0.0160\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1532 - val_loss: 0.0152\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2402 - val_loss: 0.0173\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3431 - val_loss: 0.0148\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4253 - val_loss: 0.0187\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4778 - val_loss: 0.0149\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5113 - val_loss: 0.0194\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5103 - val_loss: 0.0148\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5393 - val_loss: 0.0216\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5829 - val_loss: 0.0139\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6235 - val_loss: 0.0237\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7872 - val_loss: 0.0137\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8269 - val_loss: 0.0256\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8109 - val_loss: 0.0146\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6544 - val_loss: 0.0202\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5912 - val_loss: 0.0149\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5778 - val_loss: 0.0168\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5038 - val_loss: 0.0187\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6338 - val_loss: 0.0145\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7299 - val_loss: 0.0219\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0131 - val_loss: 0.0124\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.9130 - val_loss: 0.0219\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1550BE50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_288 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 332ms/step - loss: 2.4296 - val_loss: 0.0487\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 22.7529 - val_loss: 0.0274\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2598 - val_loss: 0.0307\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.1522 - val_loss: 0.0303\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.2132 - val_loss: 0.0256\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7966 - val_loss: 0.0299\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.2947 - val_loss: 0.0247\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.3879 - val_loss: 0.0302\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9781 - val_loss: 0.0249\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.3531 - val_loss: 0.0281\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9539 - val_loss: 0.0263\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4423 - val_loss: 0.0260\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1620 - val_loss: 0.0301\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.2120 - val_loss: 0.0267\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9691 - val_loss: 0.0330\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7020 - val_loss: 0.0304\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5701 - val_loss: 0.0358\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4180 - val_loss: 0.0346\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2456 - val_loss: 0.0378\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1808 - val_loss: 0.0394\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1506 - val_loss: 0.0405\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1464 - val_loss: 0.0434\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC51B55E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_289 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 2.4878 - val_loss: 0.0396\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 14.5460 - val_loss: 0.0293\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 15.3447 - val_loss: 0.0249\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.2906 - val_loss: 0.0197\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.4108 - val_loss: 0.0178\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.9721 - val_loss: 0.0183\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.2691 - val_loss: 0.0170\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.8628 - val_loss: 0.0172\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.8794 - val_loss: 0.0154\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1059 - val_loss: 0.0179\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4334 - val_loss: 0.0143\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7826 - val_loss: 0.0175\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9554 - val_loss: 0.0140\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6432 - val_loss: 0.0160\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4133 - val_loss: 0.0146\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2803 - val_loss: 0.0152\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2423 - val_loss: 0.0153\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2700 - val_loss: 0.0147\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3423 - val_loss: 0.0165\n",
      "Epoch 20/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4499 - val_loss: 0.0140\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5146 - val_loss: 0.0173\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5569 - val_loss: 0.0134\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5794 - val_loss: 0.0186\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7118 - val_loss: 0.0132\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8364 - val_loss: 0.0191\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7344 - val_loss: 0.0128\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5349 - val_loss: 0.0191\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4743 - val_loss: 0.0130\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4721 - val_loss: 0.0173\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4956 - val_loss: 0.0141\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5837 - val_loss: 0.0160\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7337 - val_loss: 0.0147\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8416 - val_loss: 0.0177\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1307 - val_loss: 0.0131\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2600 - val_loss: 0.0161\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3443 - val_loss: 0.0119\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5145 - val_loss: 0.0181\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4815 - val_loss: 0.0143\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7775 - val_loss: 0.0152\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.1266 - val_loss: 0.0124\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FF74BB0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_290 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.5279 - val_loss: 0.0437\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 22.6708 - val_loss: 0.0363\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 18.9174 - val_loss: 0.0288\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.2433 - val_loss: 0.0278\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.0523 - val_loss: 0.0235\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.6701 - val_loss: 0.0223\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.4980 - val_loss: 0.0214\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8126 - val_loss: 0.0216\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.3593 - val_loss: 0.0184\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.7707 - val_loss: 0.0195\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1232 - val_loss: 0.0174\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4636 - val_loss: 0.0184\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9475 - val_loss: 0.0167\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4615 - val_loss: 0.0182\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2155 - val_loss: 0.0172\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1423 - val_loss: 0.0186\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.1533 - val_loss: 0.0174\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2164 - val_loss: 0.0191\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3226 - val_loss: 0.0172\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4280 - val_loss: 0.0203\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4647 - val_loss: 0.0169\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4555 - val_loss: 0.0206\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4378 - val_loss: 0.0172\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5508 - val_loss: 0.0227\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7146 - val_loss: 0.0159\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8687 - val_loss: 0.0230\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0683 - val_loss: 0.0167\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3186 - val_loss: 0.0209\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4708 - val_loss: 0.0208\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9589 - val_loss: 0.0175\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7908 - val_loss: 0.0217\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6947 - val_loss: 0.0190\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6072 - val_loss: 0.0207\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7782 - val_loss: 0.0160\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8465 - val_loss: 0.0258\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7926 - val_loss: 0.0130\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9276 - val_loss: 0.0283\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0874 - val_loss: 0.0121\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0057 - val_loss: 0.0226\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9970 - val_loss: 0.0127\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F270721F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_291 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 451ms/step - loss: 1.9959 - val_loss: 0.0390\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.4156 - val_loss: 0.0349\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 19.7136 - val_loss: 0.0299\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.4664 - val_loss: 0.0242\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.4075 - val_loss: 0.0215\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.7763 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.7724 - val_loss: 0.0209\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.0992 - val_loss: 0.0194\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.1133 - val_loss: 0.0195\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7738 - val_loss: 0.0193\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.9612 - val_loss: 0.0183\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8885 - val_loss: 0.0191\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1221 - val_loss: 0.0186\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8286 - val_loss: 0.0204\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7413 - val_loss: 0.0199\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6603 - val_loss: 0.0206\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5143 - val_loss: 0.0214\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3744 - val_loss: 0.0212\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2867 - val_loss: 0.0221\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2878 - val_loss: 0.0217\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3645 - val_loss: 0.0234\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.4906 - val_loss: 0.0213\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6122 - val_loss: 0.0244\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5736 - val_loss: 0.0216\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5035 - val_loss: 0.0240\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4202 - val_loss: 0.0218\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB91D79D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_292 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 324ms/step - loss: 1.6685 - val_loss: 0.0384\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.9412 - val_loss: 0.0347\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 20.1724 - val_loss: 0.0224\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.2120 - val_loss: 0.0225\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.9160 - val_loss: 0.0206\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1557 - val_loss: 0.0186\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9957 - val_loss: 0.0202\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.5835 - val_loss: 0.0189\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5993 - val_loss: 0.0170\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.7267 - val_loss: 0.0170\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9530 - val_loss: 0.0168\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9309 - val_loss: 0.0160\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3592 - val_loss: 0.0174\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8088 - val_loss: 0.0154\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5175 - val_loss: 0.0166\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4193 - val_loss: 0.0152\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3723 - val_loss: 0.0160\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4035 - val_loss: 0.0145\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5303 - val_loss: 0.0156\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6598 - val_loss: 0.0137\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5933 - val_loss: 0.0148\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5532 - val_loss: 0.0135\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6024 - val_loss: 0.0141\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8345 - val_loss: 0.0129\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9742 - val_loss: 0.0135\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8004 - val_loss: 0.0129\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6639 - val_loss: 0.0135\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5739 - val_loss: 0.0126\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5525 - val_loss: 0.0131\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5064 - val_loss: 0.0118\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4664 - val_loss: 0.0131\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4289 - val_loss: 0.0114\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4726 - val_loss: 0.0127\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5827 - val_loss: 0.0113\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6672 - val_loss: 0.0122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6949 - val_loss: 0.0113\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6662 - val_loss: 0.0115\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6888 - val_loss: 0.0120\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8177 - val_loss: 0.0108\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9023 - val_loss: 0.0133\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC74E0040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_293 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.2037 - val_loss: 0.0388\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.0639 - val_loss: 0.0338\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 20.0237 - val_loss: 0.0262\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.4716 - val_loss: 0.0237\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2545 - val_loss: 0.0179\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.7420 - val_loss: 0.0174\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.4136 - val_loss: 0.0135\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.1477 - val_loss: 0.0155\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.8943 - val_loss: 0.0130\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7253 - val_loss: 0.0131\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8956 - val_loss: 0.0118\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5072 - val_loss: 0.0120\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4079 - val_loss: 0.0114\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4320 - val_loss: 0.0119\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4598 - val_loss: 0.0113\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4235 - val_loss: 0.0114\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3568 - val_loss: 0.0112\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2747 - val_loss: 0.0111\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2427 - val_loss: 0.0111\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2897 - val_loss: 0.0109\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5613 - val_loss: 0.0108\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0575 - val_loss: 0.0109\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2333 - val_loss: 0.0106\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8823 - val_loss: 0.0106\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7677 - val_loss: 0.0105\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9364 - val_loss: 0.0108\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0674 - val_loss: 0.0104\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3183 - val_loss: 0.0106\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2949 - val_loss: 0.0105\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0977 - val_loss: 0.0112\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0929 - val_loss: 0.0111\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8652 - val_loss: 0.0126\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5300 - val_loss: 0.0105\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4594 - val_loss: 0.0126\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4599 - val_loss: 0.0106\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4632 - val_loss: 0.0141\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4894 - val_loss: 0.0105\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4088 - val_loss: 0.0135\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4509 - val_loss: 0.0106\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4710 - val_loss: 0.0125\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FDD3CB8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_294 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 516ms/step - loss: 1.5787 - val_loss: 0.0345\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 16.8855 - val_loss: 0.0363\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 21.0928 - val_loss: 0.0254\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.6477 - val_loss: 0.0221\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.5879 - val_loss: 0.0213\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.2108 - val_loss: 0.0178\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.0947 - val_loss: 0.0172\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.1687 - val_loss: 0.0150\n",
      "Epoch 9/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2584 - val_loss: 0.0150\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5077 - val_loss: 0.0133\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7807 - val_loss: 0.0148\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1104 - val_loss: 0.0128\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6918 - val_loss: 0.0145\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5385 - val_loss: 0.0128\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4802 - val_loss: 0.0142\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3879 - val_loss: 0.0133\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3574 - val_loss: 0.0137\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3637 - val_loss: 0.0137\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3787 - val_loss: 0.0134\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3470 - val_loss: 0.0137\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3038 - val_loss: 0.0132\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3039 - val_loss: 0.0134\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3591 - val_loss: 0.0133\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4930 - val_loss: 0.0128\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6414 - val_loss: 0.0139\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6979 - val_loss: 0.0119\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7397 - val_loss: 0.0133\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7075 - val_loss: 0.0134\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6790 - val_loss: 0.0123\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7124 - val_loss: 0.0140\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5230 - val_loss: 0.0115\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4711 - val_loss: 0.0149\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6104 - val_loss: 0.0106\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7381 - val_loss: 0.0177\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0209 - val_loss: 0.0101\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0287 - val_loss: 0.0239\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1834 - val_loss: 0.0126\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2086 - val_loss: 0.0233\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1247 - val_loss: 0.0113\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7141 - val_loss: 0.0248\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA86B04C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_295 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 2.3544 - val_loss: 0.0334\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.7549 - val_loss: 0.0329\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 15.9444 - val_loss: 0.0257\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.6636 - val_loss: 0.0237\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.9592 - val_loss: 0.0220\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.7145 - val_loss: 0.0202\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.0003 - val_loss: 0.0201\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.4005 - val_loss: 0.0191\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 5.4609 - val_loss: 0.0176\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.7817 - val_loss: 0.0163\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.6957 - val_loss: 0.0146\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2218 - val_loss: 0.0147\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6398 - val_loss: 0.0136\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3300 - val_loss: 0.0132\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.3191 - val_loss: 0.0126\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2514 - val_loss: 0.0122\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2271 - val_loss: 0.0120\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2284 - val_loss: 0.0119\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.2602 - val_loss: 0.0118\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3739 - val_loss: 0.0116\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5603 - val_loss: 0.0112\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7805 - val_loss: 0.0113\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8510 - val_loss: 0.0108\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9078 - val_loss: 0.0111\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7713 - val_loss: 0.0100\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6604 - val_loss: 0.0109\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4794 - val_loss: 0.0098\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3258 - val_loss: 0.0105\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2239 - val_loss: 0.0097\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2001 - val_loss: 0.0105\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2843 - val_loss: 0.0092\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5944 - val_loss: 0.0120\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0541 - val_loss: 0.0093\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1391 - val_loss: 0.0125\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.2732 - val_loss: 0.0110\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.4621 - val_loss: 0.0149\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step - loss: 1.3284 - val_loss: 0.0097\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.5607 - val_loss: 0.0151\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.9621 - val_loss: 0.0102\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.9963 - val_loss: 0.0111\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E81C9BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_296 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 1.5521 - val_loss: 0.0352\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.6218 - val_loss: 0.0343\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 17.6589 - val_loss: 0.0290\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.0750 - val_loss: 0.0285\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.2027 - val_loss: 0.0274\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.1475 - val_loss: 0.0216\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.6239 - val_loss: 0.0218\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.5315 - val_loss: 0.0204\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.1733 - val_loss: 0.0197\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.2116 - val_loss: 0.0194\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.7044 - val_loss: 0.0184\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3855 - val_loss: 0.0190\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1261 - val_loss: 0.0176\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9011 - val_loss: 0.0191\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5960 - val_loss: 0.0184\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4626 - val_loss: 0.0190\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3737 - val_loss: 0.0193\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2861 - val_loss: 0.0189\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2375 - val_loss: 0.0199\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2065 - val_loss: 0.0195\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2306 - val_loss: 0.0203\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2751 - val_loss: 0.0198\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3408 - val_loss: 0.0211\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3900 - val_loss: 0.0195\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4032 - val_loss: 0.0222\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3813 - val_loss: 0.0204\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3832 - val_loss: 0.0229\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3881 - val_loss: 0.0215\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEE7FF0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_297 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 2.0100 - val_loss: 0.0442\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.8736 - val_loss: 0.0270\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 19.1724 - val_loss: 0.0271\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 13.8922 - val_loss: 0.0217\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.1883 - val_loss: 0.0237\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.4057 - val_loss: 0.0180\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0116 - val_loss: 0.0188\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.1026 - val_loss: 0.0162\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.8611 - val_loss: 0.0163\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.0309 - val_loss: 0.0151\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1176 - val_loss: 0.0156\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5827 - val_loss: 0.0142\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5068 - val_loss: 0.0150\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5625 - val_loss: 0.0135\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5190 - val_loss: 0.0143\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3717 - val_loss: 0.0132\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 131ms/step - loss: 0.2455 - val_loss: 0.0141\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1870 - val_loss: 0.0128\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1978 - val_loss: 0.0142\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3152 - val_loss: 0.0123\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5832 - val_loss: 0.0149\n",
      "Epoch 22/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7913 - val_loss: 0.0121\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8157 - val_loss: 0.0155\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7655 - val_loss: 0.0119\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.8409 - val_loss: 0.0166\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0300 - val_loss: 0.0132\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1183 - val_loss: 0.0153\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0937 - val_loss: 0.0162\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0118 - val_loss: 0.0133\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.0885 - val_loss: 0.0201\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3918 - val_loss: 0.0119\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2776 - val_loss: 0.0190\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9348 - val_loss: 0.0114\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8978 - val_loss: 0.0239\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8274 - val_loss: 0.0118\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7695 - val_loss: 0.0256\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7865 - val_loss: 0.0122\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8388 - val_loss: 0.0209\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8099 - val_loss: 0.0174\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6551 - val_loss: 0.0184\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEC88CAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_298 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 1.6788 - val_loss: 0.0401\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 16.0769 - val_loss: 0.0318\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 15.2173 - val_loss: 0.0285\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 12.1171 - val_loss: 0.0264\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.2483 - val_loss: 0.0223\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.4610 - val_loss: 0.0215\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.1901 - val_loss: 0.0186\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.3518 - val_loss: 0.0196\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.3272 - val_loss: 0.0155\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.0229 - val_loss: 0.0161\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.9993 - val_loss: 0.0143\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.4222 - val_loss: 0.0160\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.7046 - val_loss: 0.0138\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0432 - val_loss: 0.0159\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4866 - val_loss: 0.0140\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2429 - val_loss: 0.0148\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1695 - val_loss: 0.0147\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1776 - val_loss: 0.0144\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2563 - val_loss: 0.0148\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4355 - val_loss: 0.0144\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5994 - val_loss: 0.0149\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5881 - val_loss: 0.0146\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4406 - val_loss: 0.0151\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3310 - val_loss: 0.0156\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2382 - val_loss: 0.0147\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1792 - val_loss: 0.0167\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1475 - val_loss: 0.0145\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1573 - val_loss: 0.0177\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F44D83670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_299 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 2.2955 - val_loss: 0.0593\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.2729 - val_loss: 0.0333\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.7643 - val_loss: 0.0290\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.5792 - val_loss: 0.0272\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.2640 - val_loss: 0.0245\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.0139 - val_loss: 0.0231\n",
      "Epoch 7/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6945 - val_loss: 0.0229\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.6016 - val_loss: 0.0229\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.8985 - val_loss: 0.0230\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3418 - val_loss: 0.0220\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3538 - val_loss: 0.0249\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2003 - val_loss: 0.0247\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7141 - val_loss: 0.0271\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5053 - val_loss: 0.0253\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5097 - val_loss: 0.0296\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5577 - val_loss: 0.0267\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6429 - val_loss: 0.0325\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6642 - val_loss: 0.0283\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6578 - val_loss: 0.0367\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6464 - val_loss: 0.0291\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6573 - val_loss: 0.0402\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5737 - val_loss: 0.0314\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4340 - val_loss: 0.0424\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2921 - val_loss: 0.0356\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2260 - val_loss: 0.0445\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1550B670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_300 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 334ms/step - loss: 1.6075 - val_loss: 0.0437\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.4840 - val_loss: 0.0377\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 18.1758 - val_loss: 0.0296\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.2197 - val_loss: 0.0260\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5659 - val_loss: 0.0255\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 7.3316 - val_loss: 0.0204\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.4208 - val_loss: 0.0214\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.0184 - val_loss: 0.0182\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.7093 - val_loss: 0.0171\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.6374 - val_loss: 0.0164\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1851 - val_loss: 0.0181\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3083 - val_loss: 0.0168\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9169 - val_loss: 0.0179\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5001 - val_loss: 0.0166\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4187 - val_loss: 0.0193\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4939 - val_loss: 0.0158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5505 - val_loss: 0.0211\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4746 - val_loss: 0.0159\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3610 - val_loss: 0.0222\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2647 - val_loss: 0.0170\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2258 - val_loss: 0.0223\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2974 - val_loss: 0.0176\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5461 - val_loss: 0.0213\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8298 - val_loss: 0.0204\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8222 - val_loss: 0.0179\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7504 - val_loss: 0.0223\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6294 - val_loss: 0.0166\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6825 - val_loss: 0.0266\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8383 - val_loss: 0.0132\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0727 - val_loss: 0.0384\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0211 - val_loss: 0.0157\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0489 - val_loss: 0.0336\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1074 - val_loss: 0.0172\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9495 - val_loss: 0.0388\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.8627 - val_loss: 0.0154\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9427 - val_loss: 0.0399\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9246 - val_loss: 0.0157\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8660 - val_loss: 0.0476\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7927 - val_loss: 0.0193\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7772 - val_loss: 0.0301\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FDF3383A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_301 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 271ms/step - loss: 1.8357 - val_loss: 0.0290\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.5654 - val_loss: 0.0409\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 20.9534 - val_loss: 0.0362\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.9395 - val_loss: 0.0330\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.0459 - val_loss: 0.0290\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.1582 - val_loss: 0.0260\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.3052 - val_loss: 0.0240\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.5870 - val_loss: 0.0201\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.0693 - val_loss: 0.0206\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.2486 - val_loss: 0.0192\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8534 - val_loss: 0.0187\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 128ms/step - loss: 1.1184 - val_loss: 0.0204\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6521 - val_loss: 0.0189\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4025 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5127 - val_loss: 0.0192\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7564 - val_loss: 0.0190\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0834 - val_loss: 0.0222\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2045 - val_loss: 0.0164\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2005 - val_loss: 0.0260\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9075 - val_loss: 0.0162\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5778 - val_loss: 0.0286\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4621 - val_loss: 0.0160\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4505 - val_loss: 0.0322\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5195 - val_loss: 0.0175\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6307 - val_loss: 0.0346\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7091 - val_loss: 0.0183\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6217 - val_loss: 0.0333\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4976 - val_loss: 0.0215\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3737 - val_loss: 0.0315\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2849 - val_loss: 0.0247\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2566 - val_loss: 0.0312\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2804 - val_loss: 0.0270\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3556 - val_loss: 0.0314\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4626 - val_loss: 0.0287\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5443 - val_loss: 0.0364\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6406 - val_loss: 0.0240\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8518 - val_loss: 0.0450\n",
      "Epoch 00037: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD982BD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_302 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 1.7632 - val_loss: 0.0360\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 16.2715 - val_loss: 0.0414\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.7373 - val_loss: 0.0289\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.6347 - val_loss: 0.0276\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.8445 - val_loss: 0.0253\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.3862 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.9037 - val_loss: 0.0220\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.4615 - val_loss: 0.0201\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0829 - val_loss: 0.0179\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1307 - val_loss: 0.0202\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4126 - val_loss: 0.0161\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6441 - val_loss: 0.0179\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0974 - val_loss: 0.0147\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6318 - val_loss: 0.0171\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3106 - val_loss: 0.0144\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1225 - val_loss: 0.0161\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0714 - val_loss: 0.0147\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0719 - val_loss: 0.0158\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1075 - val_loss: 0.0149\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1780 - val_loss: 0.0159\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2892 - val_loss: 0.0148\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4095 - val_loss: 0.0157\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5161 - val_loss: 0.0150\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5982 - val_loss: 0.0153\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6808 - val_loss: 0.0151\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7304 - val_loss: 0.0139\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7190 - val_loss: 0.0184\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7312 - val_loss: 0.0112\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7306 - val_loss: 0.0219\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7663 - val_loss: 0.0100\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6603 - val_loss: 0.0200\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6491 - val_loss: 0.0113\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5807 - val_loss: 0.0177\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5287 - val_loss: 0.0142\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6084 - val_loss: 0.0116\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7547 - val_loss: 0.0247\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6848 - val_loss: 0.0104\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7484 - val_loss: 0.0310\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8092 - val_loss: 0.0089\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7597 - val_loss: 0.0303\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F50173790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_303 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 2.2966 - val_loss: 0.0610\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 23.4004 - val_loss: 0.0377\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 15.5565 - val_loss: 0.0351\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.1434 - val_loss: 0.0317\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.7021 - val_loss: 0.0288\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.5281 - val_loss: 0.0280\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.2911 - val_loss: 0.0238\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.7047 - val_loss: 0.0255\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.9974 - val_loss: 0.0209\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2481 - val_loss: 0.0226\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7138 - val_loss: 0.0222\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0743 - val_loss: 0.0217\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7149 - val_loss: 0.0230\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4691 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4671 - val_loss: 0.0246\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4901 - val_loss: 0.0197\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4177 - val_loss: 0.0261\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3796 - val_loss: 0.0195\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3353 - val_loss: 0.0268\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3536 - val_loss: 0.0196\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3817 - val_loss: 0.0270\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3868 - val_loss: 0.0208\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3759 - val_loss: 0.0256\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3889 - val_loss: 0.0218\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4120 - val_loss: 0.0232\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4433 - val_loss: 0.0236\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4485 - val_loss: 0.0205\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4285 - val_loss: 0.0262\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4274 - val_loss: 0.0166\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5178 - val_loss: 0.0333\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8131 - val_loss: 0.0110\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9871 - val_loss: 0.0426\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4082 - val_loss: 0.0069\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.8270 - val_loss: 0.0552\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4458 - val_loss: 0.0066\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8945 - val_loss: 0.0529\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7746 - val_loss: 0.0134\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9676 - val_loss: 0.0260\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0355 - val_loss: 0.0131\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8885 - val_loss: 0.0207\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE09FF8B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_304 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 352ms/step - loss: 1.9859 - val_loss: 0.0615\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 15.8190 - val_loss: 0.0275\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.4242 - val_loss: 0.0285\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 14.1207 - val_loss: 0.0232\n",
      "Epoch 5/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 37ms/step - loss: 9.6060 - val_loss: 0.0170\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.1505 - val_loss: 0.0145\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.9766 - val_loss: 0.0116\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.0674 - val_loss: 0.0100\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9654 - val_loss: 0.0077\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0704 - val_loss: 0.0078\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5107 - val_loss: 0.0064\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3348 - val_loss: 0.0063\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2987 - val_loss: 0.0057\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2968 - val_loss: 0.0054\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3003 - val_loss: 0.0052\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2895 - val_loss: 0.0050\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2866 - val_loss: 0.0050\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2896 - val_loss: 0.0050\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3577 - val_loss: 0.0049\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5177 - val_loss: 0.0056\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7985 - val_loss: 0.0050\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9319 - val_loss: 0.0064\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8887 - val_loss: 0.0053\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8144 - val_loss: 0.0075\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8451 - val_loss: 0.0079\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9129 - val_loss: 0.0058\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0976 - val_loss: 0.0144\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2122 - val_loss: 0.0060\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9162 - val_loss: 0.0121\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7521 - val_loss: 0.0082\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7766 - val_loss: 0.0111\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6493 - val_loss: 0.0127\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6263 - val_loss: 0.0089\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6525 - val_loss: 0.0160\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F9B49E5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_305 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 247ms/step - loss: 1.7028 - val_loss: 0.0468\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.7432 - val_loss: 0.0387\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 19.8022 - val_loss: 0.0273\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.7838 - val_loss: 0.0237\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.2254 - val_loss: 0.0179\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.9963 - val_loss: 0.0180\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.0434 - val_loss: 0.0145\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.0265 - val_loss: 0.0134\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.2682 - val_loss: 0.0124\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9373 - val_loss: 0.0118\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0064 - val_loss: 0.0109\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5007 - val_loss: 0.0109\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3052 - val_loss: 0.0087\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3297 - val_loss: 0.0101\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5576 - val_loss: 0.0070\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8218 - val_loss: 0.0099\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.8581 - val_loss: 0.0060\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5963 - val_loss: 0.0091\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3897 - val_loss: 0.0058\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2512 - val_loss: 0.0083\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2021 - val_loss: 0.0055\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2062 - val_loss: 0.0077\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2722 - val_loss: 0.0053\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3615 - val_loss: 0.0072\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4901 - val_loss: 0.0052\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6464 - val_loss: 0.0060\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7770 - val_loss: 0.0054\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7262 - val_loss: 0.0053\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5913 - val_loss: 0.0056\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5250 - val_loss: 0.0053\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5101 - val_loss: 0.0074\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5176 - val_loss: 0.0073\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6314 - val_loss: 0.0121\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8480 - val_loss: 0.0114\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1690 - val_loss: 0.0120\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3293 - val_loss: 0.0106\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5564 - val_loss: 0.0109\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8230 - val_loss: 0.0073\n",
      "Epoch 39/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6463 - val_loss: 0.0065\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5936 - val_loss: 0.0051\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FECEE30D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_306 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.2371 - val_loss: 0.0466\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 18.5561 - val_loss: 0.0256\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.9291 - val_loss: 0.0233\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 14.3751 - val_loss: 0.0191\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0208 - val_loss: 0.0159\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.8555 - val_loss: 0.0127\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.9280 - val_loss: 0.0103\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.4055 - val_loss: 0.0092\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9375 - val_loss: 0.0084\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.8260 - val_loss: 0.0081\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0568 - val_loss: 0.0078\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4792 - val_loss: 0.0076\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2904 - val_loss: 0.0070\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2525 - val_loss: 0.0069\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2852 - val_loss: 0.0063\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3096 - val_loss: 0.0063\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3214 - val_loss: 0.0059\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3152 - val_loss: 0.0060\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3710 - val_loss: 0.0056\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4682 - val_loss: 0.0060\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6217 - val_loss: 0.0055\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6519 - val_loss: 0.0061\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.7844 - val_loss: 0.0057\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8638 - val_loss: 0.0063\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.9796 - val_loss: 0.0065\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0320 - val_loss: 0.0067\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0366 - val_loss: 0.0065\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 1.044 - 0s 27ms/step - loss: 1.0441 - val_loss: 0.0058\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7908 - val_loss: 0.0064\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4698 - val_loss: 0.0056\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3744 - val_loss: 0.0058\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3355 - val_loss: 0.0060\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3271 - val_loss: 0.0056\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4020 - val_loss: 0.0077\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5333 - val_loss: 0.0060\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6189 - val_loss: 0.0081\n",
      "Epoch 00036: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F92C654C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_307 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 2.1281 - val_loss: 0.0429\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.4153 - val_loss: 0.0395\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 14.4847 - val_loss: 0.0331\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.8222 - val_loss: 0.0261\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.3459 - val_loss: 0.0239\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.0334 - val_loss: 0.0195\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.5690 - val_loss: 0.0191\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.0251 - val_loss: 0.0163\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.0343 - val_loss: 0.0172\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6756 - val_loss: 0.0136\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0142 - val_loss: 0.0146\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3587 - val_loss: 0.0126\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8183 - val_loss: 0.0133\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4640 - val_loss: 0.0115\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3420 - val_loss: 0.0127\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2979 - val_loss: 0.0104\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3605 - val_loss: 0.0127\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4661 - val_loss: 0.0095\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5497 - val_loss: 0.0131\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5022 - val_loss: 0.0089\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4092 - val_loss: 0.0130\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3508 - val_loss: 0.0082\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3127 - val_loss: 0.0126\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3028 - val_loss: 0.0078\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3098 - val_loss: 0.0123\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3617 - val_loss: 0.0073\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4377 - val_loss: 0.0121\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5106 - val_loss: 0.0071\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5373 - val_loss: 0.0112\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5860 - val_loss: 0.0073\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7319 - val_loss: 0.0086\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1598 - val_loss: 0.0077\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3151 - val_loss: 0.0086\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1636 - val_loss: 0.0074\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9791 - val_loss: 0.0111\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.8139 - val_loss: 0.0075\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9673 - val_loss: 0.0158\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5945 - val_loss: 0.0058\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9929 - val_loss: 0.0119\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8218 - val_loss: 0.0070\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F19BFC1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_308 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.7611 - val_loss: 0.0353\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.4026 - val_loss: 0.0492\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 16.2949 - val_loss: 0.0277\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 13.3633 - val_loss: 0.0297\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.1058 - val_loss: 0.0263\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.8539 - val_loss: 0.0270\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3507 - val_loss: 0.0254\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.9928 - val_loss: 0.0213\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4198 - val_loss: 0.0229\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.3861 - val_loss: 0.0181\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7227 - val_loss: 0.0202\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2650 - val_loss: 0.0178\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1669 - val_loss: 0.0178\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2256 - val_loss: 0.0166\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3985 - val_loss: 0.0152\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5867 - val_loss: 0.0163\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7009 - val_loss: 0.0135\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6063 - val_loss: 0.0167\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5171 - val_loss: 0.0124\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4891 - val_loss: 0.0177\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4700 - val_loss: 0.0111\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4085 - val_loss: 0.0179\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4045 - val_loss: 0.0106\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4598 - val_loss: 0.0176\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6665 - val_loss: 0.0116\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9394 - val_loss: 0.0151\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0474 - val_loss: 0.0148\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0563 - val_loss: 0.0118\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1127 - val_loss: 0.0175\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2554 - val_loss: 0.0088\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4480 - val_loss: 0.0205\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3626 - val_loss: 0.0081\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1160 - val_loss: 0.0250\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0001 - val_loss: 0.0091\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8437 - val_loss: 0.0207\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6566 - val_loss: 0.0129\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5567 - val_loss: 0.0179\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4083 - val_loss: 0.0150\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3483 - val_loss: 0.0171\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3046 - val_loss: 0.0166\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70E99820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_309 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 615ms/step - loss: 1.8318 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.6398 - val_loss: 0.0488\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 19.5729 - val_loss: 0.0347\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.2955 - val_loss: 0.0271\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.7721 - val_loss: 0.0265\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.7026 - val_loss: 0.0186\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8490 - val_loss: 0.0227\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.2511 - val_loss: 0.0157\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.9958 - val_loss: 0.0175\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.9733 - val_loss: 0.0153\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1590 - val_loss: 0.0130\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.5960 - val_loss: 0.0145\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1127 - val_loss: 0.0118\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9249 - val_loss: 0.0124\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.7330 - val_loss: 0.0116\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.5654 - val_loss: 0.0110\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4107 - val_loss: 0.0108\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3030 - val_loss: 0.0099\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1675 - val_loss: 0.0101\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1060 - val_loss: 0.0092\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.0759 - val_loss: 0.0095\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0801 - val_loss: 0.0087\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1303 - val_loss: 0.0091\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2880 - val_loss: 0.0082\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6493 - val_loss: 0.0088\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9943 - val_loss: 0.0076\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.9154 - val_loss: 0.0086\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7000 - val_loss: 0.0072\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5861 - val_loss: 0.0081\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5215 - val_loss: 0.0070\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5141 - val_loss: 0.0076\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5479 - val_loss: 0.0074\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5714 - val_loss: 0.0068\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5362 - val_loss: 0.0079\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6233 - val_loss: 0.0065\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6124 - val_loss: 0.0071\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7180 - val_loss: 0.0060\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.8566 - val_loss: 0.0073\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8255 - val_loss: 0.0060\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8212 - val_loss: 0.0061\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA34B5160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_310 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 2.1629 - val_loss: 0.0485\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.2810 - val_loss: 0.0376\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 13.2631 - val_loss: 0.0281\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.8493 - val_loss: 0.0296\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 9.8292 - val_loss: 0.0220\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 7.793 - 0s 17ms/step - loss: 7.7934 - val_loss: 0.0229\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.2579 - val_loss: 0.0172\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.6796 - val_loss: 0.0234\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.9936 - val_loss: 0.0178\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7776 - val_loss: 0.0213\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5842 - val_loss: 0.0187\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0345 - val_loss: 0.0201\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6904 - val_loss: 0.0196\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5635 - val_loss: 0.0206\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4756 - val_loss: 0.0200\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3100 - val_loss: 0.0227\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2306 - val_loss: 0.0210\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2009 - val_loss: 0.0252\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2153 - val_loss: 0.0220\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2665 - val_loss: 0.0276\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3411 - val_loss: 0.0229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4072 - val_loss: 0.0296\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8E0110D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_311 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 1.7480 - val_loss: 0.0400\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.8442 - val_loss: 0.0396\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.8738 - val_loss: 0.0288\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.2142 - val_loss: 0.0262\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.1803 - val_loss: 0.0242\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.0126 - val_loss: 0.0219\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.9275 - val_loss: 0.0225\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.2294 - val_loss: 0.0193\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.0825 - val_loss: 0.0218\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.4721 - val_loss: 0.0187\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5892 - val_loss: 0.0227\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9564 - val_loss: 0.0200\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6796 - val_loss: 0.0231\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 0.6306 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 88ms/step - loss: 0.5391 - val_loss: 0.0239\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4037 - val_loss: 0.0204\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 0.2890 - val_loss: 0.0244\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2149 - val_loss: 0.0217\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1790 - val_loss: 0.0254\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1739 - val_loss: 0.0233\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2193 - val_loss: 0.0266\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3372 - val_loss: 0.0248\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5461 - val_loss: 0.0259\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7389 - val_loss: 0.0239\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9557 - val_loss: 0.0249\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA37EF4C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_312 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 1.4384 - val_loss: 0.0477\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.8986 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14.4951 - val_loss: 0.0300\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.1454 - val_loss: 0.0232\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.5311 - val_loss: 0.0206\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.9282 - val_loss: 0.0174\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.7176 - val_loss: 0.0176\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.9858 - val_loss: 0.0158\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.5979 - val_loss: 0.0165\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1753 - val_loss: 0.0159\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7508 - val_loss: 0.0151\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1256 - val_loss: 0.0159\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6376 - val_loss: 0.0150\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4115 - val_loss: 0.0161\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2424 - val_loss: 0.0157\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2014 - val_loss: 0.0163\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2111 - val_loss: 0.0167\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2266 - val_loss: 0.0169\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2299 - val_loss: 0.0174\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2335 - val_loss: 0.0180\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2492 - val_loss: 0.0182\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2891 - val_loss: 0.0195\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4000 - val_loss: 0.0178\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5156 - val_loss: 0.0212\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5926 - val_loss: 0.0164\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7454 - val_loss: 0.0263\n",
      "Epoch 27/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9884 - val_loss: 0.0141\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1748 - val_loss: 0.0302\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2089 - val_loss: 0.0131\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1104 - val_loss: 0.0311\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8768 - val_loss: 0.0135\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9249 - val_loss: 0.0311\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8081 - val_loss: 0.0151\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6363 - val_loss: 0.0292\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6691 - val_loss: 0.0171\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7329 - val_loss: 0.0243\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7492 - val_loss: 0.0207\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7678 - val_loss: 0.0194\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7101 - val_loss: 0.0228\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5766 - val_loss: 0.0210\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FF0C59DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_313 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1.4307 - val_loss: 0.0428\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.1603 - val_loss: 0.0348\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.4518 - val_loss: 0.0273\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.7707 - val_loss: 0.0233\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.0115 - val_loss: 0.0221\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.5384 - val_loss: 0.0192\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.8495 - val_loss: 0.0185\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.6856 - val_loss: 0.0164\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.9537 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7420 - val_loss: 0.0140\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2397 - val_loss: 0.0144\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2058 - val_loss: 0.0118\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8106 - val_loss: 0.0120\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.7110 - val_loss: 0.0108\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5266 - val_loss: 0.0109\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4199 - val_loss: 0.0103\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6032 - val_loss: 0.0105\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0804 - val_loss: 0.0096\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1080 - val_loss: 0.0104\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0171 - val_loss: 0.0095\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8483 - val_loss: 0.0103\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.6557 - val_loss: 0.0095\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4811 - val_loss: 0.0101\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3668 - val_loss: 0.0095\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2992 - val_loss: 0.0098\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.2761 - val_loss: 0.0095\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2600 - val_loss: 0.0096\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2668 - val_loss: 0.0095\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2872 - val_loss: 0.0095\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3115 - val_loss: 0.0095\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3462 - val_loss: 0.0096\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3802 - val_loss: 0.0095\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4335 - val_loss: 0.0098\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5280 - val_loss: 0.0096\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6123 - val_loss: 0.0109\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7281 - val_loss: 0.0095\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8167 - val_loss: 0.0110\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9809 - val_loss: 0.0111\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3574 - val_loss: 0.0185\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.9313 - val_loss: 0.0130\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBC5EE430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_314 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 416ms/step - loss: 1.8396 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.6879 - val_loss: 0.0345\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 18.7920 - val_loss: 0.0226\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.2168 - val_loss: 0.0207\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.2502 - val_loss: 0.0187\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.2255 - val_loss: 0.0176\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.4852 - val_loss: 0.0180\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1878 - val_loss: 0.0159\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7328 - val_loss: 0.0160\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3541 - val_loss: 0.0151\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.2811 - val_loss: 0.0148\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2898 - val_loss: 0.0148\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2287 - val_loss: 0.0142\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9696 - val_loss: 0.0146\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6415 - val_loss: 0.0134\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4564 - val_loss: 0.0147\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3655 - val_loss: 0.0129\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4155 - val_loss: 0.0147\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4534 - val_loss: 0.0127\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4180 - val_loss: 0.0145\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2963 - val_loss: 0.0127\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2133 - val_loss: 0.0140\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1620 - val_loss: 0.0128\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1707 - val_loss: 0.0130\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2981 - val_loss: 0.0134\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5411 - val_loss: 0.0120\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7175 - val_loss: 0.0138\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7020 - val_loss: 0.0120\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5173 - val_loss: 0.0135\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5025 - val_loss: 0.0115\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4716 - val_loss: 0.0129\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3752 - val_loss: 0.0117\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4285 - val_loss: 0.0127\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6806 - val_loss: 0.0116\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1190 - val_loss: 0.0122\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3008 - val_loss: 0.0123\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4741 - val_loss: 0.0115\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7308 - val_loss: 0.0117\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4309 - val_loss: 0.0125\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3149 - val_loss: 0.0106\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FDDCAD3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_315 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 465ms/step - loss: 1.7074 - val_loss: 0.0455\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 23.1240 - val_loss: 0.0242\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 20.9218 - val_loss: 0.0252\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.8450 - val_loss: 0.0243\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.5918 - val_loss: 0.0199\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.8618 - val_loss: 0.0216\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.0453 - val_loss: 0.0181\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.0725 - val_loss: 0.0199\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5068 - val_loss: 0.0178\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5839 - val_loss: 0.0196\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4610 - val_loss: 0.0190\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5168 - val_loss: 0.0191\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2639 - val_loss: 0.0206\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0874 - val_loss: 0.0181\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6438 - val_loss: 0.0206\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3463 - val_loss: 0.0180\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2367 - val_loss: 0.0209\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1699 - val_loss: 0.0176\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.1660 - val_loss: 0.0207\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1693 - val_loss: 0.0170\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2045 - val_loss: 0.0214\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2748 - val_loss: 0.0163\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4497 - val_loss: 0.0232\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6680 - val_loss: 0.0153\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7737 - val_loss: 0.0237\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7060 - val_loss: 0.0159\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6823 - val_loss: 0.0201\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6973 - val_loss: 0.0183\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6382 - val_loss: 0.0191\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6128 - val_loss: 0.0185\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6736 - val_loss: 0.0179\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7501 - val_loss: 0.0166\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0063 - val_loss: 0.0207\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9867 - val_loss: 0.0153\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8738 - val_loss: 0.0251\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6207 - val_loss: 0.0135\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9962 - val_loss: 0.0257\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2173 - val_loss: 0.0141\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3605 - val_loss: 0.0252\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2801 - val_loss: 0.0135\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70EA6AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_316 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 2.5549 - val_loss: 0.0342\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10.3002 - val_loss: 0.0349\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.1079 - val_loss: 0.0252\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.6792 - val_loss: 0.0238\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.0686 - val_loss: 0.0243\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.0380 - val_loss: 0.0217\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.2142 - val_loss: 0.0224\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.5347 - val_loss: 0.0211\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.8214 - val_loss: 0.0220\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9742 - val_loss: 0.0208\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.8435 - val_loss: 0.0211\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.3287 - val_loss: 0.0207\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2605 - val_loss: 0.0217\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7841 - val_loss: 0.0201\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5013 - val_loss: 0.0210\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2399 - val_loss: 0.0195\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1386 - val_loss: 0.0206\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1031 - val_loss: 0.0191\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1067 - val_loss: 0.0205\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1120 - val_loss: 0.0187\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1272 - val_loss: 0.0203\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1638 - val_loss: 0.0181\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2708 - val_loss: 0.0216\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4887 - val_loss: 0.0163\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.8328 - val_loss: 0.0252\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1431 - val_loss: 0.0152\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.2934 - val_loss: 0.0267\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9951 - val_loss: 0.0153\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9739 - val_loss: 0.0309\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6371 - val_loss: 0.0162\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4797 - val_loss: 0.0242\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4178 - val_loss: 0.0184\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3112 - val_loss: 0.0208\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2180 - val_loss: 0.0204\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1808 - val_loss: 0.0194\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2163 - val_loss: 0.0215\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3191 - val_loss: 0.0187\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4591 - val_loss: 0.0220\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5061 - val_loss: 0.0182\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4634 - val_loss: 0.0229\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1A8AF280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_317 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.9284 - val_loss: 0.0278\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step - loss: 13.1852 - val_loss: 0.0446\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.2885 - val_loss: 0.0294\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.4707 - val_loss: 0.0251\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.2468 - val_loss: 0.0259\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.6428 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.6225 - val_loss: 0.0230\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.0331 - val_loss: 0.0221\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2908 - val_loss: 0.0212\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.1190 - val_loss: 0.0212\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2397 - val_loss: 0.0202\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6554 - val_loss: 0.0204\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4747 - val_loss: 0.0203\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6283 - val_loss: 0.0206\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8714 - val_loss: 0.0205\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7754 - val_loss: 0.0214\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5683 - val_loss: 0.0216\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3402 - val_loss: 0.0214\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2127 - val_loss: 0.0222\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.1679 - val_loss: 0.0214\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1728 - val_loss: 0.0227\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2092 - val_loss: 0.0222\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2738 - val_loss: 0.0230\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3469 - val_loss: 0.0232\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4010 - val_loss: 0.0232\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4281 - val_loss: 0.0239\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE3D37670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_318 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.3869 - val_loss: 0.0422\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 24.7551 - val_loss: 0.0296\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 18.6366 - val_loss: 0.0292\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.9259 - val_loss: 0.0269\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.1063 - val_loss: 0.0253\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.7970 - val_loss: 0.0232\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9366 - val_loss: 0.0242\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 4.3845 - val_loss: 0.0229\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8630 - val_loss: 0.0238\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2331 - val_loss: 0.0224\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3359 - val_loss: 0.0239\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6443 - val_loss: 0.0226\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3678 - val_loss: 0.0235\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3149 - val_loss: 0.0229\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3881 - val_loss: 0.0230\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4606 - val_loss: 0.0232\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.5440 - val_loss: 0.0226\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5232 - val_loss: 0.0237\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4920 - val_loss: 0.0216\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4375 - val_loss: 0.0247\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4183 - val_loss: 0.0206\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4145 - val_loss: 0.0260\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4192 - val_loss: 0.0201\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4190 - val_loss: 0.0260\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4298 - val_loss: 0.0200\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4291 - val_loss: 0.0248\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4825 - val_loss: 0.0207\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5484 - val_loss: 0.0219\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6687 - val_loss: 0.0241\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9120 - val_loss: 0.0192\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2551 - val_loss: 0.0313\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4370 - val_loss: 0.0182\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5342 - val_loss: 0.0329\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7219 - val_loss: 0.0181\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.0990 - val_loss: 0.0375\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.2540 - val_loss: 0.0186\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0712 - val_loss: 0.0240\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.5588 - val_loss: 0.0185\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0911 - val_loss: 0.0235\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.8470 - val_loss: 0.0182\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002501747D160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_319 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 495ms/step - loss: 3.3527 - val_loss: 0.0331\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.9801 - val_loss: 0.0349\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 16.0269 - val_loss: 0.0278\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.5301 - val_loss: 0.0236\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.0236 - val_loss: 0.0238\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.4321 - val_loss: 0.0227\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.5746 - val_loss: 0.0226\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.7364 - val_loss: 0.0205\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.8817 - val_loss: 0.0212\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.5721 - val_loss: 0.0207\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6948 - val_loss: 0.0204\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8159 - val_loss: 0.0201\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4302 - val_loss: 0.0200\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3264 - val_loss: 0.0199\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2859 - val_loss: 0.0200\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2905 - val_loss: 0.0199\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3326 - val_loss: 0.0200\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4383 - val_loss: 0.0198\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6746 - val_loss: 0.0201\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9130 - val_loss: 0.0199\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9226 - val_loss: 0.0202\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6824 - val_loss: 0.0200\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4905 - val_loss: 0.0202\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3700 - val_loss: 0.0201\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3042 - val_loss: 0.0201\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.2508 - val_loss: 0.0201\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2378 - val_loss: 0.0200\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2587 - val_loss: 0.0201\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3049 - val_loss: 0.0199\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3699 - val_loss: 0.0201\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4736 - val_loss: 0.0198\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5521 - val_loss: 0.0198\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6256 - val_loss: 0.0201\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EFA98B700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_320 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 1.9415 - val_loss: 0.0336\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.8215 - val_loss: 0.0305\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 21.0400 - val_loss: 0.0279\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.2686 - val_loss: 0.0252\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.9599 - val_loss: 0.0257\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.0882 - val_loss: 0.0247\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.7742 - val_loss: 0.0252\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.9802 - val_loss: 0.0240\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.4766 - val_loss: 0.0251\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.6373 - val_loss: 0.0235\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.0919 - val_loss: 0.0241\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.6977 - val_loss: 0.0232\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.3893 - val_loss: 0.0234\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3295 - val_loss: 0.0230\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.4521 - val_loss: 0.0233\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5459 - val_loss: 0.0229\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5729 - val_loss: 0.0234\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4138 - val_loss: 0.0229\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3296 - val_loss: 0.0234\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2851 - val_loss: 0.0230\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3032 - val_loss: 0.0233\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3379 - val_loss: 0.0230\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 0.3707 - val_loss: 0.0232\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3802 - val_loss: 0.0232\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3759 - val_loss: 0.0230\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3589 - val_loss: 0.0235\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3522 - val_loss: 0.0230\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3691 - val_loss: 0.0240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4030 - val_loss: 0.0231\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4264 - val_loss: 0.0242\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4583 - val_loss: 0.0234\n",
      "Epoch 00031: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F455B5CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_321 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 2.2487 - val_loss: 0.0357\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.1934 - val_loss: 0.0329\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 18.5996 - val_loss: 0.0308\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.5260 - val_loss: 0.0325\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.0589 - val_loss: 0.0290\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.7749 - val_loss: 0.0289\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.2724 - val_loss: 0.0284\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.3451 - val_loss: 0.0267\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0683 - val_loss: 0.0278\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.2220 - val_loss: 0.0267\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.6696 - val_loss: 0.0271\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0502 - val_loss: 0.0266\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6964 - val_loss: 0.0268\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7075 - val_loss: 0.0269\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6769 - val_loss: 0.0268\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4887 - val_loss: 0.0269\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2821 - val_loss: 0.0271\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1584 - val_loss: 0.0270\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1133 - val_loss: 0.0275\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1370 - val_loss: 0.0269\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2611 - val_loss: 0.0291\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4946 - val_loss: 0.0266\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7461 - val_loss: 0.0312\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8673 - val_loss: 0.0265\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9063 - val_loss: 0.0306\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8891 - val_loss: 0.0272\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7777 - val_loss: 0.0289\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6868 - val_loss: 0.0287\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7796 - val_loss: 0.0279\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7537 - val_loss: 0.0310\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7566 - val_loss: 0.0272\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7556 - val_loss: 0.0339\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5678 - val_loss: 0.0271\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4659 - val_loss: 0.0351\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3983 - val_loss: 0.0266\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3979 - val_loss: 0.0342\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4732 - val_loss: 0.0275\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7178 - val_loss: 0.0308\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7802 - val_loss: 0.0302\n",
      "Epoch 00039: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1F7680D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_322 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 1.7148 - val_loss: 0.0379\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.1546 - val_loss: 0.0366\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 19.6835 - val_loss: 0.0331\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.6269 - val_loss: 0.0303\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.1413 - val_loss: 0.0300\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8185 - val_loss: 0.0296\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.3282 - val_loss: 0.0294\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3428 - val_loss: 0.0295\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.8230 - val_loss: 0.0296\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9843 - val_loss: 0.0301\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2534 - val_loss: 0.0303\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6659 - val_loss: 0.0307\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3578 - val_loss: 0.0312\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2790 - val_loss: 0.0311\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2453 - val_loss: 0.0316\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2231 - val_loss: 0.0316\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2126 - val_loss: 0.0319\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2201 - val_loss: 0.0324\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2383 - val_loss: 0.0318\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2831 - val_loss: 0.0335\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4473 - val_loss: 0.0309\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9447 - val_loss: 0.0360\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEAAB4820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_323 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 333ms/step - loss: 1.5709 - val_loss: 0.0370\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.8797 - val_loss: 0.0362\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 12.8980 - val_loss: 0.0340\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.2894 - val_loss: 0.0321\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2845 - val_loss: 0.0311\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.3297 - val_loss: 0.0315\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.2112 - val_loss: 0.0309\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.7004 - val_loss: 0.0310\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8146 - val_loss: 0.0305\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5237 - val_loss: 0.0306\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2477 - val_loss: 0.0305\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4855 - val_loss: 0.0306\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8179 - val_loss: 0.0306\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6534 - val_loss: 0.0307\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5732 - val_loss: 0.0307\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5599 - val_loss: 0.0306\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5733 - val_loss: 0.0306\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4886 - val_loss: 0.0306\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3257 - val_loss: 0.0306\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2264 - val_loss: 0.0306\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1753 - val_loss: 0.0307\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1781 - val_loss: 0.0307\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2144 - val_loss: 0.0309\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2824 - val_loss: 0.0310\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E99CF1F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_324 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 462ms/step - loss: 1.7457 - val_loss: 0.0400\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6174 - val_loss: 0.0421\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 16.8976 - val_loss: 0.0367\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 13.7622 - val_loss: 0.0349\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.3018 - val_loss: 0.0349\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.6327 - val_loss: 0.0329\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.0731 - val_loss: 0.0332\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 3.5907 - val_loss: 0.0323\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4037 - val_loss: 0.0325\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.0051 - val_loss: 0.0318\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.3615 - val_loss: 0.0320\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.8305 - val_loss: 0.0319\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.2873 - val_loss: 0.0318\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6898 - val_loss: 0.0318\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3514 - val_loss: 0.0320\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2382 - val_loss: 0.0318\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2200 - val_loss: 0.0321\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2393 - val_loss: 0.0319\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2590 - val_loss: 0.0324\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2835 - val_loss: 0.0318\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3356 - val_loss: 0.0331\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4122 - val_loss: 0.0317\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4982 - val_loss: 0.0341\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4945 - val_loss: 0.0316\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4315 - val_loss: 0.0345\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3704 - val_loss: 0.0316\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3472 - val_loss: 0.0349\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3591 - val_loss: 0.0316\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4442 - val_loss: 0.0352\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5874 - val_loss: 0.0315\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7156 - val_loss: 0.0345\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8381 - val_loss: 0.0316\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8522 - val_loss: 0.0324\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7652 - val_loss: 0.0319\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7137 - val_loss: 0.0315\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5568 - val_loss: 0.0353\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6613 - val_loss: 0.0349\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9689 - val_loss: 0.0445\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.1141 - val_loss: 0.0394\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9131 - val_loss: 0.0382\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1F7E1160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_325 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 351ms/step - loss: 2.0067 - val_loss: 0.0400\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.6706 - val_loss: 0.0403\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 19.0120 - val_loss: 0.0344\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 10.0981 - val_loss: 0.0341\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 12.4794 - val_loss: 0.0331\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.6262 - val_loss: 0.0328\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.8082 - val_loss: 0.0328\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.7193 - val_loss: 0.0332\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.1875 - val_loss: 0.0330\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.1284 - val_loss: 0.0334\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2186 - val_loss: 0.0333\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6405 - val_loss: 0.0335\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4542 - val_loss: 0.0337\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3773 - val_loss: 0.0340\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3019 - val_loss: 0.0341\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1956 - val_loss: 0.0345\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1313 - val_loss: 0.0344\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0970 - val_loss: 0.0350\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0939 - val_loss: 0.0347\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1308 - val_loss: 0.0357\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2720 - val_loss: 0.0348\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE3CB6430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_326 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 2.2899 - val_loss: 0.0379\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.9462 - val_loss: 0.0443\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 26.3072 - val_loss: 0.0355\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 9.3552 - val_loss: 0.0353\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.0043 - val_loss: 0.0342\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.3489 - val_loss: 0.0337\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.2234 - val_loss: 0.0336\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.9973 - val_loss: 0.0335\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5426 - val_loss: 0.0335\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 2.9485 - val_loss: 0.0334\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8323 - val_loss: 0.0334\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0929 - val_loss: 0.0335\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4243 - val_loss: 0.0335\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1793 - val_loss: 0.0335\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2297 - val_loss: 0.0335\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3869 - val_loss: 0.0335\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5496 - val_loss: 0.0336\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6383 - val_loss: 0.0335\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6891 - val_loss: 0.0337\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5922 - val_loss: 0.0336\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4908 - val_loss: 0.0337\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4080 - val_loss: 0.0337\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4109 - val_loss: 0.0336\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5017 - val_loss: 0.0341\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6520 - val_loss: 0.0336\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7578 - val_loss: 0.0356\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002500C5C4B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_327 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 1.6887 - val_loss: 0.0415\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.8402 - val_loss: 0.0401\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 21.1530 - val_loss: 0.0345\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.3220 - val_loss: 0.0351\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.9307 - val_loss: 0.0341\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.0055 - val_loss: 0.0341\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.7993 - val_loss: 0.0337\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.8938 - val_loss: 0.0339\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.2919 - val_loss: 0.0338\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.0722 - val_loss: 0.0336\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.2568 - val_loss: 0.0337\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8028 - val_loss: 0.0336\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9682 - val_loss: 0.0336\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9621 - val_loss: 0.0337\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6441 - val_loss: 0.0337\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4143 - val_loss: 0.0337\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2911 - val_loss: 0.0339\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2982 - val_loss: 0.0337\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4488 - val_loss: 0.0341\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6151 - val_loss: 0.0337\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6545 - val_loss: 0.0339\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5319 - val_loss: 0.0341\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4347 - val_loss: 0.0338\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 0.4156 - val_loss: 0.0343\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.4539 - val_loss: 0.0338\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5308 - val_loss: 0.0339\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5564 - val_loss: 0.0341\n",
      "Epoch 00027: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025020A20550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_328 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 315ms/step - loss: 2.4574 - val_loss: 0.0406\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.8829 - val_loss: 0.0406\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 33.7782 - val_loss: 0.0350\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.4345 - val_loss: 0.0358\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.4553 - val_loss: 0.0345\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.5526 - val_loss: 0.0344\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.0600 - val_loss: 0.0348\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.8735 - val_loss: 0.0347\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.6820 - val_loss: 0.0358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7436 - val_loss: 0.0353\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7306 - val_loss: 0.0357\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3507 - val_loss: 0.0358\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1790 - val_loss: 0.0357\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2060 - val_loss: 0.0363\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2799 - val_loss: 0.0357\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3566 - val_loss: 0.0365\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4071 - val_loss: 0.0356\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3635 - val_loss: 0.0366\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3354 - val_loss: 0.0358\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3765 - val_loss: 0.0369\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5904 - val_loss: 0.0366\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB91D7EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_329 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 426ms/step - loss: 2.4243 - val_loss: 0.0404\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 12.8328 - val_loss: 0.0394\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 32.0234 - val_loss: 0.0371\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.1517 - val_loss: 0.0359\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2611 - val_loss: 0.0356\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.7917 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.7451 - val_loss: 0.0354\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.1295 - val_loss: 0.0354\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.9470 - val_loss: 0.0354\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.3095 - val_loss: 0.0353\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2786 - val_loss: 0.0355\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7095 - val_loss: 0.0354\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4795 - val_loss: 0.0357\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3614 - val_loss: 0.0355\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3656 - val_loss: 0.0357\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3992 - val_loss: 0.0356\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5348 - val_loss: 0.0356\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6839 - val_loss: 0.0357\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7963 - val_loss: 0.0356\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6345 - val_loss: 0.0358\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5314 - val_loss: 0.0357\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F226D0820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_330 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 472ms/step - loss: 1.9613 - val_loss: 0.0388\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 12.9210 - val_loss: 0.0398\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 27.2751 - val_loss: 0.0369\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.4059 - val_loss: 0.0369\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.8552 - val_loss: 0.0365\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.3319 - val_loss: 0.0364\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5026 - val_loss: 0.0365\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9425 - val_loss: 0.0364\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.9784 - val_loss: 0.0364\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.3550 - val_loss: 0.0365\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.8846 - val_loss: 0.0364\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2042 - val_loss: 0.0364\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7183 - val_loss: 0.0364\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3952 - val_loss: 0.0364\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2044 - val_loss: 0.0365\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1081 - val_loss: 0.0364\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0685 - val_loss: 0.0365\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0564 - val_loss: 0.0364\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0611 - val_loss: 0.0365\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 76ms/step - loss: 0.0874 - val_loss: 0.0364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1615 - val_loss: 0.0367\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3307 - val_loss: 0.0370\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5960 - val_loss: 0.0372\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE43543A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_331 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 573ms/step - loss: 2.4850 - val_loss: 0.0410\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 12.0570 - val_loss: 0.0382\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 17.5214 - val_loss: 0.0379\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.3279 - val_loss: 0.0381\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.0286 - val_loss: 0.0381\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.8567 - val_loss: 0.0383\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.4145 - val_loss: 0.0384\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.1440 - val_loss: 0.0382\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4410 - val_loss: 0.0386\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8532 - val_loss: 0.0378\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4008 - val_loss: 0.0382\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9579 - val_loss: 0.0379\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2315 - val_loss: 0.0380\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7612 - val_loss: 0.0378\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4374 - val_loss: 0.0378\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2442 - val_loss: 0.0378\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1823 - val_loss: 0.0378\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.1865 - val_loss: 0.0378\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2050 - val_loss: 0.0379\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2524 - val_loss: 0.0379\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3178 - val_loss: 0.0379\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3934 - val_loss: 0.0380\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5092 - val_loss: 0.0383\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6701 - val_loss: 0.0384\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8190 - val_loss: 0.0387\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9502 - val_loss: 0.0385\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9337 - val_loss: 0.0384\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9296 - val_loss: 0.0379\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7833 - val_loss: 0.0381\n",
      "Epoch 00029: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F8C0DADC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_332 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 1.7716 - val_loss: 0.0412\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.8653 - val_loss: 0.0390\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 18.8063 - val_loss: 0.0373\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.2215 - val_loss: 0.0374\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.0740 - val_loss: 0.0375\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.8068 - val_loss: 0.0375\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.3006 - val_loss: 0.0379\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.0182 - val_loss: 0.0377\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9685 - val_loss: 0.0379\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.4696 - val_loss: 0.0376\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6403 - val_loss: 0.0380\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7285 - val_loss: 0.0376\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0585 - val_loss: 0.0378\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6006 - val_loss: 0.0373\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4860 - val_loss: 0.0379\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5205 - val_loss: 0.0372\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6831 - val_loss: 0.0377\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9568 - val_loss: 0.0372\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1147 - val_loss: 0.0372\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9123 - val_loss: 0.0372\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7970 - val_loss: 0.0373\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6027 - val_loss: 0.0372\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4387 - val_loss: 0.0378\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3166 - val_loss: 0.0372\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2614 - val_loss: 0.0382\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2782 - val_loss: 0.0372\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3267 - val_loss: 0.0386\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4150 - val_loss: 0.0374\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4675 - val_loss: 0.0389\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4998 - val_loss: 0.0377\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4914 - val_loss: 0.0394\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5168 - val_loss: 0.0377\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6081 - val_loss: 0.0415\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E9D1F3AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_333 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 421ms/step - loss: 2.8545 - val_loss: 0.0391\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.1072 - val_loss: 0.0374\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.6641 - val_loss: 0.0375\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.7461 - val_loss: 0.0374\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.4386 - val_loss: 0.0376\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.2195 - val_loss: 0.0375\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.5095 - val_loss: 0.0382\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.5570 - val_loss: 0.0377\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.9695 - val_loss: 0.0377\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.8022 - val_loss: 0.0375\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.5317 - val_loss: 0.0376\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.5569 - val_loss: 0.0375\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6858 - val_loss: 0.0377\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5938 - val_loss: 0.0381\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0244 - val_loss: 0.0385\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4880 - val_loss: 0.0390\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2509 - val_loss: 0.0393\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2303 - val_loss: 0.0404\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3106 - val_loss: 0.0406\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1DD15E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_334 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 509ms/step - loss: 2.1291 - val_loss: 0.0420\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 19.2144 - val_loss: 0.0375\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 15.2572 - val_loss: 0.0371\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 13.3502 - val_loss: 0.0370\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.3824 - val_loss: 0.0389\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.8207 - val_loss: 0.0386\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.2607 - val_loss: 0.0411\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.3312 - val_loss: 0.0405\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.5007 - val_loss: 0.0423\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.0670 - val_loss: 0.0402\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1543 - val_loss: 0.0422\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0365 - val_loss: 0.0405\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7859 - val_loss: 0.0421\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4470 - val_loss: 0.0410\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3543 - val_loss: 0.0418\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3723 - val_loss: 0.0410\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4151 - val_loss: 0.0414\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4800 - val_loss: 0.0407\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5549 - val_loss: 0.0412\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1550BEE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_335 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2.0495 - val_loss: 0.0383\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.0159 - val_loss: 0.0373\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 22.1723 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 160ms/step - loss: 10.2581 - val_loss: 0.0391\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.8539 - val_loss: 0.0399\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.4607 - val_loss: 0.0410\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.0002 - val_loss: 0.0434\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.0056 - val_loss: 0.0443\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.3726 - val_loss: 0.0470\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2403 - val_loss: 0.0475\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0000 - val_loss: 0.0495\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.8120 - val_loss: 0.0509\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2097 - val_loss: 0.0504\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9830 - val_loss: 0.0550\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0324 - val_loss: 0.0517\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9089 - val_loss: 0.0570\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6722 - val_loss: 0.0558\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025000685160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_336 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 1.6093 - val_loss: 0.0377\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.0489 - val_loss: 0.0384\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 17.2060 - val_loss: 0.0392\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 15.0567 - val_loss: 0.0403\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.2729 - val_loss: 0.0404\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.4673 - val_loss: 0.0425\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.4731 - val_loss: 0.0435\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.3870 - val_loss: 0.0441\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.1088 - val_loss: 0.0469\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.3233 - val_loss: 0.0467\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1854 - val_loss: 0.0506\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7079 - val_loss: 0.0496\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4470 - val_loss: 0.0543\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3746 - val_loss: 0.0526\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3115 - val_loss: 0.0582\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2802 - val_loss: 0.0560\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025013FD1C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_337 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 1.5024 - val_loss: 0.0398\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.5964 - val_loss: 0.0419\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 19.6357 - val_loss: 0.0385\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 15.8706 - val_loss: 0.0409\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.5998 - val_loss: 0.0393\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.0515 - val_loss: 0.0405\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.3762 - val_loss: 0.0402\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4146 - val_loss: 0.0412\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.5736 - val_loss: 0.0408\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7739 - val_loss: 0.0420\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9511 - val_loss: 0.0412\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7038 - val_loss: 0.0418\n",
      "Epoch 13/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4516 - val_loss: 0.0416\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2930 - val_loss: 0.0415\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2339 - val_loss: 0.0420\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2866 - val_loss: 0.0413\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4200 - val_loss: 0.0419\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5468 - val_loss: 0.0414\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F05666700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_338 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 620ms/step - loss: 2.5245 - val_loss: 0.0397\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.5777 - val_loss: 0.0391\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 22.2796 - val_loss: 0.0405\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.8566 - val_loss: 0.0403\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.0460 - val_loss: 0.0418\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.7508 - val_loss: 0.0411\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.8132 - val_loss: 0.0412\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.3894 - val_loss: 0.0405\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.1103 - val_loss: 0.0410\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.2217 - val_loss: 0.0409\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.3976 - val_loss: 0.0395\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.8936 - val_loss: 0.0406\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.3012 - val_loss: 0.0393\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3900 - val_loss: 0.0398\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0064 - val_loss: 0.0390\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5472 - val_loss: 0.0392\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3763 - val_loss: 0.0389\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.3368 - val_loss: 0.0389\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3446 - val_loss: 0.0387\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3253 - val_loss: 0.0388\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3079 - val_loss: 0.0388\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3379 - val_loss: 0.0388\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4638 - val_loss: 0.0389\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6378 - val_loss: 0.0394\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6918 - val_loss: 0.0389\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6485 - val_loss: 0.0402\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5059 - val_loss: 0.0392\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3614 - val_loss: 0.0407\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2615 - val_loss: 0.0393\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2514 - val_loss: 0.0417\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3179 - val_loss: 0.0391\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5197 - val_loss: 0.0447\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8429 - val_loss: 0.0390\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0116 - val_loss: 0.0460\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEE7C0EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_339 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 2.1956 - val_loss: 0.0389\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.1513 - val_loss: 0.0405\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 18.2080 - val_loss: 0.0412\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.9787 - val_loss: 0.0447\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 7.8215 - val_loss: 0.0455\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.1160 - val_loss: 0.0503\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.4172 - val_loss: 0.0512\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0918 - val_loss: 0.0564\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.9795 - val_loss: 0.0552\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7475 - val_loss: 0.0637\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4683 - val_loss: 0.0609\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9145 - val_loss: 0.0672\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6407 - val_loss: 0.0640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4816 - val_loss: 0.0711\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5128 - val_loss: 0.0684\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5034 - val_loss: 0.0748\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE7E07D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_340 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 521ms/step - loss: 2.0023 - val_loss: 0.0442\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.0493 - val_loss: 0.0382\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 22.2459 - val_loss: 0.0419\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.9210 - val_loss: 0.0421\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.3835 - val_loss: 0.0430\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.6008 - val_loss: 0.0444\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.6041 - val_loss: 0.0464\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.3600 - val_loss: 0.0480\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 4.0911 - val_loss: 0.0509\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.5192 - val_loss: 0.0510\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.4185 - val_loss: 0.0523\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2419 - val_loss: 0.0525\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.4309 - val_loss: 0.0539\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7601 - val_loss: 0.0533\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2715 - val_loss: 0.0529\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9038 - val_loss: 0.0543\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6885 - val_loss: 0.0517\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD6750280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_341 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.0452 - val_loss: 0.0370\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.8043 - val_loss: 0.0377\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 18.5534 - val_loss: 0.0411\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.8136 - val_loss: 0.0413\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.9231 - val_loss: 0.0459\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.4830 - val_loss: 0.0467\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.7103 - val_loss: 0.0515\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.6881 - val_loss: 0.0526\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2317 - val_loss: 0.0570\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2055 - val_loss: 0.0570\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.1680 - val_loss: 0.0578\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 2.0628 - val_loss: 0.0630\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.5041 - val_loss: 0.0604\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8973 - val_loss: 0.0668\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4372 - val_loss: 0.0643\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3017 - val_loss: 0.0690\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB1685B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_342 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 256ms/step - loss: 2.2345 - val_loss: 0.0362\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 13.5022 - val_loss: 0.0360\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.7610 - val_loss: 0.0410\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.3750 - val_loss: 0.0404\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.8103 - val_loss: 0.0447\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.5150 - val_loss: 0.0466\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.1585 - val_loss: 0.0495\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.3118 - val_loss: 0.0498\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.2272 - val_loss: 0.0511\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8030 - val_loss: 0.0506\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6642 - val_loss: 0.0541\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2065 - val_loss: 0.0512\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7608 - val_loss: 0.0550\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3465 - val_loss: 0.0527\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1656 - val_loss: 0.0554\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1621 - val_loss: 0.0527\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2212 - val_loss: 0.0564\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1F7E1670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_343 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 234ms/step - loss: 1.6989 - val_loss: 0.0364\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.4254 - val_loss: 0.0367\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 21.6916 - val_loss: 0.0379\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.3468 - val_loss: 0.0384\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8484 - val_loss: 0.0390\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.4553 - val_loss: 0.0375\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.8388 - val_loss: 0.0396\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.7155 - val_loss: 0.0396\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.0431 - val_loss: 0.0406\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.4299 - val_loss: 0.0403\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.5504 - val_loss: 0.0407\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1290 - val_loss: 0.0419\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0252 - val_loss: 0.0395\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.7355 - val_loss: 0.0413\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6754 - val_loss: 0.0394\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2171 - val_loss: 0.0403\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FD6D13D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_344 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 237ms/step - loss: 2.3435 - val_loss: 0.0342\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.8223 - val_loss: 0.0388\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 23.2451 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.8774 - val_loss: 0.0399\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.1520 - val_loss: 0.0383\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.9505 - val_loss: 0.0411\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.7672 - val_loss: 0.0399\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.8380 - val_loss: 0.0416\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8400 - val_loss: 0.0403\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0990 - val_loss: 0.0414\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5070 - val_loss: 0.0404\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9948 - val_loss: 0.0400\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6302 - val_loss: 0.0399\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5456 - val_loss: 0.0382\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6289 - val_loss: 0.0395\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6382 - val_loss: 0.0362\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250322A8700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_345 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.5685 - val_loss: 0.0341\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 20.5471 - val_loss: 0.0350\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 18.2344 - val_loss: 0.0415\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.1628 - val_loss: 0.0431\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 9.2825 - val_loss: 0.0452\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0383 - val_loss: 0.0450\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.8557 - val_loss: 0.0448\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.9649 - val_loss: 0.0446\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.8124 - val_loss: 0.0455\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2073 - val_loss: 0.0440\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1153 - val_loss: 0.0442\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.5807 - val_loss: 0.0431\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0397 - val_loss: 0.0445\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7005 - val_loss: 0.0424\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6350 - val_loss: 0.0454\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.8610 - val_loss: 0.0407\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025032EDF040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_346 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 379ms/step - loss: 1.2894 - val_loss: 0.0338\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 7.6355 - val_loss: 0.0377\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 20.4325 - val_loss: 0.0390\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.1575 - val_loss: 0.0416\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.2947 - val_loss: 0.0475\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.3324 - val_loss: 0.0538\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.0325 - val_loss: 0.0540\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.1344 - val_loss: 0.0589\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.9664 - val_loss: 0.0600\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.4966 - val_loss: 0.0659\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4179 - val_loss: 0.0649\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.1108 - val_loss: 0.0701\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6936 - val_loss: 0.0715\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.0885 - val_loss: 0.0740\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7057 - val_loss: 0.0752\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5077 - val_loss: 0.0794\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F74214280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_347 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.0729 - val_loss: 0.0348\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.8979 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14.1585 - val_loss: 0.0378\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.3041 - val_loss: 0.0381\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.5060 - val_loss: 0.0393\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.6795 - val_loss: 0.0419\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.3993 - val_loss: 0.0429\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.7718 - val_loss: 0.0458\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.9475 - val_loss: 0.0455\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.5442 - val_loss: 0.0478\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 4.2473 - val_loss: 0.0487\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9415 - val_loss: 0.0481\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.5952 - val_loss: 0.0490\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0059 - val_loss: 0.0503\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.3060 - val_loss: 0.0494\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.9916 - val_loss: 0.0492\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6147 - val_loss: 0.0492\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FCC068820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_348 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2.5911 - val_loss: 0.0342\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.1339 - val_loss: 0.0383\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.8086 - val_loss: 0.0375\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 13.9081 - val_loss: 0.0434\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.9194 - val_loss: 0.0450\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.1528 - val_loss: 0.0463\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.2644 - val_loss: 0.0499\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.3508 - val_loss: 0.0497\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.0398 - val_loss: 0.0508\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5522 - val_loss: 0.0533\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.4198 - val_loss: 0.0544\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3169 - val_loss: 0.0564\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.0152 - val_loss: 0.0556\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6733 - val_loss: 0.0573\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2796 - val_loss: 0.0576\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9342 - val_loss: 0.0593\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025020A20550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_349 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 269ms/step - loss: 1.6749 - val_loss: 0.0340\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.7723 - val_loss: 0.0371\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 22.1668 - val_loss: 0.0388\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.9912 - val_loss: 0.0408\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.3920 - val_loss: 0.0441\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4949 - val_loss: 0.0459\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 6.8082 - val_loss: 0.0491\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.5219 - val_loss: 0.0485\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.0098 - val_loss: 0.0536\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.3260 - val_loss: 0.0510\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.8035 - val_loss: 0.0544\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.8642 - val_loss: 0.0523\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.6281 - val_loss: 0.0555\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1726 - val_loss: 0.0520\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6721 - val_loss: 0.0542\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4299 - val_loss: 0.0528\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FFC9F99D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_350 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.2694 - val_loss: 0.0347\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 8.5581 - val_loss: 0.0360\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18.1004 - val_loss: 0.0441\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.7276 - val_loss: 0.0476\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.3227 - val_loss: 0.0508\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.5058 - val_loss: 0.0548\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.5106 - val_loss: 0.0623\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.1933 - val_loss: 0.0647\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.6956 - val_loss: 0.0757\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.6790 - val_loss: 0.0791\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.2383 - val_loss: 0.0833\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2964 - val_loss: 0.0955\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7423 - val_loss: 0.0937\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.2471 - val_loss: 0.1047\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6378 - val_loss: 0.1044\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3095 - val_loss: 0.1136\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002500A542280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_351 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 2.6746 - val_loss: 0.0352\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.8254 - val_loss: 0.0396\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 14.8024 - val_loss: 0.0380\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 9.3242 - val_loss: 0.0408\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.3974 - val_loss: 0.0426\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.9190 - val_loss: 0.0449\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.8964 - val_loss: 0.0462\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.9014 - val_loss: 0.0492\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3.4805 - val_loss: 0.0491\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.6290 - val_loss: 0.0488\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4378 - val_loss: 0.0529\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1331 - val_loss: 0.0481\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.0166 - val_loss: 0.0527\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8207 - val_loss: 0.0484\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7087 - val_loss: 0.0505\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2843 - val_loss: 0.0473\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F10DB04C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_352 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 380ms/step - loss: 1.8722 - val_loss: 0.0350\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 6.2778 - val_loss: 0.0401\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.2208 - val_loss: 0.0421\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.2438 - val_loss: 0.0438\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.0622 - val_loss: 0.0458\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.8698 - val_loss: 0.0511\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.3733 - val_loss: 0.0526\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.1686 - val_loss: 0.0578\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.0418 - val_loss: 0.0583\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.3291 - val_loss: 0.0650\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.0005 - val_loss: 0.0657\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6148 - val_loss: 0.0700\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4081 - val_loss: 0.0690\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.6778 - val_loss: 0.0750\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.5175 - val_loss: 0.0760\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.3127 - val_loss: 0.0778\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F13A03F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_353 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 1.9083 - val_loss: 0.0349\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.2402 - val_loss: 0.0360\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 19.7863 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.5588 - val_loss: 0.0386\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.7779 - val_loss: 0.0387\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.6701 - val_loss: 0.0407\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.6714 - val_loss: 0.0386\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.3975 - val_loss: 0.0401\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.1479 - val_loss: 0.0399\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4966 - val_loss: 0.0397\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8331 - val_loss: 0.0394\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.8764 - val_loss: 0.0393\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4176 - val_loss: 0.0387\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.3557 - val_loss: 0.0387\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3708 - val_loss: 0.0379\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3484 - val_loss: 0.0379\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EEF452040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_354 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 1.4311 - val_loss: 0.0345\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.3442 - val_loss: 0.0365\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 14.8178 - val_loss: 0.0382\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.1068 - val_loss: 0.0389\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.8627 - val_loss: 0.0411\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.1032 - val_loss: 0.0423\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.8169 - val_loss: 0.0452\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.8666 - val_loss: 0.0443\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.1763 - val_loss: 0.0490\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.4161 - val_loss: 0.0460\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5973 - val_loss: 0.0493\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 1.6496 - val_loss: 0.0476\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8734 - val_loss: 0.0474\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3809 - val_loss: 0.0473\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2275 - val_loss: 0.0459\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1883 - val_loss: 0.0468\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025038DA8CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_355 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 1.3861 - val_loss: 0.0345\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 16.5090 - val_loss: 0.0387\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.0665 - val_loss: 0.0414\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 14.7171 - val_loss: 0.0458\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.5968 - val_loss: 0.0496\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.0444 - val_loss: 0.0565\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.3839 - val_loss: 0.0609\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.0762 - val_loss: 0.0654\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.9081 - val_loss: 0.0678\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2181 - val_loss: 0.0744\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.3489 - val_loss: 0.0743\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3217 - val_loss: 0.0818\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1213 - val_loss: 0.0757\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9129 - val_loss: 0.0875\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6062 - val_loss: 0.0818\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4374 - val_loss: 0.0888\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503C715670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_356 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 328ms/step - loss: 1.5403 - val_loss: 0.0343\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.0317 - val_loss: 0.0337\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 17.1684 - val_loss: 0.0383\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 17.3542 - val_loss: 0.0406\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.8569 - val_loss: 0.0475\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.8247 - val_loss: 0.0494\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 5.6704 - val_loss: 0.0557\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 5.5010 - val_loss: 0.0551\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.4648 - val_loss: 0.0578\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.9318 - val_loss: 0.0564\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3018 - val_loss: 0.0573\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7438 - val_loss: 0.0573\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.419 - 0s 51ms/step - loss: 0.4193 - val_loss: 0.0597\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2958 - val_loss: 0.0588\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3807 - val_loss: 0.0622\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6302 - val_loss: 0.0586\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8029 - val_loss: 0.0646\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC1255160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_357 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 409ms/step - loss: 2.3611 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 16.6718 - val_loss: 0.0367\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.6683 - val_loss: 0.0412\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.5023 - val_loss: 0.0419\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 8.7178 - val_loss: 0.0442\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 8.4546 - val_loss: 0.0469\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 8.2257 - val_loss: 0.0488\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 5.7581 - val_loss: 0.0487\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 4.0329 - val_loss: 0.0496\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.3992 - val_loss: 0.0511\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 1.5896 - val_loss: 0.0533\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.1469 - val_loss: 0.0549\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9840 - val_loss: 0.0565\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9848 - val_loss: 0.0542\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8608 - val_loss: 0.0587\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8194 - val_loss: 0.0537\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F9A87D550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_358 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 517ms/step - loss: 2.3241 - val_loss: 0.0321\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 13.9155 - val_loss: 0.0328\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 25.8930 - val_loss: 0.0346\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.1044 - val_loss: 0.0391\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 12.4124 - val_loss: 0.0381\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.6994 - val_loss: 0.0441\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.0169 - val_loss: 0.0439\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.4567 - val_loss: 0.0487\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.5704 - val_loss: 0.0473\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.1376 - val_loss: 0.0533\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2973 - val_loss: 0.0506\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5485 - val_loss: 0.0546\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2140 - val_loss: 0.0537\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1501 - val_loss: 0.0554\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2825 - val_loss: 0.0552\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.7272 - val_loss: 0.0550\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC6296E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_359 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 394ms/step - loss: 1.6604 - val_loss: 0.0335\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.3344 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 18.9540 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.5314 - val_loss: 0.0453\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.1994 - val_loss: 0.0428\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.3573 - val_loss: 0.0541\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.1099 - val_loss: 0.0537\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5283 - val_loss: 0.0655\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.6494 - val_loss: 0.0661\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.8001 - val_loss: 0.0712\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.3030 - val_loss: 0.0804\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7083 - val_loss: 0.0787\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2843 - val_loss: 0.0931\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6285 - val_loss: 0.0915\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3636 - val_loss: 0.1045\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2365 - val_loss: 0.1040\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FF73F7160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_360 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 403ms/step - loss: 1.5833 - val_loss: 0.0304\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.2855 - val_loss: 0.0365\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 13.1815 - val_loss: 0.0343\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.5692 - val_loss: 0.0387\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.2029 - val_loss: 0.0436\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.4289 - val_loss: 0.0444\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.0709 - val_loss: 0.0490\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8606 - val_loss: 0.0488\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.7694 - val_loss: 0.0517\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9406 - val_loss: 0.0519\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.5949 - val_loss: 0.0522\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9594 - val_loss: 0.0546\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5353 - val_loss: 0.0529\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5413 - val_loss: 0.0565\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5254 - val_loss: 0.0544\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4286 - val_loss: 0.0566\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502B775310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_361 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 504ms/step - loss: 1.5132 - val_loss: 0.0303\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.5912 - val_loss: 0.0345\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 18.3643 - val_loss: 0.0354\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.5740 - val_loss: 0.0379\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.9476 - val_loss: 0.0389\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.8841 - val_loss: 0.0427\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.2212 - val_loss: 0.0408\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.3290 - val_loss: 0.0444\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.5168 - val_loss: 0.0454\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.0239 - val_loss: 0.0485\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6760 - val_loss: 0.0488\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1009 - val_loss: 0.0511\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9282 - val_loss: 0.0512\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9276 - val_loss: 0.0536\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8613 - val_loss: 0.0515\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6532 - val_loss: 0.0546\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336B20D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_362 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 2.1496 - val_loss: 0.0311\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.4681 - val_loss: 0.0354\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 21.3025 - val_loss: 0.0409\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 10.7993 - val_loss: 0.0443\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.4734 - val_loss: 0.0495\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.3535 - val_loss: 0.0471\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.9048 - val_loss: 0.0537\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.9501 - val_loss: 0.0509\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.6538 - val_loss: 0.0557\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.4671 - val_loss: 0.0562\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4638 - val_loss: 0.0564\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8900 - val_loss: 0.0606\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5597 - val_loss: 0.0565\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4233 - val_loss: 0.0651\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4836 - val_loss: 0.0546\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6033 - val_loss: 0.0686\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025010A96820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_363 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 1.7590 - val_loss: 0.0298\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.1329 - val_loss: 0.0330\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 20.9494 - val_loss: 0.0374\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.6749 - val_loss: 0.0401\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.6976 - val_loss: 0.0427\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.9149 - val_loss: 0.0452\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.9277 - val_loss: 0.0489\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.3436 - val_loss: 0.0497\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.3954 - val_loss: 0.0546\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.8430 - val_loss: 0.0551\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.6812 - val_loss: 0.0558\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4812 - val_loss: 0.0579\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1504 - val_loss: 0.0549\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 1.3756 - val_loss: 0.0567\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9059 - val_loss: 0.0550\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6134 - val_loss: 0.0555\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250016B2430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_364 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 366ms/step - loss: 2.2863 - val_loss: 0.0354\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.1223 - val_loss: 0.0326\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 17.1698 - val_loss: 0.0361\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.1371 - val_loss: 0.0352\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.1242 - val_loss: 0.0408\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.2519 - val_loss: 0.0399\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.4739 - val_loss: 0.0435\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.0908 - val_loss: 0.0434\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.2992 - val_loss: 0.0466\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7742 - val_loss: 0.0450\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.3019 - val_loss: 0.0468\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0176 - val_loss: 0.0425\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3439 - val_loss: 0.0449\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8103 - val_loss: 0.0411\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4662 - val_loss: 0.0426\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2517 - val_loss: 0.0398\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.1256 - val_loss: 0.0409\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503E1E79D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_365 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 2.1051 - val_loss: 0.0308\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 16.3807 - val_loss: 0.0358\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 17.1062 - val_loss: 0.0370\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 13.6028 - val_loss: 0.0425\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 9.1632 - val_loss: 0.0433\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.7694 - val_loss: 0.0471\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 6.8169 - val_loss: 0.0497\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 4.2983 - val_loss: 0.0526\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.5078 - val_loss: 0.0554\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.3699 - val_loss: 0.0561\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4449 - val_loss: 0.0578\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9018 - val_loss: 0.0591\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7579 - val_loss: 0.0573\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 0.8045 - val_loss: 0.0597\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 124ms/step - loss: 0.8137 - val_loss: 0.0556\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6721 - val_loss: 0.0613\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA34B5430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_366 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.4536 - val_loss: 0.0318\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0723 - val_loss: 0.0340\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.4653 - val_loss: 0.0392\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 19.8825 - val_loss: 0.0378\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 8.5036 - val_loss: 0.0406\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.5525 - val_loss: 0.0420\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 10.7074 - val_loss: 0.0455\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2678 - val_loss: 0.0486\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.6917 - val_loss: 0.0481\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1090 - val_loss: 0.0533\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5211 - val_loss: 0.0512\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6516 - val_loss: 0.0562\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3347 - val_loss: 0.0547\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2373 - val_loss: 0.0575\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2797 - val_loss: 0.0557\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4273 - val_loss: 0.0586\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025017A46280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_367 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 298ms/step - loss: 3.5079 - val_loss: 0.0308\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.6234 - val_loss: 0.0356\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 15.1342 - val_loss: 0.0347\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.2781 - val_loss: 0.0362\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.6060 - val_loss: 0.0364\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 11.1196 - val_loss: 0.0373\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.9604 - val_loss: 0.0409\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.4139 - val_loss: 0.0401\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.9380 - val_loss: 0.0428\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0277 - val_loss: 0.0407\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.2768 - val_loss: 0.0427\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6631 - val_loss: 0.0380\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4273 - val_loss: 0.0399\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2197 - val_loss: 0.0369\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9884 - val_loss: 0.0366\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8281 - val_loss: 0.0360\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F74214280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_368 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.0929 - val_loss: 0.0292\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 19.2987 - val_loss: 0.0355\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 20.0944 - val_loss: 0.0347\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 15.1803 - val_loss: 0.0358\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.7871 - val_loss: 0.0364\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 8.5188 - val_loss: 0.0357\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.6776 - val_loss: 0.0364\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.8365 - val_loss: 0.0348\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2.9101 - val_loss: 0.0338\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7678 - val_loss: 0.0327\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1802 - val_loss: 0.0321\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8094 - val_loss: 0.0308\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4807 - val_loss: 0.0302\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3833 - val_loss: 0.0292\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4133 - val_loss: 0.0290\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4705 - val_loss: 0.0284\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4352 - val_loss: 0.0283\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3346 - val_loss: 0.0284\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3039 - val_loss: 0.0284\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3837 - val_loss: 0.0292\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5936 - val_loss: 0.0292\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.8294 - val_loss: 0.0306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8245 - val_loss: 0.0320\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7686 - val_loss: 0.0314\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6722 - val_loss: 0.0367\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5343 - val_loss: 0.0331\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4600 - val_loss: 0.0405\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3726 - val_loss: 0.0357\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3977 - val_loss: 0.0426\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.5813 - val_loss: 0.0425\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.8763 - val_loss: 0.0415\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1036 - val_loss: 0.0475\n",
      "Epoch 00032: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025035203670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_369 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1.4280 - val_loss: 0.0329\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.1866 - val_loss: 0.0331\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 17.7528 - val_loss: 0.0354\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.3230 - val_loss: 0.0393\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.8743 - val_loss: 0.0437\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.5721 - val_loss: 0.0445\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0210 - val_loss: 0.0488\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.5258 - val_loss: 0.0511\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.3951 - val_loss: 0.0538\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3.6205 - val_loss: 0.0538\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4622 - val_loss: 0.0580\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.1399 - val_loss: 0.0581\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4158 - val_loss: 0.0618\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1963 - val_loss: 0.0620\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1492 - val_loss: 0.0645\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2730 - val_loss: 0.0658\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002504C38ED30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_370 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 337ms/step - loss: 2.5373 - val_loss: 0.0296\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 13.5826 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 23.7816 - val_loss: 0.0358\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.2158 - val_loss: 0.0377\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 11.0404 - val_loss: 0.0437\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.7123 - val_loss: 0.0467\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2355 - val_loss: 0.0508\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.9420 - val_loss: 0.0529\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.2486 - val_loss: 0.0540\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.2882 - val_loss: 0.0580\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4161 - val_loss: 0.0588\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2071 - val_loss: 0.0617\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8873 - val_loss: 0.0621\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.6921 - val_loss: 0.0648\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6995 - val_loss: 0.0642\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6934 - val_loss: 0.0685\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336B2B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_371 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 248ms/step - loss: 1.5932 - val_loss: 0.0335\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.0776 - val_loss: 0.0319\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 16.2730 - val_loss: 0.0374\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 11.7937 - val_loss: 0.0382\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.0201 - val_loss: 0.0418\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.7173 - val_loss: 0.0412\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 8.2173 - val_loss: 0.0477\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.7096 - val_loss: 0.0466\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.4406 - val_loss: 0.0485\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.9433 - val_loss: 0.0472\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.2091 - val_loss: 0.0524\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.8716 - val_loss: 0.0479\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8850 - val_loss: 0.0531\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.5848 - val_loss: 0.0490\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3292 - val_loss: 0.0532\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 133ms/step - loss: 0.2427 - val_loss: 0.0495\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3231 - val_loss: 0.0522\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE09FF0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_372 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.5546 - val_loss: 0.0282\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 12.2336 - val_loss: 0.0374\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 19.4047 - val_loss: 0.0381\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.2535 - val_loss: 0.0399\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.7334 - val_loss: 0.0466\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.7091 - val_loss: 0.0455\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.6704 - val_loss: 0.0517\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.5163 - val_loss: 0.0505\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.3913 - val_loss: 0.0561\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.6213 - val_loss: 0.0571\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.0429 - val_loss: 0.0611\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0550 - val_loss: 0.0619\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3784 - val_loss: 0.0661\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2975 - val_loss: 0.0630\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5827 - val_loss: 0.0671\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.0702 - val_loss: 0.0657\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAB9D28B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_373 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 2.2702 - val_loss: 0.0325\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.2872 - val_loss: 0.0347\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 19.2687 - val_loss: 0.0407\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.3222 - val_loss: 0.0441\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.5889 - val_loss: 0.0450\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.2896 - val_loss: 0.0463\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 6.0271 - val_loss: 0.0509\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.0179 - val_loss: 0.0520\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.8347 - val_loss: 0.0564\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.3043 - val_loss: 0.0584\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7160 - val_loss: 0.0624\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3723 - val_loss: 0.0657\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1924 - val_loss: 0.0708\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0336 - val_loss: 0.0734\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6657 - val_loss: 0.0777\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4517 - val_loss: 0.0768\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025018CDD790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_374 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 302ms/step - loss: 2.0386 - val_loss: 0.0304\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.2271 - val_loss: 0.0339\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.0137 - val_loss: 0.0367\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 13.6421 - val_loss: 0.0422\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 11.2316 - val_loss: 0.0422\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.2493 - val_loss: 0.0497\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.0911 - val_loss: 0.0474\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.5216 - val_loss: 0.0549\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.5251 - val_loss: 0.0542\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 4.3709 - val_loss: 0.0602\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6577 - val_loss: 0.0580\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9868 - val_loss: 0.0635\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6172 - val_loss: 0.0616\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8236 - val_loss: 0.0647\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.4225 - val_loss: 0.0643\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2581 - val_loss: 0.0647\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025018F280D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_375 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 2.8077 - val_loss: 0.0286\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 12.0523 - val_loss: 0.0321\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 19.6778 - val_loss: 0.0393\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.2530 - val_loss: 0.0404\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 11.7463 - val_loss: 0.0467\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0988 - val_loss: 0.0452\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.6018 - val_loss: 0.0490\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.5089 - val_loss: 0.0445\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.5943 - val_loss: 0.0504\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9015 - val_loss: 0.0474\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.8558 - val_loss: 0.0516\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5037 - val_loss: 0.0521\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.0218 - val_loss: 0.0522\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6487 - val_loss: 0.0543\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.4461 - val_loss: 0.0546\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3493 - val_loss: 0.0565\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025041D75DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_376 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 289ms/step - loss: 2.1512 - val_loss: 0.0319\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 11.5474 - val_loss: 0.0338\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.8867 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.1164 - val_loss: 0.0451\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.5168 - val_loss: 0.0513\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.2537 - val_loss: 0.0533\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.6197 - val_loss: 0.0576\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.8374 - val_loss: 0.0680\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.2324 - val_loss: 0.0706\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.4516 - val_loss: 0.0799\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3280 - val_loss: 0.0834\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1.8039 - val_loss: 0.0907\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2750 - val_loss: 0.0938\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6932 - val_loss: 0.1015\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4130 - val_loss: 0.1034\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3257 - val_loss: 0.1100\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBF912790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_377 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 2.0694 - val_loss: 0.0303\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.8085 - val_loss: 0.0306\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 23.0103 - val_loss: 0.0327\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.6505 - val_loss: 0.0349\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.3756 - val_loss: 0.0380\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.1058 - val_loss: 0.0392\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.9110 - val_loss: 0.0405\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3846 - val_loss: 0.0436\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.6376 - val_loss: 0.0432\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3388 - val_loss: 0.0458\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0528 - val_loss: 0.0452\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5773 - val_loss: 0.0474\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3208 - val_loss: 0.0462\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.2765 - val_loss: 0.0478\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 0.3673 - val_loss: 0.0456\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5822 - val_loss: 0.0487\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC0C150D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_378 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 398ms/step - loss: 2.0563 - val_loss: 0.0278\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.0756 - val_loss: 0.0327\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17.4488 - val_loss: 0.0352\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.9835 - val_loss: 0.0382\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.0872 - val_loss: 0.0416\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.6317 - val_loss: 0.0483\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.3543 - val_loss: 0.0461\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.1596 - val_loss: 0.0490\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.4834 - val_loss: 0.0500\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9166 - val_loss: 0.0512\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.9974 - val_loss: 0.0505\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2924 - val_loss: 0.0535\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1271 - val_loss: 0.0505\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8302 - val_loss: 0.0559\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6076 - val_loss: 0.0515\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3974 - val_loss: 0.0563\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002504FD34AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_379 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 280ms/step - loss: 2.4072 - val_loss: 0.0296\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.7893 - val_loss: 0.0295\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 18.4661 - val_loss: 0.0330\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 12.7632 - val_loss: 0.0382\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.0587 - val_loss: 0.0380\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.6449 - val_loss: 0.0418\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5.4635 - val_loss: 0.0401\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.2678 - val_loss: 0.0445\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 2.5902 - val_loss: 0.0419\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0774 - val_loss: 0.0458\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4689 - val_loss: 0.0440\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0219 - val_loss: 0.0476\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9509 - val_loss: 0.0458\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.1162 - val_loss: 0.0504\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0566 - val_loss: 0.0461\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9410 - val_loss: 0.0518\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7171 - val_loss: 0.0478\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025047185430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_380 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 432ms/step - loss: 2.0512 - val_loss: 0.0277\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8696 - val_loss: 0.0283\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 20.5255 - val_loss: 0.0300\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.1346 - val_loss: 0.0351\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.9125 - val_loss: 0.0344\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.2476 - val_loss: 0.0422\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.9371 - val_loss: 0.0371\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.5540 - val_loss: 0.0476\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.0386 - val_loss: 0.0443\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.8834 - val_loss: 0.0485\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.9267 - val_loss: 0.0476\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8443 - val_loss: 0.0503\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7976 - val_loss: 0.0497\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9658 - val_loss: 0.0523\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4113 - val_loss: 0.0514\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1911 - val_loss: 0.0523\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002505A854CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_381 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.9660 - val_loss: 0.0298\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.9779 - val_loss: 0.0280\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 18.8969 - val_loss: 0.0294\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.6945 - val_loss: 0.0325\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.3906 - val_loss: 0.0341\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.6344 - val_loss: 0.0380\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.4160 - val_loss: 0.0426\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.1550 - val_loss: 0.0453\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.5054 - val_loss: 0.0497\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.5888 - val_loss: 0.0528\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.9109 - val_loss: 0.0552\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1116 - val_loss: 0.0608\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4632 - val_loss: 0.0628\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1898 - val_loss: 0.0669\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1169 - val_loss: 0.0706\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.1333 - val_loss: 0.0725\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.1962 - val_loss: 0.0789\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA70C28B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_382 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 2.0823 - val_loss: 0.0242\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.8523 - val_loss: 0.0243\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 21.2736 - val_loss: 0.0284\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.5560 - val_loss: 0.0269\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 8.3271 - val_loss: 0.0300\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.4109 - val_loss: 0.0277\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.6350 - val_loss: 0.0308\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.5879 - val_loss: 0.0280\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.6582 - val_loss: 0.0305\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1685 - val_loss: 0.0281\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.7302 - val_loss: 0.0290\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4589 - val_loss: 0.0279\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9874 - val_loss: 0.0277\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8265 - val_loss: 0.0276\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8696 - val_loss: 0.0270\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.0360 - val_loss: 0.0273\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAB9D2E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_383 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 255ms/step - loss: 2.2423 - val_loss: 0.0228\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.5223 - val_loss: 0.0251\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.4966 - val_loss: 0.0265\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.0263 - val_loss: 0.0279\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.2185 - val_loss: 0.0306\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.9113 - val_loss: 0.0323\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.7683 - val_loss: 0.0345\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.2488 - val_loss: 0.0343\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.3414 - val_loss: 0.0412\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3762 - val_loss: 0.0384\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.2345 - val_loss: 0.0449\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3351 - val_loss: 0.0449\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4391 - val_loss: 0.0490\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4031 - val_loss: 0.0480\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6208 - val_loss: 0.0530\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0487 - val_loss: 0.0527\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002501328D160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_384 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 2.1727 - val_loss: 0.0225\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.5526 - val_loss: 0.0246\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 20.2971 - val_loss: 0.0246\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 12.5806 - val_loss: 0.0267\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.1316 - val_loss: 0.0295\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 7.0158 - val_loss: 0.0316\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.6288 - val_loss: 0.0349\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.2291 - val_loss: 0.0360\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.8924 - val_loss: 0.0376\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7908 - val_loss: 0.0418\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.1088 - val_loss: 0.0402\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1419 - val_loss: 0.0440\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6156 - val_loss: 0.0451\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step - loss: 0.3905 - val_loss: 0.0459\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3420 - val_loss: 0.0491\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4555 - val_loss: 0.0454\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB1624EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_385 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 243ms/step - loss: 1.2477 - val_loss: 0.0220\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.6533 - val_loss: 0.0219\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.7244 - val_loss: 0.0238\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.8434 - val_loss: 0.0256\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.9746 - val_loss: 0.0268\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.9733 - val_loss: 0.0312\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 6.4466 - val_loss: 0.0278\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.2644 - val_loss: 0.0325\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.3620 - val_loss: 0.0330\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8316 - val_loss: 0.0368\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7779 - val_loss: 0.0333\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.4306 - val_loss: 0.0379\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0662 - val_loss: 0.0384\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.3978 - val_loss: 0.0400\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8576 - val_loss: 0.0413\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8012 - val_loss: 0.0423\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6888 - val_loss: 0.0436\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAEC19040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_386 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 564ms/step - loss: 2.3014 - val_loss: 0.0215\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 11.4229 - val_loss: 0.0212\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 21.0705 - val_loss: 0.0218\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.1106 - val_loss: 0.0223\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.3335 - val_loss: 0.0240\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.4050 - val_loss: 0.0229\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.8010 - val_loss: 0.0243\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.8099 - val_loss: 0.0232\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.9812 - val_loss: 0.0242\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3661 - val_loss: 0.0239\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.4735 - val_loss: 0.0241\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.5290 - val_loss: 0.0237\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.9099 - val_loss: 0.0241\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5916 - val_loss: 0.0235\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3540 - val_loss: 0.0239\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4278 - val_loss: 0.0240\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7515 - val_loss: 0.0230\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F337268B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_387 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 268ms/step - loss: 1.7056 - val_loss: 0.0221\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.4328 - val_loss: 0.0212\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 17.6009 - val_loss: 0.0218\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 12.3286 - val_loss: 0.0237\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.7767 - val_loss: 0.0243\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.2121 - val_loss: 0.0251\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8065 - val_loss: 0.0259\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.7244 - val_loss: 0.0267\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.2305 - val_loss: 0.0271\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.3350 - val_loss: 0.0270\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4648 - val_loss: 0.0287\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.5238 - val_loss: 0.0294\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6345 - val_loss: 0.0314\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.1887 - val_loss: 0.0294\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5210 - val_loss: 0.0314\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1118 - val_loss: 0.0302\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7292 - val_loss: 0.0322\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502B731310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_388 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 360ms/step - loss: 1.6071 - val_loss: 0.0215\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 12.4157 - val_loss: 0.0212\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.7374 - val_loss: 0.0216\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.9816 - val_loss: 0.0224\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.5486 - val_loss: 0.0237\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.7146 - val_loss: 0.0250\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 5.5202 - val_loss: 0.0254\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.6393 - val_loss: 0.0251\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.6935 - val_loss: 0.0261\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.4777 - val_loss: 0.0256\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7591 - val_loss: 0.0265\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7810 - val_loss: 0.0256\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4594 - val_loss: 0.0267\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.5097 - val_loss: 0.0241\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6822 - val_loss: 0.0263\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 0.6870 - val_loss: 0.0238\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6292 - val_loss: 0.0245\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025003698EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_389 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 443ms/step - loss: 1.4324 - val_loss: 0.0219\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.7269 - val_loss: 0.0222\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 19.9077 - val_loss: 0.0212\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.1608 - val_loss: 0.0223\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.5362 - val_loss: 0.0230\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.5845 - val_loss: 0.0236\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8698 - val_loss: 0.0234\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.0014 - val_loss: 0.0254\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.5233 - val_loss: 0.0241\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8784 - val_loss: 0.0272\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.4397 - val_loss: 0.0245\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.1669 - val_loss: 0.0278\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6791 - val_loss: 0.0252\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1545 - val_loss: 0.0280\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.0169 - val_loss: 0.0258\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4307 - val_loss: 0.0298\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0214 - val_loss: 0.0257\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7649 - val_loss: 0.0298\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250372D8040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_390 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 307ms/step - loss: 1.7890 - val_loss: 0.0225\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.2651 - val_loss: 0.0207\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 19.3973 - val_loss: 0.0211\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 13.7305 - val_loss: 0.0212\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 10.2958 - val_loss: 0.0238\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.4935 - val_loss: 0.0233\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2292 - val_loss: 0.0250\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.7124 - val_loss: 0.0268\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.8042 - val_loss: 0.0277\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2225 - val_loss: 0.0293\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.5310 - val_loss: 0.0315\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8058 - val_loss: 0.0351\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.2077 - val_loss: 0.0338\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.7815 - val_loss: 0.0395\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1120 - val_loss: 0.0340\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8724 - val_loss: 0.0415\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5333 - val_loss: 0.0359\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002505FC23430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_391 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 2.1787 - val_loss: 0.0232\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 14.4579 - val_loss: 0.0194\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 17.2030 - val_loss: 0.0200\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 11.8910 - val_loss: 0.0199\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.8091 - val_loss: 0.0209\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.2985 - val_loss: 0.0214\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.4028 - val_loss: 0.0214\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.3121 - val_loss: 0.0233\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.5278 - val_loss: 0.0218\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.7704 - val_loss: 0.0252\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 3.5473 - val_loss: 0.0231\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.6268 - val_loss: 0.0251\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3321 - val_loss: 0.0242\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9316 - val_loss: 0.0244\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4575 - val_loss: 0.0247\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2944 - val_loss: 0.0237\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2005 - val_loss: 0.0244\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEAAB49D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_392 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 267ms/step - loss: 2.5817 - val_loss: 0.0185\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17.8036 - val_loss: 0.0182\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 25.7208 - val_loss: 0.0180\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.8189 - val_loss: 0.0187\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.3221 - val_loss: 0.0194\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.5815 - val_loss: 0.0204\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.0932 - val_loss: 0.0209\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.2234 - val_loss: 0.0212\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.3400 - val_loss: 0.0219\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.7758 - val_loss: 0.0222\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8764 - val_loss: 0.0219\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3538 - val_loss: 0.0233\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8756 - val_loss: 0.0222\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4771 - val_loss: 0.0236\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3494 - val_loss: 0.0216\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3787 - val_loss: 0.0242\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4907 - val_loss: 0.0208\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5298 - val_loss: 0.0244\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAFB8D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_393 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.1698 - val_loss: 0.0192\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.8612 - val_loss: 0.0174\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 19.6044 - val_loss: 0.0168\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.3859 - val_loss: 0.0166\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 11.0190 - val_loss: 0.0166\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.0297 - val_loss: 0.0167\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.6356 - val_loss: 0.0167\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 5.4928 - val_loss: 0.0167\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.8956 - val_loss: 0.0166\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.7147 - val_loss: 0.0168\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0270 - val_loss: 0.0168\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.2423 - val_loss: 0.0167\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8889 - val_loss: 0.0171\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5409 - val_loss: 0.0167\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 1.0383 - val_loss: 0.0172\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6492 - val_loss: 0.0167\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4187 - val_loss: 0.0174\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3017 - val_loss: 0.0170\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2527 - val_loss: 0.0177\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FDEE13AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_394 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 434ms/step - loss: 2.3379 - val_loss: 0.0156\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.3154 - val_loss: 0.0166\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 24.5141 - val_loss: 0.0152\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.3901 - val_loss: 0.0153\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.6678 - val_loss: 0.0159\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.9359 - val_loss: 0.0164\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.3203 - val_loss: 0.0168\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.4401 - val_loss: 0.0169\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.5259 - val_loss: 0.0179\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0018 - val_loss: 0.0179\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9446 - val_loss: 0.0182\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2375 - val_loss: 0.0185\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7940 - val_loss: 0.0187\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4821 - val_loss: 0.0202\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5994 - val_loss: 0.0188\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7394 - val_loss: 0.0219\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6772 - val_loss: 0.0194\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5684 - val_loss: 0.0227\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE93CD280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_395 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 321ms/step - loss: 2.4204 - val_loss: 0.0202\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 16.7061 - val_loss: 0.0150\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.5115 - val_loss: 0.0146\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.0281 - val_loss: 0.0146\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.0247 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.0731 - val_loss: 0.0154\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.1416 - val_loss: 0.0158\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.1282 - val_loss: 0.0162\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.6227 - val_loss: 0.0156\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.3188 - val_loss: 0.0170\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8227 - val_loss: 0.0158\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0798 - val_loss: 0.0174\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8640 - val_loss: 0.0157\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.6133 - val_loss: 0.0170\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5527 - val_loss: 0.0161\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4172 - val_loss: 0.0171\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3763 - val_loss: 0.0164\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3764 - val_loss: 0.0169\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBF856D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_396 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 1.7809 - val_loss: 0.0140\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 14.4676 - val_loss: 0.0135\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 17.5136 - val_loss: 0.0136\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.8659 - val_loss: 0.0137\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.9276 - val_loss: 0.0137\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.5479 - val_loss: 0.0156\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.7841 - val_loss: 0.0150\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.8275 - val_loss: 0.0169\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.5381 - val_loss: 0.0164\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.4227 - val_loss: 0.0180\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9021 - val_loss: 0.0184\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.4279 - val_loss: 0.0180\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8030 - val_loss: 0.0206\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6095 - val_loss: 0.0192\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5079 - val_loss: 0.0224\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3730 - val_loss: 0.0212\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2654 - val_loss: 0.0237\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF8BC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_397 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 277ms/step - loss: 2.4661 - val_loss: 0.0153\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.7183 - val_loss: 0.0121\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.4900 - val_loss: 0.0125\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 16.3386 - val_loss: 0.0119\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.5480 - val_loss: 0.0118\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.5162 - val_loss: 0.0120\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.3933 - val_loss: 0.0121\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.8379 - val_loss: 0.0129\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.0676 - val_loss: 0.0124\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.4815 - val_loss: 0.0128\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9176 - val_loss: 0.0124\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1108 - val_loss: 0.0125\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9154 - val_loss: 0.0122\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6840 - val_loss: 0.0122\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5296 - val_loss: 0.0122\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4820 - val_loss: 0.0121\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5233 - val_loss: 0.0120\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6158 - val_loss: 0.0124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7508 - val_loss: 0.0119\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7628 - val_loss: 0.0129\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBC5BB550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_398 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 361ms/step - loss: 1.9525 - val_loss: 0.0212\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.1448 - val_loss: 0.0103\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 21.0206 - val_loss: 0.0108\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.1597 - val_loss: 0.0103\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.9513 - val_loss: 0.0103\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.1766 - val_loss: 0.0103\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.0771 - val_loss: 0.0103\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.3568 - val_loss: 0.0105\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.3914 - val_loss: 0.0105\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.0754 - val_loss: 0.0108\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7669 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2663 - val_loss: 0.0113\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9083 - val_loss: 0.0116\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.8691 - val_loss: 0.0125\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8213 - val_loss: 0.0123\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6175 - val_loss: 0.0138\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4287 - val_loss: 0.0135\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3885 - val_loss: 0.0151\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3173 - val_loss: 0.0150\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.3350 - val_loss: 0.0162\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002501747D5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_399 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 1.6771 - val_loss: 0.0188\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.8668 - val_loss: 0.0099\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 22.5019 - val_loss: 0.0117\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.5433 - val_loss: 0.0109\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10.0417 - val_loss: 0.0105\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.9821 - val_loss: 0.0111\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2942 - val_loss: 0.0110\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.6833 - val_loss: 0.0111\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.6334 - val_loss: 0.0111\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.2022 - val_loss: 0.0118\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4133 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4421 - val_loss: 0.0121\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4205 - val_loss: 0.0118\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.5393 - val_loss: 0.0128\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1366 - val_loss: 0.0138\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7577 - val_loss: 0.0129\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.649 - 0s 18ms/step - loss: 0.6490 - val_loss: 0.0149\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503E1A6670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_400 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 281ms/step - loss: 3.7057 - val_loss: 0.0138\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.4783 - val_loss: 0.0104\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 17.4439 - val_loss: 0.0085\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.7096 - val_loss: 0.0072\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.9379 - val_loss: 0.0072\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.1250 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.3106 - val_loss: 0.0073\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.6951 - val_loss: 0.0069\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.2726 - val_loss: 0.0073\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9309 - val_loss: 0.0069\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.3372 - val_loss: 0.0072\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6189 - val_loss: 0.0069\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0965 - val_loss: 0.0071\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6245 - val_loss: 0.0072\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.3755 - val_loss: 0.0071\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2734 - val_loss: 0.0075\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2356 - val_loss: 0.0071\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2253 - val_loss: 0.0078\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2720 - val_loss: 0.0074\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3666 - val_loss: 0.0076\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4547 - val_loss: 0.0079\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5544 - val_loss: 0.0072\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6480 - val_loss: 0.0087\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6958 - val_loss: 0.0069\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6590 - val_loss: 0.0100\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002505C41B1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_401 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 399ms/step - loss: 2.4905 - val_loss: 0.0096\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.4587 - val_loss: 0.0066\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 18.1709 - val_loss: 0.0071\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 15.6293 - val_loss: 0.0065\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.7882 - val_loss: 0.0065\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.8769 - val_loss: 0.0066\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.7300 - val_loss: 0.0068\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.4264 - val_loss: 0.0069\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.2066 - val_loss: 0.0073\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1593 - val_loss: 0.0077\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4427 - val_loss: 0.0078\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.1953 - val_loss: 0.0080\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2650 - val_loss: 0.0082\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0146 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6484 - val_loss: 0.0088\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3695 - val_loss: 0.0100\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.2461 - val_loss: 0.0098\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2255 - val_loss: 0.0110\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3142 - val_loss: 0.0111\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FB101C0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_402 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 418ms/step - loss: 1.6613 - val_loss: 0.0124\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.5826 - val_loss: 0.0099\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 17.8321 - val_loss: 0.0094\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.9324 - val_loss: 0.0077\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.7755 - val_loss: 0.0078\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.8212 - val_loss: 0.0070\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.0425 - val_loss: 0.0072\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.6477 - val_loss: 0.0070\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.5723 - val_loss: 0.0069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6785 - val_loss: 0.0072\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2364 - val_loss: 0.0070\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5318 - val_loss: 0.0072\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6930 - val_loss: 0.0070\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2139 - val_loss: 0.0073\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9043 - val_loss: 0.0071\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8443 - val_loss: 0.0076\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5991 - val_loss: 0.0072\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.4691 - val_loss: 0.0078\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3717 - val_loss: 0.0076\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3477 - val_loss: 0.0081\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2754 - val_loss: 0.0079\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2322 - val_loss: 0.0082\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2821 - val_loss: 0.0087\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4978 - val_loss: 0.0077\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFCF1E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_403 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 390ms/step - loss: 1.6147 - val_loss: 0.0109\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.6093 - val_loss: 0.0104\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 24.0202 - val_loss: 0.0124\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.9644 - val_loss: 0.0102\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 10.0140 - val_loss: 0.0108\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.9814 - val_loss: 0.0101\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.4629 - val_loss: 0.0107\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.2537 - val_loss: 0.0101\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.7582 - val_loss: 0.0108\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.5095 - val_loss: 0.0110\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5183 - val_loss: 0.0125\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6815 - val_loss: 0.0134\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3152 - val_loss: 0.0143\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2575 - val_loss: 0.0159\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3592 - val_loss: 0.0161\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5314 - val_loss: 0.0197\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6898 - val_loss: 0.0178\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7037 - val_loss: 0.0235\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5708 - val_loss: 0.0212\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4505 - val_loss: 0.0257\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3735 - val_loss: 0.0259\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3529 - val_loss: 0.0260\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3939 - val_loss: 0.0329\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE93CD1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_404 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 253ms/step - loss: 2.0974 - val_loss: 0.0205\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 12.5064 - val_loss: 0.0086\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 19.5278 - val_loss: 0.0094\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 10.6441 - val_loss: 0.0082\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.3003 - val_loss: 0.0083\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.5413 - val_loss: 0.0083\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.8812 - val_loss: 0.0087\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.2591 - val_loss: 0.0084\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.9434 - val_loss: 0.0085\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.4856 - val_loss: 0.0084\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6595 - val_loss: 0.0086\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7901 - val_loss: 0.0084\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3110 - val_loss: 0.0085\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1715 - val_loss: 0.0083\n",
      "Epoch 15/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1629 - val_loss: 0.0084\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2411 - val_loss: 0.0084\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4104 - val_loss: 0.0085\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6906 - val_loss: 0.0085\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9753 - val_loss: 0.0084\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FBF8C2AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_405 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.9826 - val_loss: 0.0180\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.0758 - val_loss: 0.0114\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 24.4645 - val_loss: 0.0114\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.5023 - val_loss: 0.0103\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.4422 - val_loss: 0.0102\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.0659 - val_loss: 0.0101\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.4282 - val_loss: 0.0100\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.5321 - val_loss: 0.0101\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.5457 - val_loss: 0.0100\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.3477 - val_loss: 0.0102\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.8000 - val_loss: 0.0101\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7166 - val_loss: 0.0101\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9055 - val_loss: 0.0100\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5123 - val_loss: 0.0101\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8855 - val_loss: 0.0100\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4112 - val_loss: 0.0101\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2321 - val_loss: 0.0101\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1938 - val_loss: 0.0102\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2948 - val_loss: 0.0101\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5530 - val_loss: 0.0107\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8790 - val_loss: 0.0102\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9987 - val_loss: 0.0121\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F51C9D310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_406 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 389ms/step - loss: 2.2453 - val_loss: 0.0147\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.8868 - val_loss: 0.0140\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.6051 - val_loss: 0.0139\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.1235 - val_loss: 0.0141\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.8409 - val_loss: 0.0140\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.8587 - val_loss: 0.0144\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.0759 - val_loss: 0.0142\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 5.3252 - val_loss: 0.0147\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.0229 - val_loss: 0.0151\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.2677 - val_loss: 0.0152\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.5619 - val_loss: 0.0156\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.8166 - val_loss: 0.0152\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5375 - val_loss: 0.0158\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.5274 - val_loss: 0.0156\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4039 - val_loss: 0.0162\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8486 - val_loss: 0.0158\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6130 - val_loss: 0.0173\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5143 - val_loss: 0.0158\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF8B5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_407 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 368ms/step - loss: 2.5005 - val_loss: 0.0190\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 12.2757 - val_loss: 0.0200\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.1895 - val_loss: 0.0185\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 14.4230 - val_loss: 0.0188\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.4500 - val_loss: 0.0190\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.1989 - val_loss: 0.0196\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.2914 - val_loss: 0.0195\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.9211 - val_loss: 0.0201\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.8952 - val_loss: 0.0199\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5619 - val_loss: 0.0213\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4857 - val_loss: 0.0197\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4029 - val_loss: 0.0216\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9177 - val_loss: 0.0202\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.7990 - val_loss: 0.0218\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6417 - val_loss: 0.0205\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5072 - val_loss: 0.0217\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3103 - val_loss: 0.0208\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1645 - val_loss: 0.0217\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024EA26C5B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_408 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 309ms/step - loss: 1.7786 - val_loss: 0.0225\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.9542 - val_loss: 0.0228\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 19.0702 - val_loss: 0.0229\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 11.1458 - val_loss: 0.0231\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.4203 - val_loss: 0.0228\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 10.0763 - val_loss: 0.0242\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.8568 - val_loss: 0.0235\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.3494 - val_loss: 0.0247\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.2823 - val_loss: 0.0239\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.8370 - val_loss: 0.0261\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.3846 - val_loss: 0.0242\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8989 - val_loss: 0.0268\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4146 - val_loss: 0.0250\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9387 - val_loss: 0.0264\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6837 - val_loss: 0.0259\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5705 - val_loss: 0.0268\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1B939940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_409 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 2.3911 - val_loss: 0.0262\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 13.3363 - val_loss: 0.0270\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 18.3847 - val_loss: 0.0271\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.1917 - val_loss: 0.0277\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 11.4940 - val_loss: 0.0278\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.0086 - val_loss: 0.0307\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.3802 - val_loss: 0.0286\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.6264 - val_loss: 0.0310\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.9750 - val_loss: 0.0299\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6219 - val_loss: 0.0322\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4359 - val_loss: 0.0317\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8396 - val_loss: 0.0327\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5230 - val_loss: 0.0328\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4175 - val_loss: 0.0323\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6255 - val_loss: 0.0341\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8449 - val_loss: 0.0319\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025018F28280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_410 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 364ms/step - loss: 1.9728 - val_loss: 0.0307\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 9.7080 - val_loss: 0.0307\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 18.6268 - val_loss: 0.0328\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 8.5750 - val_loss: 0.0342\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.5372 - val_loss: 0.0354\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.4601 - val_loss: 0.0376\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.3747 - val_loss: 0.0380\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.3110 - val_loss: 0.0412\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.1804 - val_loss: 0.0410\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.0046 - val_loss: 0.0408\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.8204 - val_loss: 0.0443\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1416 - val_loss: 0.0428\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5630 - val_loss: 0.0431\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9457 - val_loss: 0.0436\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6061 - val_loss: 0.0451\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3535 - val_loss: 0.0437\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEA6DF430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_411 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 2.2361 - val_loss: 0.0326\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 13.4329 - val_loss: 0.0330\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 27.8634 - val_loss: 0.0341\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.0148 - val_loss: 0.0350\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 14.8990 - val_loss: 0.0359\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.7461 - val_loss: 0.0361\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.6183 - val_loss: 0.0363\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.3661 - val_loss: 0.0371\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.9590 - val_loss: 0.0361\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2422 - val_loss: 0.0372\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5820 - val_loss: 0.0358\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3160 - val_loss: 0.0370\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3662 - val_loss: 0.0355\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5756 - val_loss: 0.0368\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7499 - val_loss: 0.0351\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6212 - val_loss: 0.0366\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250280A6DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_412 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 512ms/step - loss: 2.8305 - val_loss: 0.0340\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 19.2839 - val_loss: 0.0381\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.9150 - val_loss: 0.0394\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 13.7482 - val_loss: 0.0420\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.8274 - val_loss: 0.0453\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0198 - val_loss: 0.0456\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.9443 - val_loss: 0.0499\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.6945 - val_loss: 0.0514\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.8504 - val_loss: 0.0537\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.6128 - val_loss: 0.0558\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1686 - val_loss: 0.0558\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.3202 - val_loss: 0.0642\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7373 - val_loss: 0.0596\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4769 - val_loss: 0.0666\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2714 - val_loss: 0.0647\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1533 - val_loss: 0.0688\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA140EA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_413 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 1.8996 - val_loss: 0.0366\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.9834 - val_loss: 0.0374\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 19.9386 - val_loss: 0.0418\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 10.2430 - val_loss: 0.0426\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6901 - val_loss: 0.0483\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.2209 - val_loss: 0.0490\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.6339 - val_loss: 0.0486\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.4849 - val_loss: 0.0512\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.1059 - val_loss: 0.0499\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6113 - val_loss: 0.0514\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.8223 - val_loss: 0.0522\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9358 - val_loss: 0.0522\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4968 - val_loss: 0.0526\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2836 - val_loss: 0.0539\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.1865 - val_loss: 0.0535\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1447 - val_loss: 0.0559\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002505C41BAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_414 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.5863 - val_loss: 0.0365\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 18.7142 - val_loss: 0.0408\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 23.8737 - val_loss: 0.0413\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.6654 - val_loss: 0.0435\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.8579 - val_loss: 0.0423\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.6883 - val_loss: 0.0458\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 6.9096 - val_loss: 0.0409\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.3167 - val_loss: 0.0426\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.1113 - val_loss: 0.0401\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 2.2818 - val_loss: 0.0405\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3320 - val_loss: 0.0397\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6457 - val_loss: 0.0397\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4558 - val_loss: 0.0388\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3748 - val_loss: 0.0391\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2974 - val_loss: 0.0386\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2693 - val_loss: 0.0389\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA34B5790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_415 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 319ms/step - loss: 2.2817 - val_loss: 0.0382\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.0784 - val_loss: 0.0425\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 17.4934 - val_loss: 0.0468\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10.9593 - val_loss: 0.0458\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.4295 - val_loss: 0.0498\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 9.3937 - val_loss: 0.0479\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 7.7454 - val_loss: 0.0533\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6737 - val_loss: 0.0503\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 3.6792 - val_loss: 0.0546\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4272 - val_loss: 0.0523\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7894 - val_loss: 0.0561\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5171 - val_loss: 0.0541\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3401 - val_loss: 0.0572\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.3213 - val_loss: 0.0557\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3503 - val_loss: 0.0594\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3807 - val_loss: 0.0563\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250025380D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_416 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 338ms/step - loss: 2.4462 - val_loss: 0.0382\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.5782 - val_loss: 0.0395\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 17.5430 - val_loss: 0.0425\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.4764 - val_loss: 0.0464\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 9.1159 - val_loss: 0.0471\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.1282 - val_loss: 0.0466\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.5047 - val_loss: 0.0501\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.3085 - val_loss: 0.0474\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4623 - val_loss: 0.0514\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.8053 - val_loss: 0.0470\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.4364 - val_loss: 0.0512\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3112 - val_loss: 0.0460\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8163 - val_loss: 0.0504\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7011 - val_loss: 0.0458\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7694 - val_loss: 0.0508\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6246 - val_loss: 0.0462\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002507D99E940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_417 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 268ms/step - loss: 1.9517 - val_loss: 0.0393\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.9318 - val_loss: 0.0377\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 21.4198 - val_loss: 0.0430\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.1654 - val_loss: 0.0456\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.0922 - val_loss: 0.0464\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 6.0368 - val_loss: 0.0503\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.8479 - val_loss: 0.0497\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.9652 - val_loss: 0.0560\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.5666 - val_loss: 0.0518\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.1709 - val_loss: 0.0595\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.0521 - val_loss: 0.0532\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.4568 - val_loss: 0.0615\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.2330 - val_loss: 0.0554\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5730 - val_loss: 0.0606\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0056 - val_loss: 0.0581\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6097 - val_loss: 0.0603\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3701 - val_loss: 0.0602\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FE93CDCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_418 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 241ms/step - loss: 1.1506 - val_loss: 0.0395\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.4525 - val_loss: 0.0402\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 17.2718 - val_loss: 0.0467\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 13.4120 - val_loss: 0.0480\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 8.1625 - val_loss: 0.0474\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 8.0530 - val_loss: 0.0490\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.6069 - val_loss: 0.0474\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.0511 - val_loss: 0.0480\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.8677 - val_loss: 0.0484\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.9266 - val_loss: 0.0471\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.1376 - val_loss: 0.0490\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5562 - val_loss: 0.0474\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.2519 - val_loss: 0.0481\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9945 - val_loss: 0.0457\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7797 - val_loss: 0.0463\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.5391 - val_loss: 0.0444\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025015FE3820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_419 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 2.0318 - val_loss: 0.0372\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 16.8812 - val_loss: 0.0400\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 19.2070 - val_loss: 0.0412\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.2095 - val_loss: 0.0464\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.6560 - val_loss: 0.0474\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.5809 - val_loss: 0.0501\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.0379 - val_loss: 0.0520\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.3343 - val_loss: 0.0537\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.3584 - val_loss: 0.0546\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.8200 - val_loss: 0.0533\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4083 - val_loss: 0.0546\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8764 - val_loss: 0.0522\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7185 - val_loss: 0.0564\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.6393 - val_loss: 0.0520\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6328 - val_loss: 0.0576\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7939 - val_loss: 0.0517\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEE7C08B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_420 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 272ms/step - loss: 2.2188 - val_loss: 0.0383\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 15ms/step - loss: 11.5492 - val_loss: 0.0472\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 17.0515 - val_loss: 0.0469\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.0446 - val_loss: 0.0452\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.9144 - val_loss: 0.0497\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.5921 - val_loss: 0.0469\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.4578 - val_loss: 0.0524\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.3784 - val_loss: 0.0483\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2644 - val_loss: 0.0516\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.1246 - val_loss: 0.0478\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.5422 - val_loss: 0.0514\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3828 - val_loss: 0.0483\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1278 - val_loss: 0.0512\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8080 - val_loss: 0.0477\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7155 - val_loss: 0.0513\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8233 - val_loss: 0.0470\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250616D1310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_421 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 248ms/step - loss: 1.9435 - val_loss: 0.0416\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 9.6515 - val_loss: 0.0504\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 28.4627 - val_loss: 0.0488\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.2258 - val_loss: 0.0518\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.5293 - val_loss: 0.0535\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.0056 - val_loss: 0.0553\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.5064 - val_loss: 0.0566\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.4916 - val_loss: 0.0573\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.9800 - val_loss: 0.0601\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.8056 - val_loss: 0.0591\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0588 - val_loss: 0.0634\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.9077 - val_loss: 0.0586\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.8024 - val_loss: 0.0668\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7141 - val_loss: 0.0605\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7085 - val_loss: 0.0681\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6617 - val_loss: 0.0637\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503E066C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_422 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 2.2855 - val_loss: 0.0466\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.3022 - val_loss: 0.0448\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 17.3923 - val_loss: 0.0501\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 12.3149 - val_loss: 0.0521\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9738 - val_loss: 0.0547\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 6.1311 - val_loss: 0.0559\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.7791 - val_loss: 0.0573\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.8936 - val_loss: 0.0596\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2129 - val_loss: 0.0570\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.0716 - val_loss: 0.0635\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4844 - val_loss: 0.0582\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8450 - val_loss: 0.0635\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7763 - val_loss: 0.0588\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8083 - val_loss: 0.0645\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7013 - val_loss: 0.0592\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5882 - val_loss: 0.0658\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4043 - val_loss: 0.0593\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250926895E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_423 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.5987 - val_loss: 0.0452\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 5.9571 - val_loss: 0.0502\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 18.2406 - val_loss: 0.0493\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.7211 - val_loss: 0.0523\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.0272 - val_loss: 0.0538\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 8.3166 - val_loss: 0.0513\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 7.9408 - val_loss: 0.0548\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.4508 - val_loss: 0.0556\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.1517 - val_loss: 0.0549\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.6002 - val_loss: 0.0578\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.6986 - val_loss: 0.0557\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7471 - val_loss: 0.0576\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4332 - val_loss: 0.0551\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.9672 - val_loss: 0.0580\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7380 - val_loss: 0.0568\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5619 - val_loss: 0.0577\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F70E99A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_424 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 262ms/step - loss: 2.7527 - val_loss: 0.0485\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.8909 - val_loss: 0.0488\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.6814 - val_loss: 0.0514\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.5270 - val_loss: 0.0542\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.5625 - val_loss: 0.0537\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7501 - val_loss: 0.0565\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.9467 - val_loss: 0.0586\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5294 - val_loss: 0.0604\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6602 - val_loss: 0.0607\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.9374 - val_loss: 0.0602\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5263 - val_loss: 0.0611\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2739 - val_loss: 0.0597\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1635 - val_loss: 0.0605\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1742 - val_loss: 0.0596\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2258 - val_loss: 0.0610\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3074 - val_loss: 0.0595\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503C7159D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_425 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 273ms/step - loss: 1.7337 - val_loss: 0.0475\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.8761 - val_loss: 0.0514\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.0685 - val_loss: 0.0506\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.3656 - val_loss: 0.0529\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.1292 - val_loss: 0.0544\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.6872 - val_loss: 0.0549\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.4745 - val_loss: 0.0569\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.5673 - val_loss: 0.0572\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.3110 - val_loss: 0.0606\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4365 - val_loss: 0.0592\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0029 - val_loss: 0.0629\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5764 - val_loss: 0.0612\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4083 - val_loss: 0.0621\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5205 - val_loss: 0.0635\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.9202 - val_loss: 0.0601\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1016 - val_loss: 0.0631\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F444CA1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_426 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 276ms/step - loss: 3.3679 - val_loss: 0.0476\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.0760 - val_loss: 0.0511\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 21.2388 - val_loss: 0.0529\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.3149 - val_loss: 0.0549\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.3222 - val_loss: 0.0579\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.0777 - val_loss: 0.0594\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4497 - val_loss: 0.0603\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.6298 - val_loss: 0.0642\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7609 - val_loss: 0.0639\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2591 - val_loss: 0.0682\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6718 - val_loss: 0.0680\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4571 - val_loss: 0.0722\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3406 - val_loss: 0.0691\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2696 - val_loss: 0.0759\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2167 - val_loss: 0.0710\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1682 - val_loss: 0.0793\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F51C9C5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_427 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 3.4991 - val_loss: 0.0485\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 11.9748 - val_loss: 0.0494\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.0543 - val_loss: 0.0509\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 10.1112 - val_loss: 0.0523\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.6698 - val_loss: 0.0528\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.1688 - val_loss: 0.0546\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.9647 - val_loss: 0.0554\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.5924 - val_loss: 0.0561\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.4706 - val_loss: 0.0575\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4055 - val_loss: 0.0581\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.2606 - val_loss: 0.0607\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.8223 - val_loss: 0.0593\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4954 - val_loss: 0.0639\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3858 - val_loss: 0.0604\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5819 - val_loss: 0.0681\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.9233 - val_loss: 0.0606\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF9D0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_428 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 1.9188 - val_loss: 0.0503\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.2490 - val_loss: 0.0499\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step - loss: 13.0512 - val_loss: 0.0555\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.9196 - val_loss: 0.0600\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 10.9830 - val_loss: 0.0627\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.7148 - val_loss: 0.0650\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.7939 - val_loss: 0.0684\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.1818 - val_loss: 0.0682\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.9834 - val_loss: 0.0711\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.9297 - val_loss: 0.0717\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0260 - val_loss: 0.0752\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7629 - val_loss: 0.0753\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6357 - val_loss: 0.0799\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7540 - val_loss: 0.0794\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1033 - val_loss: 0.0788\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.1092 - val_loss: 0.0885\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9576 - val_loss: 0.0792\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F4A026B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_429 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1.7301 - val_loss: 0.0477\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 9.1254 - val_loss: 0.0502\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 17.3526 - val_loss: 0.0524\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 13.3861 - val_loss: 0.0519\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.8992 - val_loss: 0.0555\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.1874 - val_loss: 0.0519\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.8411 - val_loss: 0.0565\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4.4121 - val_loss: 0.0521\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.8838 - val_loss: 0.0575\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 1.9393 - val_loss: 0.0536\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9400 - val_loss: 0.0568\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5805 - val_loss: 0.0540\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5108 - val_loss: 0.0567\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4587 - val_loss: 0.0546\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3971 - val_loss: 0.0568\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3522 - val_loss: 0.0558\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002504F2B79D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_430 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 251ms/step - loss: 1.7335 - val_loss: 0.0479\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 14.4225 - val_loss: 0.0510\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.6731 - val_loss: 0.0519\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.0054 - val_loss: 0.0534\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 7.5942 - val_loss: 0.0529\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6.7726 - val_loss: 0.0572\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2469 - val_loss: 0.0557\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.6556 - val_loss: 0.0606\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.2721 - val_loss: 0.0601\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1316 - val_loss: 0.0622\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6521 - val_loss: 0.0630\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4898 - val_loss: 0.0627\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5013 - val_loss: 0.0649\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5937 - val_loss: 0.0639\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6852 - val_loss: 0.0644\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5648 - val_loss: 0.0659\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025015FE3670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_431 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 430ms/step - loss: 1.9879 - val_loss: 0.0482\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 28.3754 - val_loss: 0.0524\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 16.5539 - val_loss: 0.0508\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 10.1721 - val_loss: 0.0508\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.0980 - val_loss: 0.0518\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 5.3865 - val_loss: 0.0506\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.2737 - val_loss: 0.0525\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.9431 - val_loss: 0.0510\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.7669 - val_loss: 0.0519\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.4188 - val_loss: 0.0512\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0948 - val_loss: 0.0509\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1055 - val_loss: 0.0520\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.9350 - val_loss: 0.0501\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7374 - val_loss: 0.0513\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5268 - val_loss: 0.0497\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4328 - val_loss: 0.0501\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC4536430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_432 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 254ms/step - loss: 1.7963 - val_loss: 0.0485\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.6245 - val_loss: 0.0508\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 16.4783 - val_loss: 0.0532\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 16.3173 - val_loss: 0.0576\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 7.1357 - val_loss: 0.0557\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.1811 - val_loss: 0.0605\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.9097 - val_loss: 0.0583\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3.8166 - val_loss: 0.0610\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2.4984 - val_loss: 0.0587\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.5133 - val_loss: 0.0616\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.9675 - val_loss: 0.0607\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6517 - val_loss: 0.0624\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.6091 - val_loss: 0.0599\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6255 - val_loss: 0.0634\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5682 - val_loss: 0.0596\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.5527 - val_loss: 0.0639\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025039140280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_433 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.4776 - val_loss: 0.0486\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.7774 - val_loss: 0.0539\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 16.6772 - val_loss: 0.0518\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 11.6011 - val_loss: 0.0527\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.2739 - val_loss: 0.0549\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.3086 - val_loss: 0.0532\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.8704 - val_loss: 0.0560\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6813 - val_loss: 0.0557\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.5395 - val_loss: 0.0570\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.4491 - val_loss: 0.0564\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6910 - val_loss: 0.0562\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.8399 - val_loss: 0.0573\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4462 - val_loss: 0.0548\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4148 - val_loss: 0.0582\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8622 - val_loss: 0.0522\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3524 - val_loss: 0.0592\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A037BAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_434 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 383ms/step - loss: 2.0418 - val_loss: 0.0492\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.1993 - val_loss: 0.0515\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 16.2052 - val_loss: 0.0512\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 15.6439 - val_loss: 0.0531\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.4563 - val_loss: 0.0524\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.1748 - val_loss: 0.0526\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.6105 - val_loss: 0.0521\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.1444 - val_loss: 0.0548\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 4.0184 - val_loss: 0.0523\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.0473 - val_loss: 0.0565\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6453 - val_loss: 0.0523\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1915 - val_loss: 0.0568\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7304 - val_loss: 0.0539\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5666 - val_loss: 0.0567\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4522 - val_loss: 0.0557\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3106 - val_loss: 0.0571\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336B2A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_435 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 299ms/step - loss: 1.7626 - val_loss: 0.0492\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 11.1994 - val_loss: 0.0514\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.2105 - val_loss: 0.0521\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.2913 - val_loss: 0.0546\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.0140 - val_loss: 0.0579\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.3000 - val_loss: 0.0587\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.9560 - val_loss: 0.0617\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.3870 - val_loss: 0.0621\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.1070 - val_loss: 0.0636\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.3007 - val_loss: 0.0651\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3984 - val_loss: 0.0647\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2523 - val_loss: 0.0683\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9404 - val_loss: 0.0671\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4675 - val_loss: 0.0699\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2524 - val_loss: 0.0715\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1961 - val_loss: 0.0703\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E888829D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_436 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 569ms/step - loss: 2.3247 - val_loss: 0.0502\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.8666 - val_loss: 0.0508\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 16.9393 - val_loss: 0.0532\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.4074 - val_loss: 0.0562\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.7673 - val_loss: 0.0583\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 6.7258 - val_loss: 0.0569\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.3123 - val_loss: 0.0599\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.7794 - val_loss: 0.0573\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.3318 - val_loss: 0.0585\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7998 - val_loss: 0.0578\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0354 - val_loss: 0.0575\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5679 - val_loss: 0.0585\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3314 - val_loss: 0.0578\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2500 - val_loss: 0.0590\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2854 - val_loss: 0.0590\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.3265 - val_loss: 0.0592\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC1255700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_437 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 1.1755 - val_loss: 0.0500\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.8395 - val_loss: 0.0545\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 15.2450 - val_loss: 0.0501\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 11.8123 - val_loss: 0.0519\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.7431 - val_loss: 0.0517\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.4827 - val_loss: 0.0512\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.3301 - val_loss: 0.0516\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.9310 - val_loss: 0.0510\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9109 - val_loss: 0.0511\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9087 - val_loss: 0.0504\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4626 - val_loss: 0.0505\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4849 - val_loss: 0.0509\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6580 - val_loss: 0.0500\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4882 - val_loss: 0.0510\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9245 - val_loss: 0.0497\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6403 - val_loss: 0.0504\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3749 - val_loss: 0.0496\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2549 - val_loss: 0.0499\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2833 - val_loss: 0.0495\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4165 - val_loss: 0.0497\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5764 - val_loss: 0.0495\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5999 - val_loss: 0.0495\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4873 - val_loss: 0.0496\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3435 - val_loss: 0.0496\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2414 - val_loss: 0.0498\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2005 - val_loss: 0.0499\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2109 - val_loss: 0.0500\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3038 - val_loss: 0.0510\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4489 - val_loss: 0.0503\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6270 - val_loss: 0.0525\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7404 - val_loss: 0.0503\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.7944 - val_loss: 0.0527\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7120 - val_loss: 0.0501\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7414 - val_loss: 0.0513\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7121 - val_loss: 0.0505\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7079 - val_loss: 0.0503\n",
      "Epoch 00036: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025043670B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_438 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 266ms/step - loss: 1.2326 - val_loss: 0.0506\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.0703 - val_loss: 0.0530\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 18.0961 - val_loss: 0.0527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.0726 - val_loss: 0.0550\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.0265 - val_loss: 0.0526\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.6508 - val_loss: 0.0573\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.9761 - val_loss: 0.0550\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6586 - val_loss: 0.0565\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0819 - val_loss: 0.0558\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.5276 - val_loss: 0.0563\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3037 - val_loss: 0.0582\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4811 - val_loss: 0.0568\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3133 - val_loss: 0.0609\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7417 - val_loss: 0.0582\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1138 - val_loss: 0.0627\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.5202 - val_loss: 0.0611\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002500A9D1790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_439 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 295ms/step - loss: 1.3704 - val_loss: 0.0516\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.7209 - val_loss: 0.0524\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 16.6118 - val_loss: 0.0524\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 11.1953 - val_loss: 0.0527\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 11.2001 - val_loss: 0.0549\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 6.7161 - val_loss: 0.0535\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.4758 - val_loss: 0.0548\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.6696 - val_loss: 0.0541\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.8042 - val_loss: 0.0546\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.5134 - val_loss: 0.0541\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.7109 - val_loss: 0.0536\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.1036 - val_loss: 0.0554\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5495 - val_loss: 0.0530\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2316 - val_loss: 0.0562\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6552 - val_loss: 0.0536\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2561 - val_loss: 0.0559\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002507DA4C160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_440 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 250ms/step - loss: 2.8034 - val_loss: 0.0512\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 12.8675 - val_loss: 0.0526\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 20.4714 - val_loss: 0.0533\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.7637 - val_loss: 0.0530\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.2105 - val_loss: 0.0555\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.8840 - val_loss: 0.0546\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.0840 - val_loss: 0.0590\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.4414 - val_loss: 0.0570\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.8223 - val_loss: 0.0620\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0111 - val_loss: 0.0581\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.3292 - val_loss: 0.0610\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.8089 - val_loss: 0.0608\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0816 - val_loss: 0.0595\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.8355 - val_loss: 0.0632\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7518 - val_loss: 0.0586\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6973 - val_loss: 0.0654\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025069E41DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_441 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 286ms/step - loss: 1.7461 - val_loss: 0.0509\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8.5461 - val_loss: 0.0514\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 22.1171 - val_loss: 0.0533\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 9.5751 - val_loss: 0.0535\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.1499 - val_loss: 0.0534\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.9197 - val_loss: 0.0545\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.9532 - val_loss: 0.0531\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.8196 - val_loss: 0.0542\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1205 - val_loss: 0.0533\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2.5027 - val_loss: 0.0538\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.2428 - val_loss: 0.0530\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.7862 - val_loss: 0.0536\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.3223 - val_loss: 0.0525\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8468 - val_loss: 0.0544\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7622 - val_loss: 0.0522\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8535 - val_loss: 0.0548\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002501747D9D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_442 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.5378 - val_loss: 0.0518\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.5073 - val_loss: 0.0522\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 19.2258 - val_loss: 0.0548\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 15.2722 - val_loss: 0.0549\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.6532 - val_loss: 0.0565\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.6297 - val_loss: 0.0559\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.1451 - val_loss: 0.0567\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.6219 - val_loss: 0.0565\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.3122 - val_loss: 0.0576\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.8155 - val_loss: 0.0581\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5203 - val_loss: 0.0606\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.0973 - val_loss: 0.0597\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5933 - val_loss: 0.0620\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2870 - val_loss: 0.0617\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2107 - val_loss: 0.0643\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2064 - val_loss: 0.0629\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F1B939A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_443 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 1.5101 - val_loss: 0.0523\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 21.9697 - val_loss: 0.0516\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.4309 - val_loss: 0.0524\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.9094 - val_loss: 0.0546\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.6428 - val_loss: 0.0573\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.9237 - val_loss: 0.0575\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.1356 - val_loss: 0.0621\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.7278 - val_loss: 0.0608\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.2750 - val_loss: 0.0659\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2871 - val_loss: 0.0640\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5467 - val_loss: 0.0695\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.2038 - val_loss: 0.0678\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0643 - val_loss: 0.0707\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 0.9062 - val_loss: 0.0722\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5509 - val_loss: 0.0730\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2854 - val_loss: 0.0750\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1293 - val_loss: 0.0746\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002508021C0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_444 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 263ms/step - loss: 1.7567 - val_loss: 0.0516\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.2066 - val_loss: 0.0522\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 22.9384 - val_loss: 0.0524\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.6962 - val_loss: 0.0521\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.1116 - val_loss: 0.0531\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.1093 - val_loss: 0.0525\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.8049 - val_loss: 0.0537\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.1748 - val_loss: 0.0523\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2559 - val_loss: 0.0532\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.7141 - val_loss: 0.0523\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4645 - val_loss: 0.0525\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.9688 - val_loss: 0.0522\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7182 - val_loss: 0.0521\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6888 - val_loss: 0.0521\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.8324 - val_loss: 0.0521\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9149 - val_loss: 0.0517\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A476FB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_445 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.7177 - val_loss: 0.0509\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.6048 - val_loss: 0.0547\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 18.2538 - val_loss: 0.0568\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 14.5370 - val_loss: 0.0591\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.3246 - val_loss: 0.0613\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.7755 - val_loss: 0.0624\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.5099 - val_loss: 0.0644\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.9427 - val_loss: 0.0679\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.7201 - val_loss: 0.0687\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.5698 - val_loss: 0.0747\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3068 - val_loss: 0.0762\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.7193 - val_loss: 0.0816\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3715 - val_loss: 0.0831\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2683 - val_loss: 0.0892\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3962 - val_loss: 0.0901\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5735 - val_loss: 0.0963\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250B02071F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_446 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 265ms/step - loss: 2.3999 - val_loss: 0.0505\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 18ms/step - loss: 15.4356 - val_loss: 0.0509\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.3689 - val_loss: 0.0506\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 12.0774 - val_loss: 0.0520\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.5861 - val_loss: 0.0507\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.4796 - val_loss: 0.0513\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.8610 - val_loss: 0.0510\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.5371 - val_loss: 0.0510\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.4947 - val_loss: 0.0510\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 3.1235 - val_loss: 0.0512\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.2935 - val_loss: 0.0507\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6959 - val_loss: 0.0512\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5958 - val_loss: 0.0506\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2622 - val_loss: 0.0509\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7687 - val_loss: 0.0505\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4430 - val_loss: 0.0506\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2377 - val_loss: 0.0505\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1433 - val_loss: 0.0505\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0990 - val_loss: 0.0505\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.0868 - val_loss: 0.0505\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1008 - val_loss: 0.0504\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1446 - val_loss: 0.0505\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.2428 - val_loss: 0.0504\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4063 - val_loss: 0.0512\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5830 - val_loss: 0.0505\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7020 - val_loss: 0.0528\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7552 - val_loss: 0.0505\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7923 - val_loss: 0.0532\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.9497 - val_loss: 0.0503\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0443 - val_loss: 0.0536\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.1662 - val_loss: 0.0500\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3435 - val_loss: 0.0520\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3751 - val_loss: 0.0516\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9564 - val_loss: 0.0516\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9116 - val_loss: 0.0525\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8566 - val_loss: 0.0511\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8068 - val_loss: 0.0526\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 0.8745 - val_loss: 0.0520\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8447 - val_loss: 0.0530\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8183 - val_loss: 0.0538\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002504FCD3280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_447 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 2.8637 - val_loss: 0.0495\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 15.6358 - val_loss: 0.0537\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 17.5916 - val_loss: 0.0565\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10.2873 - val_loss: 0.0581\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.0560 - val_loss: 0.0582\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.7783 - val_loss: 0.0586\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.3939 - val_loss: 0.0600\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.0229 - val_loss: 0.0601\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.7865 - val_loss: 0.0597\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 3.0154 - val_loss: 0.0617\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8320 - val_loss: 0.0614\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.6921 - val_loss: 0.0647\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.1363 - val_loss: 0.0642\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9970 - val_loss: 0.0661\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8082 - val_loss: 0.0669\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5173 - val_loss: 0.0677\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F920F2F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_448 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 249ms/step - loss: 1.8472 - val_loss: 0.0506\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.7497 - val_loss: 0.0503\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 18.0568 - val_loss: 0.0515\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.3854 - val_loss: 0.0513\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.4538 - val_loss: 0.0518\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.5061 - val_loss: 0.0517\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.7637 - val_loss: 0.0517\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7501 - val_loss: 0.0517\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0869 - val_loss: 0.0523\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.2530 - val_loss: 0.0519\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2321 - val_loss: 0.0512\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.9071 - val_loss: 0.0521\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.7285 - val_loss: 0.0509\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8455 - val_loss: 0.0514\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3220 - val_loss: 0.0509\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.1272 - val_loss: 0.0512\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0379 - val_loss: 0.0511\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC0C49E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_449 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 257ms/step - loss: 1.7458 - val_loss: 0.0511\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 11.2397 - val_loss: 0.0506\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 22.4373 - val_loss: 0.0545\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2880 - val_loss: 0.0538\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.1239 - val_loss: 0.0530\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.4932 - val_loss: 0.0550\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.5173 - val_loss: 0.0545\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.3482 - val_loss: 0.0561\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4788 - val_loss: 0.0557\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6464 - val_loss: 0.0559\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6192 - val_loss: 0.0588\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5287 - val_loss: 0.0560\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4348 - val_loss: 0.0592\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.1305 - val_loss: 0.0577\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6976 - val_loss: 0.0590\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4145 - val_loss: 0.0597\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.2345 - val_loss: 0.0605\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A03FCA60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_450 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 252ms/step - loss: 1.9609 - val_loss: 0.0511\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.6738 - val_loss: 0.0518\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.6016 - val_loss: 0.0515\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 14.7342 - val_loss: 0.0526\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.4745 - val_loss: 0.0533\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.2502 - val_loss: 0.0542\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 7.2494 - val_loss: 0.0552\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.5174 - val_loss: 0.0531\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.8645 - val_loss: 0.0555\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.1580 - val_loss: 0.0546\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.0525 - val_loss: 0.0561\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.3115 - val_loss: 0.0554\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.4135 - val_loss: 0.0560\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.5344 - val_loss: 0.0559\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0280 - val_loss: 0.0553\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8028 - val_loss: 0.0562\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250125DA430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_451 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 308ms/step - loss: 2.1362 - val_loss: 0.0506\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.3669 - val_loss: 0.0531\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 21.9754 - val_loss: 0.0537\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.3906 - val_loss: 0.0529\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 10.4005 - val_loss: 0.0568\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.9129 - val_loss: 0.0527\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.7652 - val_loss: 0.0557\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.9600 - val_loss: 0.0533\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4.2326 - val_loss: 0.0568\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.6432 - val_loss: 0.0549\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.9661 - val_loss: 0.0541\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.7751 - val_loss: 0.0566\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2000 - val_loss: 0.0540\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2809 - val_loss: 0.0572\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7817 - val_loss: 0.0545\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4292 - val_loss: 0.0567\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F44D83D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_452 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 551ms/step - loss: 1.3435 - val_loss: 0.0511\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.4874 - val_loss: 0.0554\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 23.3223 - val_loss: 0.0522\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.5032 - val_loss: 0.0573\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.5993 - val_loss: 0.0552\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.1942 - val_loss: 0.0596\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.1682 - val_loss: 0.0566\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.2708 - val_loss: 0.0590\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.2733 - val_loss: 0.0592\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2.8113 - val_loss: 0.0599\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.2828 - val_loss: 0.0613\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.7320 - val_loss: 0.0598\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4937 - val_loss: 0.0605\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.3431 - val_loss: 0.0610\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4820 - val_loss: 0.0620\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8663 - val_loss: 0.0613\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250350410D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_453 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 374ms/step - loss: 1.5514 - val_loss: 0.0510\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.3248 - val_loss: 0.0518\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 15.9022 - val_loss: 0.0540\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 11.4776 - val_loss: 0.0524\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.0969 - val_loss: 0.0563\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.5831 - val_loss: 0.0543\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.5446 - val_loss: 0.0566\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.3627 - val_loss: 0.0545\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.6555 - val_loss: 0.0561\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.8019 - val_loss: 0.0550\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.7183 - val_loss: 0.0549\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1748 - val_loss: 0.0543\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2343 - val_loss: 0.0540\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9941 - val_loss: 0.0548\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7308 - val_loss: 0.0533\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5964 - val_loss: 0.0550\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002501404A430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_454 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 585ms/step - loss: 1.4348 - val_loss: 0.0504\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.0636 - val_loss: 0.0500\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.1728 - val_loss: 0.0514\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.2706 - val_loss: 0.0517\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.9036 - val_loss: 0.0528\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.0437 - val_loss: 0.0541\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.9276 - val_loss: 0.0542\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5300 - val_loss: 0.0553\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.2197 - val_loss: 0.0532\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2393 - val_loss: 0.0539\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4495 - val_loss: 0.0529\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.3035 - val_loss: 0.0531\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6549 - val_loss: 0.0529\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2338 - val_loss: 0.0525\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8933 - val_loss: 0.0523\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4755 - val_loss: 0.0520\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2509 - val_loss: 0.0524\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002506C42E1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_455 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 478ms/step - loss: 2.4103 - val_loss: 0.0509\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 14.1487 - val_loss: 0.0511\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 20.0830 - val_loss: 0.0525\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 11.8022 - val_loss: 0.0523\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.6605 - val_loss: 0.0532\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.7601 - val_loss: 0.0517\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.4999 - val_loss: 0.0515\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.7694 - val_loss: 0.0511\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.7733 - val_loss: 0.0513\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9964 - val_loss: 0.0514\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4987 - val_loss: 0.0509\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7557 - val_loss: 0.0516\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3203 - val_loss: 0.0514\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9330 - val_loss: 0.0519\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6625 - val_loss: 0.0516\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5390 - val_loss: 0.0520\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A1733160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_456 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 587ms/step - loss: 1.4500 - val_loss: 0.0522\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 20.5722 - val_loss: 0.0521\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 20.9168 - val_loss: 0.0539\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.4306 - val_loss: 0.0548\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.2974 - val_loss: 0.0566\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.5786 - val_loss: 0.0562\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.0365 - val_loss: 0.0572\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9706 - val_loss: 0.0585\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.7198 - val_loss: 0.0599\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1251 - val_loss: 0.0606\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2487 - val_loss: 0.0621\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7127 - val_loss: 0.0641\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6859 - val_loss: 0.0646\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8502 - val_loss: 0.0688\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9032 - val_loss: 0.0678\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7135 - val_loss: 0.0723\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5071 - val_loss: 0.0721\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250BB425280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_457 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 345ms/step - loss: 1.5163 - val_loss: 0.0507\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.6991 - val_loss: 0.0551\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.4651 - val_loss: 0.0509\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.2220 - val_loss: 0.0525\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 10.7737 - val_loss: 0.0512\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.1046 - val_loss: 0.0520\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.9896 - val_loss: 0.0511\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.5935 - val_loss: 0.0515\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.2421 - val_loss: 0.0509\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.0208 - val_loss: 0.0509\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3517 - val_loss: 0.0505\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5586 - val_loss: 0.0505\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2503 - val_loss: 0.0506\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2433 - val_loss: 0.0508\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4038 - val_loss: 0.0509\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6550 - val_loss: 0.0516\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8428 - val_loss: 0.0514\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7887 - val_loss: 0.0526\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6005 - val_loss: 0.0524\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4180 - val_loss: 0.0542\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2785 - val_loss: 0.0539\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1967 - val_loss: 0.0559\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1636 - val_loss: 0.0565\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 0.2142 - val_loss: 0.0567\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3726 - val_loss: 0.0602\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6118 - val_loss: 0.0558\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEBE5BD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_458 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 556ms/step - loss: 1.7222 - val_loss: 0.0521\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.3148 - val_loss: 0.0529\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.1189 - val_loss: 0.0548\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.0991 - val_loss: 0.0549\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.5113 - val_loss: 0.0556\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.4019 - val_loss: 0.0559\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.1169 - val_loss: 0.0574\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.2998 - val_loss: 0.0575\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.3314 - val_loss: 0.0573\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0883 - val_loss: 0.0607\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.7288 - val_loss: 0.0583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0272 - val_loss: 0.0620\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.0244 - val_loss: 0.0610\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4493 - val_loss: 0.0639\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8616 - val_loss: 0.0648\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5291 - val_loss: 0.0645\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A1733A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_459 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 476ms/step - loss: 2.0965 - val_loss: 0.0513\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 12.5440 - val_loss: 0.0534\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 21.0764 - val_loss: 0.0512\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.8384 - val_loss: 0.0531\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 10.1020 - val_loss: 0.0516\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.0530 - val_loss: 0.0521\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.2990 - val_loss: 0.0520\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.6536 - val_loss: 0.0544\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.1142 - val_loss: 0.0526\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.0876 - val_loss: 0.0549\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2657 - val_loss: 0.0529\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0578 - val_loss: 0.0548\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7734 - val_loss: 0.0531\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5857 - val_loss: 0.0551\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6395 - val_loss: 0.0533\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7166 - val_loss: 0.0558\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.7619 - val_loss: 0.0535\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5462 - val_loss: 0.0560\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025047185F70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_460 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 407ms/step - loss: 2.3891 - val_loss: 0.0527\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 15.2589 - val_loss: 0.0522\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 15.5727 - val_loss: 0.0517\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.9581 - val_loss: 0.0523\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.6137 - val_loss: 0.0529\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 7.7822 - val_loss: 0.0520\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.1030 - val_loss: 0.0529\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.9448 - val_loss: 0.0518\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 5.0617 - val_loss: 0.0524\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9518 - val_loss: 0.0519\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.8989 - val_loss: 0.0521\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.6279 - val_loss: 0.0517\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.8593 - val_loss: 0.0516\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.1735 - val_loss: 0.0518\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6980 - val_loss: 0.0513\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4457 - val_loss: 0.0515\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4669 - val_loss: 0.0512\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5758 - val_loss: 0.0514\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6333 - val_loss: 0.0513\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6606 - val_loss: 0.0514\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5906 - val_loss: 0.0514\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5132 - val_loss: 0.0515\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4161 - val_loss: 0.0517\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3204 - val_loss: 0.0515\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2576 - val_loss: 0.0517\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2230 - val_loss: 0.0515\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2058 - val_loss: 0.0517\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2128 - val_loss: 0.0516\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2391 - val_loss: 0.0517\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2997 - val_loss: 0.0521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4168 - val_loss: 0.0514\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5430 - val_loss: 0.0533\n",
      "Epoch 00032: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250125DAD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_461 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 507ms/step - loss: 1.8957 - val_loss: 0.0524\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.7612 - val_loss: 0.0523\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 24.8911 - val_loss: 0.0527\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.3857 - val_loss: 0.0525\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.9067 - val_loss: 0.0528\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.2968 - val_loss: 0.0518\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.2453 - val_loss: 0.0522\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.0541 - val_loss: 0.0523\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 5.1385 - val_loss: 0.0520\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.2074 - val_loss: 0.0517\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.9660 - val_loss: 0.0515\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.2555 - val_loss: 0.0518\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4048 - val_loss: 0.0515\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 1.6632 - val_loss: 0.0517\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9558 - val_loss: 0.0515\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4016 - val_loss: 0.0516\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1994 - val_loss: 0.0513\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.1823 - val_loss: 0.0513\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2430 - val_loss: 0.0512\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3505 - val_loss: 0.0513\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5223 - val_loss: 0.0512\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.9177 - val_loss: 0.0515\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1981 - val_loss: 0.0513\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0219 - val_loss: 0.0515\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7982 - val_loss: 0.0514\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.6194 - val_loss: 0.0516\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4035 - val_loss: 0.0515\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3603 - val_loss: 0.0514\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3437 - val_loss: 0.0513\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3729 - val_loss: 0.0513\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5134 - val_loss: 0.0515\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6759 - val_loss: 0.0512\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7064 - val_loss: 0.0519\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7999 - val_loss: 0.0518\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A1494310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_462 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 699ms/step - loss: 1.3883 - val_loss: 0.0520\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.5274 - val_loss: 0.0530\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 23.0024 - val_loss: 0.0522\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.4728 - val_loss: 0.0525\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.5341 - val_loss: 0.0534\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.1151 - val_loss: 0.0536\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.6638 - val_loss: 0.0545\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.2798 - val_loss: 0.0541\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.5521 - val_loss: 0.0546\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0529 - val_loss: 0.0551\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2248 - val_loss: 0.0553\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6727 - val_loss: 0.0573\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6099 - val_loss: 0.0556\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6675 - val_loss: 0.0599\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8326 - val_loss: 0.0562\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8638 - val_loss: 0.0622\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502B24BCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_463 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 802ms/step - loss: 2.3121 - val_loss: 0.0532\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 14.7391 - val_loss: 0.0513\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 23.8661 - val_loss: 0.0523\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.5316 - val_loss: 0.0516\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 14.8712 - val_loss: 0.0537\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 10.5261 - val_loss: 0.0537\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.0966 - val_loss: 0.0542\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 5.3482 - val_loss: 0.0540\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.8163 - val_loss: 0.0557\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.2045 - val_loss: 0.0553\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.4915 - val_loss: 0.0571\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2958 - val_loss: 0.0562\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.0760 - val_loss: 0.0577\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.7314 - val_loss: 0.0573\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5455 - val_loss: 0.0590\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5365 - val_loss: 0.0590\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5974 - val_loss: 0.0597\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250328AE670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_464 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 1.7977 - val_loss: 0.0517\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.0571 - val_loss: 0.0517\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.1189 - val_loss: 0.0512\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.7142 - val_loss: 0.0519\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.8966 - val_loss: 0.0516\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.4856 - val_loss: 0.0518\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.9176 - val_loss: 0.0515\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.5068 - val_loss: 0.0515\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.9060 - val_loss: 0.0517\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.4606 - val_loss: 0.0518\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.1584 - val_loss: 0.0522\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.9797 - val_loss: 0.0518\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6407 - val_loss: 0.0527\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8830 - val_loss: 0.0523\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6317 - val_loss: 0.0526\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6506 - val_loss: 0.0533\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7405 - val_loss: 0.0524\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6756 - val_loss: 0.0538\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F51C9D3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_465 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 482ms/step - loss: 1.3438 - val_loss: 0.0524\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 14.4271 - val_loss: 0.0513\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 20.1893 - val_loss: 0.0516\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 12.5918 - val_loss: 0.0510\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.5940 - val_loss: 0.0515\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.5614 - val_loss: 0.0513\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.2270 - val_loss: 0.0512\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4.4006 - val_loss: 0.0509\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.2705 - val_loss: 0.0511\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.8499 - val_loss: 0.0514\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 1.6582 - val_loss: 0.0511\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3539 - val_loss: 0.0520\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8571 - val_loss: 0.0513\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5482 - val_loss: 0.0519\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3036 - val_loss: 0.0518\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2092 - val_loss: 0.0517\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1468 - val_loss: 0.0522\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1262 - val_loss: 0.0516\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1287 - val_loss: 0.0531\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1630 - val_loss: 0.0517\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2679 - val_loss: 0.0545\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5773 - val_loss: 0.0515\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1361 - val_loss: 0.0566\n",
      "Epoch 00023: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF8B820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_466 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 599ms/step - loss: 2.4291 - val_loss: 0.0516\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.3759 - val_loss: 0.0509\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 23.4186 - val_loss: 0.0510\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.3241 - val_loss: 0.0505\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2433 - val_loss: 0.0509\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.4069 - val_loss: 0.0513\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.8411 - val_loss: 0.0514\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 7.3203 - val_loss: 0.0511\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.4622 - val_loss: 0.0508\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6138 - val_loss: 0.0508\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 1.4976 - val_loss: 0.0507\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8948 - val_loss: 0.0507\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5592 - val_loss: 0.0507\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4292 - val_loss: 0.0508\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.4886 - val_loss: 0.0509\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7137 - val_loss: 0.0508\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0302 - val_loss: 0.0510\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.0935 - val_loss: 0.0509\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0417 - val_loss: 0.0511\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024ECFCF1430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_467 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 397ms/step - loss: 2.0629 - val_loss: 0.0527\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 14.4295 - val_loss: 0.0471\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 16.7740 - val_loss: 0.0479\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 15.3028 - val_loss: 0.0478\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.9212 - val_loss: 0.0478\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.3141 - val_loss: 0.0483\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.1618 - val_loss: 0.0481\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.8799 - val_loss: 0.0489\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.7132 - val_loss: 0.0481\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.7970 - val_loss: 0.0489\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4215 - val_loss: 0.0488\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0872 - val_loss: 0.0496\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9566 - val_loss: 0.0504\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2943 - val_loss: 0.0509\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7553 - val_loss: 0.0519\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4179 - val_loss: 0.0520\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2002 - val_loss: 0.0533\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250CA0F24C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_468 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 534ms/step - loss: 2.3811 - val_loss: 0.0473\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 12.5879 - val_loss: 0.0461\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.9213 - val_loss: 0.0473\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.6187 - val_loss: 0.0482\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 14.8964 - val_loss: 0.0478\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.7927 - val_loss: 0.0482\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 6.2221 - val_loss: 0.0478\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.6644 - val_loss: 0.0480\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.7330 - val_loss: 0.0472\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4053 - val_loss: 0.0482\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.0141 - val_loss: 0.0475\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7593 - val_loss: 0.0492\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4645 - val_loss: 0.0477\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3771 - val_loss: 0.0509\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6475 - val_loss: 0.0485\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6580 - val_loss: 0.0512\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2554 - val_loss: 0.0488\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250BB4E71F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_469 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 601ms/step - loss: 2.5804 - val_loss: 0.0426\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 11.6993 - val_loss: 0.0456\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.7105 - val_loss: 0.0456\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 14.5792 - val_loss: 0.0435\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.6910 - val_loss: 0.0413\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.9523 - val_loss: 0.0432\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.7304 - val_loss: 0.0399\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.2617 - val_loss: 0.0413\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.5657 - val_loss: 0.0394\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.6055 - val_loss: 0.0406\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.4515 - val_loss: 0.0390\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0748 - val_loss: 0.0405\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5118 - val_loss: 0.0393\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3935 - val_loss: 0.0398\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0055 - val_loss: 0.0392\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7011 - val_loss: 0.0393\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4689 - val_loss: 0.0391\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3680 - val_loss: 0.0390\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3046 - val_loss: 0.0390\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3207 - val_loss: 0.0389\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3424 - val_loss: 0.0390\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.3797 - val_loss: 0.0390\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.3917 - val_loss: 0.0391\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3792 - val_loss: 0.0392\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3423 - val_loss: 0.0394\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3533 - val_loss: 0.0402\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4229 - val_loss: 0.0393\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6002 - val_loss: 0.0406\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7770 - val_loss: 0.0392\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8819 - val_loss: 0.0407\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9353 - val_loss: 0.0390\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.8784 - val_loss: 0.0416\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1245 - val_loss: 0.0390\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4125 - val_loss: 0.0418\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0712 - val_loss: 0.0389\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250BC0AF040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_470 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 396ms/step - loss: 2.1272 - val_loss: 0.0436\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 19.9001 - val_loss: 0.0409\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 19.3751 - val_loss: 0.0405\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.6376 - val_loss: 0.0412\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.1006 - val_loss: 0.0401\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.9714 - val_loss: 0.0391\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.0795 - val_loss: 0.0390\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.1283 - val_loss: 0.0378\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.6013 - val_loss: 0.0390\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9702 - val_loss: 0.0373\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.6167 - val_loss: 0.0378\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5183 - val_loss: 0.0376\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.3969 - val_loss: 0.0362\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0219 - val_loss: 0.0361\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3176 - val_loss: 0.0365\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9819 - val_loss: 0.0357\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6002 - val_loss: 0.0365\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.4234 - val_loss: 0.0350\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3412 - val_loss: 0.0363\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2937 - val_loss: 0.0348\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2836 - val_loss: 0.0353\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2868 - val_loss: 0.0351\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3258 - val_loss: 0.0348\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3905 - val_loss: 0.0348\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4794 - val_loss: 0.0349\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6121 - val_loss: 0.0345\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6281 - val_loss: 0.0343\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5831 - val_loss: 0.0344\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4693 - val_loss: 0.0340\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4118 - val_loss: 0.0350\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4118 - val_loss: 0.0343\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4604 - val_loss: 0.0351\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.5546 - val_loss: 0.0344\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7324 - val_loss: 0.0345\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1021 - val_loss: 0.0352\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0282 - val_loss: 0.0344\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4681 - val_loss: 0.0341\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9442 - val_loss: 0.0349\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5780 - val_loss: 0.0338\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2899 - val_loss: 0.0342\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025018CDD670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_471 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 353ms/step - loss: 2.1079 - val_loss: 0.0378\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.0131 - val_loss: 0.0353\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 38.8148 - val_loss: 0.0360\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.6558 - val_loss: 0.0385\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 12.0877 - val_loss: 0.0384\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.7274 - val_loss: 0.0389\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 7.2081 - val_loss: 0.0393\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.2725 - val_loss: 0.0393\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.8598 - val_loss: 0.0397\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9066 - val_loss: 0.0388\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.2779 - val_loss: 0.0402\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.6989 - val_loss: 0.0386\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4373 - val_loss: 0.0394\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3360 - val_loss: 0.0381\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3760 - val_loss: 0.0388\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3882 - val_loss: 0.0381\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3810 - val_loss: 0.0384\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025081884C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_472 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 331ms/step - loss: 1.5317 - val_loss: 0.0275\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 16.4483 - val_loss: 0.0472\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 20.4854 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 6.0833 - val_loss: 0.0391\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0183 - val_loss: 0.0371\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 5.6600 - val_loss: 0.0381\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.3592 - val_loss: 0.0402\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.3472 - val_loss: 0.0391\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.5978 - val_loss: 0.0403\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.0200 - val_loss: 0.0373\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0183 - val_loss: 0.0404\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1255 - val_loss: 0.0381\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.4221 - val_loss: 0.0445\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4701 - val_loss: 0.0394\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8726 - val_loss: 0.0466\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7488 - val_loss: 0.0421\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F2ACC1EE0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_473 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 506ms/step - loss: 1.4196 - val_loss: 0.0285\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.3871 - val_loss: 0.0406\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 11.8167 - val_loss: 0.0321\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.3448 - val_loss: 0.0379\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 7.4786 - val_loss: 0.0373\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 6.0876 - val_loss: 0.0396\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.5097 - val_loss: 0.0391\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.3234 - val_loss: 0.0425\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.5522 - val_loss: 0.0404\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0577 - val_loss: 0.0454\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5148 - val_loss: 0.0446\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1120 - val_loss: 0.0482\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2629 - val_loss: 0.0515\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.553 - 0s 37ms/step - loss: 0.5539 - val_loss: 0.0509\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3522 - val_loss: 0.0580\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.2497 - val_loss: 0.0540\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250792A49D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_474 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 350ms/step - loss: 1.8307 - val_loss: 0.0220\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 21.5416 - val_loss: 0.0423\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 15.0999 - val_loss: 0.0346\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.3256 - val_loss: 0.0397\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.3774 - val_loss: 0.0399\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.1240 - val_loss: 0.0414\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.2273 - val_loss: 0.0435\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3.9408 - val_loss: 0.0412\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.7361 - val_loss: 0.0450\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.5567 - val_loss: 0.0417\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1216 - val_loss: 0.0438\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6023 - val_loss: 0.0424\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1234 - val_loss: 0.0446\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8253 - val_loss: 0.0429\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5948 - val_loss: 0.0445\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4347 - val_loss: 0.0433\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250C7732550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_475 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 600ms/step - loss: 1.7663 - val_loss: 0.0297\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.0760 - val_loss: 0.0362\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 20.1035 - val_loss: 0.0391\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.9147 - val_loss: 0.0340\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.3650 - val_loss: 0.0387\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.5037 - val_loss: 0.0345\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.6295 - val_loss: 0.0356\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 5.8295 - val_loss: 0.0319\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.1773 - val_loss: 0.0334\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.9317 - val_loss: 0.0297\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.9959 - val_loss: 0.0338\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6444 - val_loss: 0.0284\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3438 - val_loss: 0.0333\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8935 - val_loss: 0.0300\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5167 - val_loss: 0.0313\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2934 - val_loss: 0.0303\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2202 - val_loss: 0.0304\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.1871 - val_loss: 0.0313\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1833 - val_loss: 0.0294\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1705 - val_loss: 0.0322\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1768 - val_loss: 0.0289\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1886 - val_loss: 0.0335\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2133 - val_loss: 0.0279\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2455 - val_loss: 0.0353\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3056 - val_loss: 0.0260\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4480 - val_loss: 0.0410\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7963 - val_loss: 0.0219\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2705 - val_loss: 0.0424\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.3437 - val_loss: 0.0206\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2370 - val_loss: 0.0430\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1075 - val_loss: 0.0251\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.2827 - val_loss: 0.0428\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.2998 - val_loss: 0.0270\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1850 - val_loss: 0.0388\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2245 - val_loss: 0.0303\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.5619 - val_loss: 0.0432\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7246 - val_loss: 0.0309\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5183 - val_loss: 0.0362\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2178 - val_loss: 0.0314\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7429 - val_loss: 0.0455\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250CF454D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_476 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 314ms/step - loss: 2.4905 - val_loss: 0.0266\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.9233 - val_loss: 0.0334\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 13.4571 - val_loss: 0.0357\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.3817 - val_loss: 0.0343\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 6.5909 - val_loss: 0.0324\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.2167 - val_loss: 0.0337\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.7904 - val_loss: 0.0339\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.7005 - val_loss: 0.0320\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.3199 - val_loss: 0.0318\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.0304 - val_loss: 0.0304\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.1247 - val_loss: 0.0310\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.0693 - val_loss: 0.0294\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.5477 - val_loss: 0.0299\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7368 - val_loss: 0.0302\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3622 - val_loss: 0.0279\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.1911 - val_loss: 0.0295\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E99CF1E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_477 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 552ms/step - loss: 1.5243 - val_loss: 0.0194\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 10.9326 - val_loss: 0.0549\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 14.0095 - val_loss: 0.0422\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.6815 - val_loss: 0.0419\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.0711 - val_loss: 0.0450\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.1683 - val_loss: 0.0455\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.5708 - val_loss: 0.0450\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9660 - val_loss: 0.0472\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.0554 - val_loss: 0.0487\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.2066 - val_loss: 0.0515\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.5903 - val_loss: 0.0562\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3628 - val_loss: 0.0522\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2828 - val_loss: 0.0626\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0491 - val_loss: 0.0582\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.9731 - val_loss: 0.0702\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6977 - val_loss: 0.0634\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A1494280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_478 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 423ms/step - loss: 2.1527 - val_loss: 0.0609\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.6228 - val_loss: 0.0325\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 16.4716 - val_loss: 0.0338\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.6087 - val_loss: 0.0296\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.9381 - val_loss: 0.0289\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.3702 - val_loss: 0.0309\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.0077 - val_loss: 0.0286\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.1586 - val_loss: 0.0283\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.7945 - val_loss: 0.0275\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2308 - val_loss: 0.0245\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.8223 - val_loss: 0.0231\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8731 - val_loss: 0.0229\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1030 - val_loss: 0.0206\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5532 - val_loss: 0.0200\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2218 - val_loss: 0.0178\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1129 - val_loss: 0.0172\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0815 - val_loss: 0.0154\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.094 - 0s 24ms/step - loss: 0.0947 - val_loss: 0.0147\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1251 - val_loss: 0.0132\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1631 - val_loss: 0.0129\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2080 - val_loss: 0.0115\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2547 - val_loss: 0.0113\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2850 - val_loss: 0.0099\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3381 - val_loss: 0.0102\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4767 - val_loss: 0.0086\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5836 - val_loss: 0.0090\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5151 - val_loss: 0.0085\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4353 - val_loss: 0.0079\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4012 - val_loss: 0.0093\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3952 - val_loss: 0.0084\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4234 - val_loss: 0.0096\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5072 - val_loss: 0.0089\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7066 - val_loss: 0.0082\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9087 - val_loss: 0.0090\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1460 - val_loss: 0.0081\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4053 - val_loss: 0.0080\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6797 - val_loss: 0.0089\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.0001 - val_loss: 0.0115\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.5216 - val_loss: 0.0080\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.7549 - val_loss: 0.0115\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025092689790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_479 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 327ms/step - loss: 2.8379 - val_loss: 0.0773\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 15.5034 - val_loss: 0.0455\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 11.5996 - val_loss: 0.0494\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.1347 - val_loss: 0.0483\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.0417 - val_loss: 0.0492\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.2610 - val_loss: 0.0535\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.8105 - val_loss: 0.0510\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.0526 - val_loss: 0.0516\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.5529 - val_loss: 0.0539\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.3319 - val_loss: 0.0511\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5245 - val_loss: 0.0548\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9362 - val_loss: 0.0508\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0436 - val_loss: 0.0542\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8233 - val_loss: 0.0504\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5861 - val_loss: 0.0541\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3675 - val_loss: 0.0511\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2613 - val_loss: 0.0547\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF82430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_480 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 270ms/step - loss: 1.3056 - val_loss: 0.0543\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.7059 - val_loss: 0.0359\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.1117 - val_loss: 0.0377\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 16.3600 - val_loss: 0.0395\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.7420 - val_loss: 0.0372\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.5546 - val_loss: 0.0330\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.9315 - val_loss: 0.0369\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.8860 - val_loss: 0.0301\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.7063 - val_loss: 0.0324\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6938 - val_loss: 0.0268\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.4791 - val_loss: 0.0284\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.7694 - val_loss: 0.0272\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1613 - val_loss: 0.0274\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.5508 - val_loss: 0.0276\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2419 - val_loss: 0.0256\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.1857 - val_loss: 0.0282\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.2413 - val_loss: 0.0227\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3956 - val_loss: 0.0294\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5276 - val_loss: 0.0206\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5049 - val_loss: 0.0284\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5017 - val_loss: 0.0201\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5034 - val_loss: 0.0277\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6503 - val_loss: 0.0200\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9975 - val_loss: 0.0271\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.2286 - val_loss: 0.0233\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0441 - val_loss: 0.0231\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9698 - val_loss: 0.0253\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8911 - val_loss: 0.0213\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6692 - val_loss: 0.0269\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5626 - val_loss: 0.0189\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4859 - val_loss: 0.0268\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4569 - val_loss: 0.0220\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4811 - val_loss: 0.0242\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5492 - val_loss: 0.0234\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5492 - val_loss: 0.0228\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6079 - val_loss: 0.0273\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5560 - val_loss: 0.0189\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4486 - val_loss: 0.0307\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3515 - val_loss: 0.0163\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3375 - val_loss: 0.0344\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002500517DDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_481 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 354ms/step - loss: 1.5738 - val_loss: 0.0546\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.3862 - val_loss: 0.0322\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 19.3232 - val_loss: 0.0388\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 14.1009 - val_loss: 0.0386\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 9.8367 - val_loss: 0.0425\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1311 - val_loss: 0.0397\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6079 - val_loss: 0.0436\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.6256 - val_loss: 0.0396\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.0988 - val_loss: 0.0426\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.1057 - val_loss: 0.0408\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6501 - val_loss: 0.0444\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0766 - val_loss: 0.0406\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4717 - val_loss: 0.0447\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2354 - val_loss: 0.0420\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1801 - val_loss: 0.0461\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2151 - val_loss: 0.0429\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2979 - val_loss: 0.0473\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250B62E7790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_482 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 1.8315 - val_loss: 0.0208\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.9741 - val_loss: 0.0404\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 24.3606 - val_loss: 0.0303\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.3201 - val_loss: 0.0313\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.6059 - val_loss: 0.0312\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 7.0751 - val_loss: 0.0263\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 6.4507 - val_loss: 0.0254\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.6138 - val_loss: 0.0260\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.2903 - val_loss: 0.0213\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6389 - val_loss: 0.0221\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4612 - val_loss: 0.0195\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7764 - val_loss: 0.0195\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4795 - val_loss: 0.0172\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2656 - val_loss: 0.0165\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1618 - val_loss: 0.0147\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1052 - val_loss: 0.0142\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0858 - val_loss: 0.0126\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0921 - val_loss: 0.0124\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1332 - val_loss: 0.0107\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2344 - val_loss: 0.0111\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4124 - val_loss: 0.0096\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6014 - val_loss: 0.0094\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7058 - val_loss: 0.0103\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6792 - val_loss: 0.0072\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7200 - val_loss: 0.0113\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6494 - val_loss: 0.0068\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6110 - val_loss: 0.0106\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6143 - val_loss: 0.0068\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6813 - val_loss: 0.0084\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6881 - val_loss: 0.0076\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5636 - val_loss: 0.0072\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4839 - val_loss: 0.0072\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.4256 - val_loss: 0.0072\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3807 - val_loss: 0.0072\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3699 - val_loss: 0.0073\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4237 - val_loss: 0.0069\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4783 - val_loss: 0.0073\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5299 - val_loss: 0.0069\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5545 - val_loss: 0.0071\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5676 - val_loss: 0.0073\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FCC068310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_483 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 546ms/step - loss: 2.3704 - val_loss: 0.0371\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.1255 - val_loss: 0.0354\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 22.3490 - val_loss: 0.0370\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.8861 - val_loss: 0.0327\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.4960 - val_loss: 0.0368\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.9747 - val_loss: 0.0383\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.3555 - val_loss: 0.0414\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.2314 - val_loss: 0.0439\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.4454 - val_loss: 0.0464\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.4038 - val_loss: 0.0472\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.3041 - val_loss: 0.0496\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7719 - val_loss: 0.0527\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.5398 - val_loss: 0.0542\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2563 - val_loss: 0.0604\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8234 - val_loss: 0.0563\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7257 - val_loss: 0.0679\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4605 - val_loss: 0.0602\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3311 - val_loss: 0.0756\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3161 - val_loss: 0.0637\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF8B3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_484 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 385ms/step - loss: 2.3527 - val_loss: 0.0191\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 11.7454 - val_loss: 0.0402\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 28.1311 - val_loss: 0.0324\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.4009 - val_loss: 0.0348\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 8.1270 - val_loss: 0.0362\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.6393 - val_loss: 0.0368\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.1279 - val_loss: 0.0393\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.3348 - val_loss: 0.0388\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.3119 - val_loss: 0.0411\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.9933 - val_loss: 0.0397\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1947 - val_loss: 0.0428\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5370 - val_loss: 0.0423\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1990 - val_loss: 0.0456\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8424 - val_loss: 0.0442\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5190 - val_loss: 0.0475\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3171 - val_loss: 0.0456\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250631E9820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_485 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 582ms/step - loss: 1.6040 - val_loss: 0.0148\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 11.4594 - val_loss: 0.0297\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 27.5794 - val_loss: 0.0310\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.2752 - val_loss: 0.0273\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.1808 - val_loss: 0.0353\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.7325 - val_loss: 0.0353\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.8322 - val_loss: 0.0443\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.7466 - val_loss: 0.0411\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 4.1785 - val_loss: 0.0459\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.1065 - val_loss: 0.0446\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4140 - val_loss: 0.0461\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.9403 - val_loss: 0.0427\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.8824 - val_loss: 0.0452\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1.6550 - val_loss: 0.0460\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7191 - val_loss: 0.0492\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4333 - val_loss: 0.0490\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025084435700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_486 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 678ms/step - loss: 1.9179 - val_loss: 0.0238\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 16.5768 - val_loss: 0.0256\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.2341 - val_loss: 0.0224\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.5041 - val_loss: 0.0292\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.7321 - val_loss: 0.0233\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.1557 - val_loss: 0.0277\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.5315 - val_loss: 0.0233\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.1395 - val_loss: 0.0288\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.0130 - val_loss: 0.0245\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.2905 - val_loss: 0.0258\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6585 - val_loss: 0.0265\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3730 - val_loss: 0.0270\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7626 - val_loss: 0.0271\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4527 - val_loss: 0.0255\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4168 - val_loss: 0.0291\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6990 - val_loss: 0.0229\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9381 - val_loss: 0.0301\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9949 - val_loss: 0.0220\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7530 - val_loss: 0.0281\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6287 - val_loss: 0.0224\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4941 - val_loss: 0.0294\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3999 - val_loss: 0.0220\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2939 - val_loss: 0.0278\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2033 - val_loss: 0.0227\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1637 - val_loss: 0.0263\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1505 - val_loss: 0.0211\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1542 - val_loss: 0.0287\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.1717 - val_loss: 0.0180\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2162 - val_loss: 0.0255\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2651 - val_loss: 0.0209\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3646 - val_loss: 0.0178\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5526 - val_loss: 0.0237\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8565 - val_loss: 0.0137\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0577 - val_loss: 0.0207\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9146 - val_loss: 0.0150\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9120 - val_loss: 0.0128\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.1433 - val_loss: 0.0226\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3752 - val_loss: 0.0093\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.5349 - val_loss: 0.0442\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1204 - val_loss: 0.0077\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250259670D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_487 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 283ms/step - loss: 1.5890 - val_loss: 0.0118\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.7252 - val_loss: 0.0314\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 27.5322 - val_loss: 0.0219\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 15.3885 - val_loss: 0.0271\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.8748 - val_loss: 0.0234\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 6.3737 - val_loss: 0.0301\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.6447 - val_loss: 0.0278\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 5.4023 - val_loss: 0.0382\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.2758 - val_loss: 0.0320\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.0128 - val_loss: 0.0411\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.9116 - val_loss: 0.0373\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8363 - val_loss: 0.0443\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.3473 - val_loss: 0.0437\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6092 - val_loss: 0.0511\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4785 - val_loss: 0.0515\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1152 - val_loss: 0.0577\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250DE8D79D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_488 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 1.6662 - val_loss: 0.0289\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.7171 - val_loss: 0.0206\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.5465 - val_loss: 0.0222\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.5025 - val_loss: 0.0298\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 9.5271 - val_loss: 0.0202\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.0996 - val_loss: 0.0255\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.8847 - val_loss: 0.0239\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.4841 - val_loss: 0.0242\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9488 - val_loss: 0.0268\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.5313 - val_loss: 0.0265\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.5689 - val_loss: 0.0286\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.5445 - val_loss: 0.0282\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6540 - val_loss: 0.0276\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1706 - val_loss: 0.0288\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7116 - val_loss: 0.0278\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4335 - val_loss: 0.0276\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3593 - val_loss: 0.0270\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3966 - val_loss: 0.0268\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4036 - val_loss: 0.0259\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3388 - val_loss: 0.0260\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250381DDCA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_489 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 591ms/step - loss: 2.0949 - val_loss: 0.0137\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.0715 - val_loss: 0.0333\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 14.6557 - val_loss: 0.0258\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 13.8669 - val_loss: 0.0223\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 11.3878 - val_loss: 0.0272\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.1296 - val_loss: 0.0282\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.0696 - val_loss: 0.0321\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.3693 - val_loss: 0.0264\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.1730 - val_loss: 0.0325\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.6718 - val_loss: 0.0281\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.2100 - val_loss: 0.0322\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.6845 - val_loss: 0.0309\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2413 - val_loss: 0.0328\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9210 - val_loss: 0.0324\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6984 - val_loss: 0.0317\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6644 - val_loss: 0.0365\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250125DACA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_490 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 435ms/step - loss: 2.0615 - val_loss: 0.0177\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 13.1137 - val_loss: 0.0367\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 18.1015 - val_loss: 0.0258\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.5288 - val_loss: 0.0357\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.7864 - val_loss: 0.0364\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.2773 - val_loss: 0.0392\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.3864 - val_loss: 0.0420\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.7593 - val_loss: 0.0387\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.9204 - val_loss: 0.0439\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.2208 - val_loss: 0.0412\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.1690 - val_loss: 0.0463\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.4193 - val_loss: 0.0427\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7731 - val_loss: 0.0491\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5911 - val_loss: 0.0438\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5494 - val_loss: 0.0512\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5674 - val_loss: 0.0433\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025092689040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_491 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 486ms/step - loss: 1.3709 - val_loss: 0.0115\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 10.0348 - val_loss: 0.0274\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 10.5353 - val_loss: 0.0266\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 8.5842 - val_loss: 0.0215\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.9230 - val_loss: 0.0211\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.2617 - val_loss: 0.0173\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 6.7625 - val_loss: 0.0164\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 7.0777 - val_loss: 0.0183\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6986 - val_loss: 0.0166\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.3920 - val_loss: 0.0156\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.9082 - val_loss: 0.0172\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.3887 - val_loss: 0.0157\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.1365 - val_loss: 0.0146\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3874 - val_loss: 0.0148\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 2.1268 - val_loss: 0.0144\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.5185 - val_loss: 0.0132\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025071645AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_492 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 348ms/step - loss: 1.5443 - val_loss: 0.0168\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.0615 - val_loss: 0.0213\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 23.8096 - val_loss: 0.0311\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.5735 - val_loss: 0.0286\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.8025 - val_loss: 0.0318\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.9765 - val_loss: 0.0314\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 6.4007 - val_loss: 0.0330\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.7300 - val_loss: 0.0355\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.7859 - val_loss: 0.0343\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.5389 - val_loss: 0.0381\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6049 - val_loss: 0.0358\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.2154 - val_loss: 0.0412\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1225 - val_loss: 0.0361\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2751 - val_loss: 0.0466\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 1.9603 - val_loss: 0.0384\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.6340 - val_loss: 0.0499\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025024E533A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_493 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 1.5045 - val_loss: 0.0180\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.0168 - val_loss: 0.0270\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.2896 - val_loss: 0.0288\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 13.2430 - val_loss: 0.0275\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 17.3101 - val_loss: 0.0256\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.3419 - val_loss: 0.0302\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.2355 - val_loss: 0.0323\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.9447 - val_loss: 0.0338\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.7094 - val_loss: 0.0351\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1502 - val_loss: 0.0308\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.7450 - val_loss: 0.0389\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6312 - val_loss: 0.0316\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.3875 - val_loss: 0.0407\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.1197 - val_loss: 0.0343\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.3288 - val_loss: 0.0444\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1373 - val_loss: 0.0367\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250B44E9DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_494 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 275ms/step - loss: 1.7843 - val_loss: 0.0158\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.7375 - val_loss: 0.0247\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 19.3390 - val_loss: 0.0224\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.7036 - val_loss: 0.0242\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 10.9603 - val_loss: 0.0185\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 7.0240 - val_loss: 0.0193\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 5.9994 - val_loss: 0.0176\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.2774 - val_loss: 0.0180\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.6206 - val_loss: 0.0180\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 3.0093 - val_loss: 0.0191\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.2979 - val_loss: 0.0171\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.3137 - val_loss: 0.0179\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7962 - val_loss: 0.0169\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5198 - val_loss: 0.0178\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.3257 - val_loss: 0.0159\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3862 - val_loss: 0.0182\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002508C443D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_495 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 492ms/step - loss: 2.7571 - val_loss: 0.0176\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.0646 - val_loss: 0.0197\n",
      "Epoch 3/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step - loss: 22.3560 - val_loss: 0.0200\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.8137 - val_loss: 0.0191\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.3009 - val_loss: 0.0178\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.1334 - val_loss: 0.0135\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.7785 - val_loss: 0.0155\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.5754 - val_loss: 0.0129\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.2700 - val_loss: 0.0144\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6873 - val_loss: 0.0112\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7309 - val_loss: 0.0136\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8779 - val_loss: 0.0112\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4619 - val_loss: 0.0111\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3016 - val_loss: 0.0108\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3460 - val_loss: 0.0092\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4552 - val_loss: 0.0105\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4392 - val_loss: 0.0081\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4035 - val_loss: 0.0099\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3341 - val_loss: 0.0074\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2850 - val_loss: 0.0094\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2368 - val_loss: 0.0067\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2531 - val_loss: 0.0104\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3882 - val_loss: 0.0066\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7162 - val_loss: 0.0113\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2757 - val_loss: 0.0067\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2589 - val_loss: 0.0078\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9892 - val_loss: 0.0067\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.0367 - val_loss: 0.0064\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1396 - val_loss: 0.0087\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.1612 - val_loss: 0.0071\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0771 - val_loss: 0.0076\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.9942 - val_loss: 0.0065\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9063 - val_loss: 0.0064\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7547 - val_loss: 0.0064\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5924 - val_loss: 0.0068\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4661 - val_loss: 0.0066\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3113 - val_loss: 0.0076\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2053 - val_loss: 0.0069\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1726 - val_loss: 0.0083\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.1538 - val_loss: 0.0067\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025039140DC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_496 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 408ms/step - loss: 1.7099 - val_loss: 0.0122\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1971 - val_loss: 0.0241\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 13.7453 - val_loss: 0.0172\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.8571 - val_loss: 0.0199\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 8.6937 - val_loss: 0.0211\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.8772 - val_loss: 0.0181\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.9716 - val_loss: 0.0216\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.0597 - val_loss: 0.0163\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.0589 - val_loss: 0.0211\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.8013 - val_loss: 0.0179\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.1601 - val_loss: 0.0214\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9868 - val_loss: 0.0193\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.3315 - val_loss: 0.0187\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4136 - val_loss: 0.0221\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2157 - val_loss: 0.0220\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.5557 - val_loss: 0.0249\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503521C160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_497 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 273ms/step - loss: 1.9111 - val_loss: 0.0128\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.3114 - val_loss: 0.0190\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.8638 - val_loss: 0.0116\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 15.5623 - val_loss: 0.0153\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.4944 - val_loss: 0.0119\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.3107 - val_loss: 0.0157\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 7.3393 - val_loss: 0.0120\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.7241 - val_loss: 0.0148\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.4226 - val_loss: 0.0112\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.6988 - val_loss: 0.0127\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8455 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2109 - val_loss: 0.0112\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.8396 - val_loss: 0.0119\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6597 - val_loss: 0.0105\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6238 - val_loss: 0.0116\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5239 - val_loss: 0.0101\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4623 - val_loss: 0.0098\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4294 - val_loss: 0.0103\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.3668 - val_loss: 0.0080\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3425 - val_loss: 0.0111\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3587 - val_loss: 0.0069\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4082 - val_loss: 0.0124\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5523 - val_loss: 0.0069\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7214 - val_loss: 0.0101\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9004 - val_loss: 0.0069\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.9404 - val_loss: 0.0081\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7629 - val_loss: 0.0072\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5764 - val_loss: 0.0072\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5289 - val_loss: 0.0078\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4880 - val_loss: 0.0068\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5212 - val_loss: 0.0088\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5620 - val_loss: 0.0071\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6263 - val_loss: 0.0099\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7560 - val_loss: 0.0076\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9374 - val_loss: 0.0101\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3842 - val_loss: 0.0069\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.4297 - val_loss: 0.0084\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5469 - val_loss: 0.0073\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6122 - val_loss: 0.0070\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4493 - val_loss: 0.0078\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002508C2B9280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_498 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 1.4657 - val_loss: 0.0185\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.0092 - val_loss: 0.0190\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 19.1003 - val_loss: 0.0173\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 10.6690 - val_loss: 0.0204\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.2985 - val_loss: 0.0175\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.7336 - val_loss: 0.0174\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.0515 - val_loss: 0.0179\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.0247 - val_loss: 0.0181\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.9245 - val_loss: 0.0164\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.1664 - val_loss: 0.0201\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.7375 - val_loss: 0.0148\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.8501 - val_loss: 0.0210\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.4272 - val_loss: 0.0169\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8119 - val_loss: 0.0213\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9999 - val_loss: 0.0150\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.5251 - val_loss: 0.0212\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9922 - val_loss: 0.0156\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6287 - val_loss: 0.0219\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3916 - val_loss: 0.0153\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3381 - val_loss: 0.0237\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3829 - val_loss: 0.0139\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4170 - val_loss: 0.0247\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3919 - val_loss: 0.0147\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3243 - val_loss: 0.0230\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2384 - val_loss: 0.0168\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1708 - val_loss: 0.0217\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1487 - val_loss: 0.0181\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1678 - val_loss: 0.0211\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2801 - val_loss: 0.0198\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5403 - val_loss: 0.0206\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9119 - val_loss: 0.0224\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.9860 - val_loss: 0.0273\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8950 - val_loss: 0.0173\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7781 - val_loss: 0.0358\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6509 - val_loss: 0.0174\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7113 - val_loss: 0.0458\n",
      "Epoch 00036: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025063F1F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_499 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 1.5371 - val_loss: 0.0067\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.1221 - val_loss: 0.0210\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 16.9347 - val_loss: 0.0191\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 15.3280 - val_loss: 0.0229\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 12.7490 - val_loss: 0.0244\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0738 - val_loss: 0.0242\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.9995 - val_loss: 0.0273\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.8812 - val_loss: 0.0272\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.8362 - val_loss: 0.0320\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.6338 - val_loss: 0.0316\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.3640 - val_loss: 0.0313\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1750 - val_loss: 0.0395\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.9759 - val_loss: 0.0320\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2937 - val_loss: 0.0425\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7037 - val_loss: 0.0359\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3727 - val_loss: 0.0437\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250D42A2430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_500 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 3s 3s/step - loss: 1.8734 - val_loss: 0.0085\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 11.1404 - val_loss: 0.0200\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 10.2105 - val_loss: 0.0141\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.9195 - val_loss: 0.0164\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.0116 - val_loss: 0.0164\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.6114 - val_loss: 0.0166\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.8829 - val_loss: 0.0154\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4290 - val_loss: 0.0160\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.1790 - val_loss: 0.0143\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.8258 - val_loss: 0.0180\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.4304 - val_loss: 0.0124\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7093 - val_loss: 0.0177\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6125 - val_loss: 0.0126\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2358 - val_loss: 0.0155\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0281 - val_loss: 0.0127\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7094 - val_loss: 0.0141\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250631E9820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_501 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 480ms/step - loss: 1.6837 - val_loss: 0.0072\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 15.2897 - val_loss: 0.0188\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 13.1131 - val_loss: 0.0152\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 12.6151 - val_loss: 0.0154\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.6432 - val_loss: 0.0174\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.8216 - val_loss: 0.0145\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.2732 - val_loss: 0.0166\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3433 - val_loss: 0.0150\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.7146 - val_loss: 0.0155\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.5173 - val_loss: 0.0171\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.1074 - val_loss: 0.0152\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3209 - val_loss: 0.0196\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7789 - val_loss: 0.0160\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5005 - val_loss: 0.0196\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4785 - val_loss: 0.0162\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1449 - val_loss: 0.0175\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC3E800D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_502 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 396ms/step - loss: 2.6325 - val_loss: 0.0125\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.3571 - val_loss: 0.0157\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.1953 - val_loss: 0.0153\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.3300 - val_loss: 0.0215\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 9.6347 - val_loss: 0.0166\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 8.0779 - val_loss: 0.0224\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 9.5847 - val_loss: 0.0196\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 7.2818 - val_loss: 0.0206\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.1507 - val_loss: 0.0257\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.3327 - val_loss: 0.0221\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.6247 - val_loss: 0.0265\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8096 - val_loss: 0.0238\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1425 - val_loss: 0.0308\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6987 - val_loss: 0.0260\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.6171 - val_loss: 0.0332\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6057 - val_loss: 0.0271\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025029B3FD30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_503 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 452ms/step - loss: 1.7764 - val_loss: 0.0074\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.6570 - val_loss: 0.0164\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 10.3780 - val_loss: 0.0157\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 11.6971 - val_loss: 0.0145\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.0771 - val_loss: 0.0131\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.4355 - val_loss: 0.0139\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.9350 - val_loss: 0.0132\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 5.9506 - val_loss: 0.0126\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 5.4531 - val_loss: 0.0126\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.6707 - val_loss: 0.0126\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 4.2833 - val_loss: 0.0099\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 3.6550 - val_loss: 0.0118\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.6882 - val_loss: 0.0094\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0088 - val_loss: 0.0099\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.9049 - val_loss: 0.0102\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5154 - val_loss: 0.0089\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025063104AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_504 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 347ms/step - loss: 1.2471 - val_loss: 0.0116\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.0691 - val_loss: 0.0201\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 16.0924 - val_loss: 0.0172\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.6860 - val_loss: 0.0136\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 8.6339 - val_loss: 0.0172\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 7.6974 - val_loss: 0.0139\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 4.5119 - val_loss: 0.0152\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.9955 - val_loss: 0.0120\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.4287 - val_loss: 0.0132\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.7315 - val_loss: 0.0112\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.0024 - val_loss: 0.0106\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.0946 - val_loss: 0.0100\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.7810 - val_loss: 0.0097\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.3977 - val_loss: 0.0105\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.4919 - val_loss: 0.0082\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.4129 - val_loss: 0.0097\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3314 - val_loss: 0.0079\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.9355 - val_loss: 0.0084\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4943 - val_loss: 0.0078\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.281 - 0s 33ms/step - loss: 0.2816 - val_loss: 0.0073\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1935 - val_loss: 0.0075\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1407 - val_loss: 0.0068\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.1202 - val_loss: 0.0070\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1097 - val_loss: 0.0066\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1091 - val_loss: 0.0064\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1167 - val_loss: 0.0068\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.1458 - val_loss: 0.0062\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2246 - val_loss: 0.0071\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4522 - val_loss: 0.0072\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8242 - val_loss: 0.0096\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.1830 - val_loss: 0.0079\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2685 - val_loss: 0.0084\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.1611 - val_loss: 0.0064\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9018 - val_loss: 0.0074\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.8349 - val_loss: 0.0077\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6511 - val_loss: 0.0071\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6254 - val_loss: 0.0075\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.9103 - val_loss: 0.0065\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2075 - val_loss: 0.0065\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.9324 - val_loss: 0.0062\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250BD522B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_505 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 520ms/step - loss: 1.5402 - val_loss: 0.0122\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.3233 - val_loss: 0.0111\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.3301 - val_loss: 0.0136\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.2082 - val_loss: 0.0112\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 9.5120 - val_loss: 0.0105\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 7.3601 - val_loss: 0.0101\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.8333 - val_loss: 0.0096\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.6817 - val_loss: 0.0097\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.6220 - val_loss: 0.0079\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.4563 - val_loss: 0.0096\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.2391 - val_loss: 0.0065\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.5111 - val_loss: 0.0078\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.2835 - val_loss: 0.0063\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5124 - val_loss: 0.0070\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.2995 - val_loss: 0.0062\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4641 - val_loss: 0.0062\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.9669 - val_loss: 0.0061\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5053 - val_loss: 0.0062\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2905 - val_loss: 0.0062\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2182 - val_loss: 0.0067\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2051 - val_loss: 0.0064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2155 - val_loss: 0.0074\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2251 - val_loss: 0.0069\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2492 - val_loss: 0.0081\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3004 - val_loss: 0.0084\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.3605 - val_loss: 0.0086\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4152 - val_loss: 0.0091\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4098 - val_loss: 0.0110\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3365 - val_loss: 0.0097\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3025 - val_loss: 0.0133\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3116 - val_loss: 0.0110\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3766 - val_loss: 0.0154\n",
      "Epoch 00032: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002509F787550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_506 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 438ms/step - loss: 2.1359 - val_loss: 0.0121\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.4897 - val_loss: 0.0167\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 22.6952 - val_loss: 0.0183\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.0771 - val_loss: 0.0163\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.9554 - val_loss: 0.0165\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.0502 - val_loss: 0.0165\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.5495 - val_loss: 0.0187\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.8178 - val_loss: 0.0202\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.1114 - val_loss: 0.0237\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.0130 - val_loss: 0.0262\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.7961 - val_loss: 0.0264\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1890 - val_loss: 0.0314\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2232 - val_loss: 0.0300\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1867 - val_loss: 0.0331\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.9868 - val_loss: 0.0348\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.3043 - val_loss: 0.0382\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250F9B7F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_507 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 404ms/step - loss: 2.1775 - val_loss: 0.0162\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.6537 - val_loss: 0.0103\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 19.3679 - val_loss: 0.0092\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 13.9274 - val_loss: 0.0115\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.0117 - val_loss: 0.0097\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.8565 - val_loss: 0.0134\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.2054 - val_loss: 0.0079\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.5192 - val_loss: 0.0112\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4.7985 - val_loss: 0.0074\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 4.2696 - val_loss: 0.0099\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.6963 - val_loss: 0.0076\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.2951 - val_loss: 0.0087\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5856 - val_loss: 0.0078\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.7777 - val_loss: 0.0086\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.6397 - val_loss: 0.0076\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.7133 - val_loss: 0.0074\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.8622 - val_loss: 0.0079\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.6943 - val_loss: 0.0065\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9547 - val_loss: 0.0080\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5785 - val_loss: 0.0064\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3534 - val_loss: 0.0080\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.3417 - val_loss: 0.0064\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.4290 - val_loss: 0.0083\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.5750 - val_loss: 0.0067\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6221 - val_loss: 0.0078\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.6512 - val_loss: 0.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.6274 - val_loss: 0.0071\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5344 - val_loss: 0.0081\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5043 - val_loss: 0.0076\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5345 - val_loss: 0.0073\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.6293 - val_loss: 0.0091\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7887 - val_loss: 0.0065\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0705 - val_loss: 0.0097\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2946 - val_loss: 0.0077\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.1016 - val_loss: 0.0111\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502BF82310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_508 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 626ms/step - loss: 1.9616 - val_loss: 0.0139\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 14.9878 - val_loss: 0.0217\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 26.9863 - val_loss: 0.0147\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 9.6876 - val_loss: 0.0124\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.8715 - val_loss: 0.0152\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 8.6810 - val_loss: 0.0119\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 6.2883 - val_loss: 0.0140\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.5234 - val_loss: 0.0124\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 3.0300 - val_loss: 0.0146\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.2565 - val_loss: 0.0142\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2165 - val_loss: 0.0155\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.4988 - val_loss: 0.0157\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.1795 - val_loss: 0.0166\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0687 - val_loss: 0.0170\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0400 - val_loss: 0.0175\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0413 - val_loss: 0.0176\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0640 - val_loss: 0.0182\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1256 - val_loss: 0.0188\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2684 - val_loss: 0.0190\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5102 - val_loss: 0.0202\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7424 - val_loss: 0.0201\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250931FEC10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_509 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 461ms/step - loss: 3.0138 - val_loss: 0.0145\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 9.4722 - val_loss: 0.0160\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 21.8977 - val_loss: 0.0146\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.4039 - val_loss: 0.0151\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.5897 - val_loss: 0.0195\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.4740 - val_loss: 0.0179\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.8502 - val_loss: 0.0238\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.7459 - val_loss: 0.0221\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.7141 - val_loss: 0.0266\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.1363 - val_loss: 0.0224\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.7844 - val_loss: 0.0292\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 2.3049 - val_loss: 0.0274\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.3596 - val_loss: 0.0331\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9903 - val_loss: 0.0324\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6037 - val_loss: 0.0353\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3663 - val_loss: 0.0381\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503521C790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_510 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 456ms/step - loss: 2.4676 - val_loss: 0.0119\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 12.3346 - val_loss: 0.0164\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 21.7535 - val_loss: 0.0156\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 11.8336 - val_loss: 0.0162\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.5110 - val_loss: 0.0165\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.5654 - val_loss: 0.0159\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.5577 - val_loss: 0.0178\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.8154 - val_loss: 0.0158\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 5.7403 - val_loss: 0.0178\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4086 - val_loss: 0.0170\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 4.2962 - val_loss: 0.0159\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4325 - val_loss: 0.0193\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.2150 - val_loss: 0.0166\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9530 - val_loss: 0.0205\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.2233 - val_loss: 0.0166\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8847 - val_loss: 0.0210\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025090B05CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_511 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 533ms/step - loss: 3.0132 - val_loss: 0.0077\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 19.0796 - val_loss: 0.0139\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 10.4381 - val_loss: 0.0131\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 11.6647 - val_loss: 0.0121\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 9.1871 - val_loss: 0.0123\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 8.7469 - val_loss: 0.0118\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 5.4290 - val_loss: 0.0119\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1008 - val_loss: 0.0123\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 4.1178 - val_loss: 0.0112\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.1313 - val_loss: 0.0112\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.2551 - val_loss: 0.0102\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4956 - val_loss: 0.0105\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0773 - val_loss: 0.0100\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6320 - val_loss: 0.0094\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4008 - val_loss: 0.0101\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.3000 - val_loss: 0.0085\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A9CDB1F0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_512 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 405ms/step - loss: 1.2071 - val_loss: 0.0081\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 13.7425 - val_loss: 0.0147\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 12.2967 - val_loss: 0.0140\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.2442 - val_loss: 0.0138\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 12.3952 - val_loss: 0.0151\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5501 - val_loss: 0.0133\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.9187 - val_loss: 0.0130\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.1279 - val_loss: 0.0111\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.7963 - val_loss: 0.0128\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.1465 - val_loss: 0.0098\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.6914 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3913 - val_loss: 0.0101\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.8173 - val_loss: 0.0118\n",
      "Epoch 14/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2875 - val_loss: 0.0108\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0554 - val_loss: 0.0105\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1042 - val_loss: 0.0116\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250E57D5E50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_513 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 401ms/step - loss: 2.1919 - val_loss: 0.0265\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 25.0645 - val_loss: 0.0130\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 12.4591 - val_loss: 0.0143\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 9.1705 - val_loss: 0.0163\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.1700 - val_loss: 0.0150\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.8483 - val_loss: 0.0169\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 4.7646 - val_loss: 0.0168\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 5.7819 - val_loss: 0.0159\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 4.2689 - val_loss: 0.0212\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.0206 - val_loss: 0.0157\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 2.6129 - val_loss: 0.0220\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.9011 - val_loss: 0.0185\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.0303 - val_loss: 0.0215\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6073 - val_loss: 0.0203\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3378 - val_loss: 0.0215\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2433 - val_loss: 0.0229\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1900 - val_loss: 0.0212\n",
      "Epoch 00017: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250FEF8C3A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_514 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 288ms/step - loss: 2.5140 - val_loss: 0.0140\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.0277 - val_loss: 0.0157\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 16.8525 - val_loss: 0.0135\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 6.4292 - val_loss: 0.0148\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.3886 - val_loss: 0.0140\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 6.0818 - val_loss: 0.0141\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.6440 - val_loss: 0.0136\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.7578 - val_loss: 0.0125\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.6448 - val_loss: 0.0122\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 3.9754 - val_loss: 0.0128\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9501 - val_loss: 0.0119\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 2.7324 - val_loss: 0.0123\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.9858 - val_loss: 0.0111\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.4534 - val_loss: 0.0110\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.7204 - val_loss: 0.0108\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3963 - val_loss: 0.0114\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2690 - val_loss: 0.0105\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.2234 - val_loss: 0.0119\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.2365 - val_loss: 0.0102\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.2763 - val_loss: 0.0126\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.3130 - val_loss: 0.0098\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4651 - val_loss: 0.0146\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7756 - val_loss: 0.0090\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9939 - val_loss: 0.0168\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0458 - val_loss: 0.0082\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0323 - val_loss: 0.0212\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.2195 - val_loss: 0.0083\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.0652 - val_loss: 0.0216\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8931 - val_loss: 0.0094\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8253 - val_loss: 0.0208\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7658 - val_loss: 0.0089\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5744 - val_loss: 0.0183\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4090 - val_loss: 0.0111\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2683 - val_loss: 0.0151\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.1994 - val_loss: 0.0137\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2071 - val_loss: 0.0142\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2004 - val_loss: 0.0131\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2410 - val_loss: 0.0149\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3279 - val_loss: 0.0133\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.5015 - val_loss: 0.0160\n",
      "Epoch 00040: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024E88B49D30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_515 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 264ms/step - loss: 1.4154 - val_loss: 0.0113\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 8.9131 - val_loss: 0.0112\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 19.2303 - val_loss: 0.0090\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.0805 - val_loss: 0.0118\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.3907 - val_loss: 0.0095\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.9616 - val_loss: 0.0113\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.0858 - val_loss: 0.0078\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.4302 - val_loss: 0.0104\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.4211 - val_loss: 0.0075\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.4042 - val_loss: 0.0113\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.6362 - val_loss: 0.0086\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1265 - val_loss: 0.0112\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.6167 - val_loss: 0.0094\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1727 - val_loss: 0.0112\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.9311 - val_loss: 0.0104\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.5829 - val_loss: 0.0109\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.5571 - val_loss: 0.0112\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6873 - val_loss: 0.0108\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7295 - val_loss: 0.0114\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6150 - val_loss: 0.0101\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5169 - val_loss: 0.0115\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4256 - val_loss: 0.0106\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.2607 - val_loss: 0.0113\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.1600 - val_loss: 0.0113\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250616D1C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_516 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 470ms/step - loss: 2.1705 - val_loss: 0.0130\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.2949 - val_loss: 0.0129\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 12.0069 - val_loss: 0.0109\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.2314 - val_loss: 0.0098\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 13.4597 - val_loss: 0.0092\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.4430 - val_loss: 0.0098\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.2538 - val_loss: 0.0073\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 7.3310 - val_loss: 0.0086\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 5.7544 - val_loss: 0.0071\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.2683 - val_loss: 0.0076\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 4.8360 - val_loss: 0.0069\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.3932 - val_loss: 0.0072\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 2.9179 - val_loss: 0.0069\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.3890 - val_loss: 0.0060\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.6634 - val_loss: 0.0072\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5758 - val_loss: 0.0056\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.8643 - val_loss: 0.0073\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.3717 - val_loss: 0.0056\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9541 - val_loss: 0.0076\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5820 - val_loss: 0.0057\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3344 - val_loss: 0.0066\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2238 - val_loss: 0.0058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2036 - val_loss: 0.0059\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.2167 - val_loss: 0.0059\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2486 - val_loss: 0.0056\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2951 - val_loss: 0.0062\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3372 - val_loss: 0.0057\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3709 - val_loss: 0.0069\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4104 - val_loss: 0.0065\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4593 - val_loss: 0.0076\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5655 - val_loss: 0.0079\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7583 - val_loss: 0.0075\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.1276 - val_loss: 0.0076\n",
      "Epoch 00033: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F418A9CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_517 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 2.0155 - val_loss: 0.0160\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 6.7658 - val_loss: 0.0100\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 9.9465 - val_loss: 0.0105\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 13.5610 - val_loss: 0.0092\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.7693 - val_loss: 0.0076\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 12.0197 - val_loss: 0.0099\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 7.8914 - val_loss: 0.0071\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.4509 - val_loss: 0.0077\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.0441 - val_loss: 0.0066\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.8486 - val_loss: 0.0067\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.7961 - val_loss: 0.0061\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.4156 - val_loss: 0.0066\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.6004 - val_loss: 0.0059\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.1157 - val_loss: 0.0058\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.0195 - val_loss: 0.0059\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0917 - val_loss: 0.0058\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 170ms/step - loss: 1.3152 - val_loss: 0.0058\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7178 - val_loss: 0.0058\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.4851 - val_loss: 0.0057\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3521 - val_loss: 0.0062\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3287 - val_loss: 0.0057\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2814 - val_loss: 0.0065\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2321 - val_loss: 0.0058\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1930 - val_loss: 0.0071\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1781 - val_loss: 0.0059\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.192 - 0s 19ms/step - loss: 0.1921 - val_loss: 0.0081\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.2400 - val_loss: 0.0060\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.3228 - val_loss: 0.0096\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 150ms/step - loss: 0.4038 - val_loss: 0.0061\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.4304 - val_loss: 0.0116\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.4379 - val_loss: 0.0059\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4781 - val_loss: 0.0147\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5806 - val_loss: 0.0059\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6536 - val_loss: 0.0132\n",
      "Epoch 00034: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250E13E1160> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_518 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 249ms/step - loss: 1.9091 - val_loss: 0.0083\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 11.2142 - val_loss: 0.0123\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 15.0388 - val_loss: 0.0108\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.2713 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.8331 - val_loss: 0.0072\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 9.6869 - val_loss: 0.0061\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.3979 - val_loss: 0.0062\n",
      "Epoch 8/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5744 - val_loss: 0.0057\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.2625 - val_loss: 0.0061\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.2297 - val_loss: 0.0057\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 2.4132 - val_loss: 0.0058\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.9450 - val_loss: 0.0065\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.8630 - val_loss: 0.0058\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.6602 - val_loss: 0.0072\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.2866 - val_loss: 0.0070\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0045 - val_loss: 0.0082\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7190 - val_loss: 0.0091\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4441 - val_loss: 0.0094\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.2810 - val_loss: 0.0119\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1970 - val_loss: 0.0111\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.1633 - val_loss: 0.0157\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2006 - val_loss: 0.0120\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4145 - val_loss: 0.0245\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0584 - val_loss: 0.0107\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.4053 - val_loss: 0.0327\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250BBD828B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_519 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 440ms/step - loss: 1.4270 - val_loss: 0.0212\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 9.0502 - val_loss: 0.0099\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 18.3872 - val_loss: 0.0129\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 13.9191 - val_loss: 0.0117\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.0598 - val_loss: 0.0102\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.9382 - val_loss: 0.0106\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.9377 - val_loss: 0.0101\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 3.9202 - val_loss: 0.0108\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3.6838 - val_loss: 0.0101\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.1230 - val_loss: 0.0112\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.5509 - val_loss: 0.0099\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.0337 - val_loss: 0.0103\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5647 - val_loss: 0.0102\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3006 - val_loss: 0.0103\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.2201 - val_loss: 0.0104\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3074 - val_loss: 0.0106\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7270 - val_loss: 0.0104\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2524 - val_loss: 0.0115\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.3911 - val_loss: 0.0104\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9966 - val_loss: 0.0109\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.8721 - val_loss: 0.0121\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6119 - val_loss: 0.0100\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4544 - val_loss: 0.0110\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4634 - val_loss: 0.0118\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7394 - val_loss: 0.0082\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.0019 - val_loss: 0.0148\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9661 - val_loss: 0.0077\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7699 - val_loss: 0.0137\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.6083 - val_loss: 0.0095\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4832 - val_loss: 0.0119\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.4013 - val_loss: 0.0110\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3770 - val_loss: 0.0128\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4869 - val_loss: 0.0108\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6660 - val_loss: 0.0165\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7056 - val_loss: 0.0082\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6811 - val_loss: 0.0218\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7764 - val_loss: 0.0063\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.7119 - val_loss: 0.0231\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.6575 - val_loss: 0.0070\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.7533 - val_loss: 0.0260\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250F2E26550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_520 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 584ms/step - loss: 2.2796 - val_loss: 0.0118\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.4609 - val_loss: 0.0140\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 21.4322 - val_loss: 0.0084\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.6743 - val_loss: 0.0097\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 9.3715 - val_loss: 0.0084\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 8.0802 - val_loss: 0.0089\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 8.0708 - val_loss: 0.0069\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9400 - val_loss: 0.0085\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.1898 - val_loss: 0.0076\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.9498 - val_loss: 0.0088\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.5659 - val_loss: 0.0069\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.3759 - val_loss: 0.0084\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8970 - val_loss: 0.0067\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2411 - val_loss: 0.0079\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.5348 - val_loss: 0.0067\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.2871 - val_loss: 0.0070\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3695 - val_loss: 0.0065\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.7892 - val_loss: 0.0068\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0566 - val_loss: 0.0060\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0276 - val_loss: 0.0070\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7811 - val_loss: 0.0058\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5999 - val_loss: 0.0064\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 0.4975 - val_loss: 0.0059\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4124 - val_loss: 0.0060\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3031 - val_loss: 0.0061\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3137 - val_loss: 0.0058\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.3913 - val_loss: 0.0070\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.4826 - val_loss: 0.0058\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6175 - val_loss: 0.0075\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.7664 - val_loss: 0.0057\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0583 - val_loss: 0.0060\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.2210 - val_loss: 0.0077\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0263 - val_loss: 0.0057\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4841 - val_loss: 0.0131\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.5275 - val_loss: 0.0058\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8790 - val_loss: 0.0125\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.0312 - val_loss: 0.0058\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1678 - val_loss: 0.0133\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1414 - val_loss: 0.0059\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.7140 - val_loss: 0.0097\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FC12550D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_521 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 467ms/step - loss: 3.2538 - val_loss: 0.0098\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 13.3438 - val_loss: 0.0127\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 15.1352 - val_loss: 0.0107\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 12.8316 - val_loss: 0.0088\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.7202 - val_loss: 0.0094\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.4918 - val_loss: 0.0079\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.9750 - val_loss: 0.0080\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.0389 - val_loss: 0.0084\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.8224 - val_loss: 0.0072\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.9405 - val_loss: 0.0088\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.0279 - val_loss: 0.0077\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.4512 - val_loss: 0.0093\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.7605 - val_loss: 0.0072\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.2206 - val_loss: 0.0092\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0022 - val_loss: 0.0062\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8261 - val_loss: 0.0091\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.6640 - val_loss: 0.0062\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5309 - val_loss: 0.0086\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4441 - val_loss: 0.0060\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3445 - val_loss: 0.0079\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.2880 - val_loss: 0.0061\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2501 - val_loss: 0.0076\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 0.2400 - val_loss: 0.0061\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.2851 - val_loss: 0.0076\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.3944 - val_loss: 0.0059\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.5424 - val_loss: 0.0073\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.6343 - val_loss: 0.0060\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 0.7147 - val_loss: 0.0067\n",
      "Epoch 29/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 66ms/step - loss: 0.8399 - val_loss: 0.0064\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0804 - val_loss: 0.0068\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.9441 - val_loss: 0.0061\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1145 - val_loss: 0.0077\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 1.0869 - val_loss: 0.0057\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 1.1018 - val_loss: 0.0105\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.2377 - val_loss: 0.0059\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.0649 - val_loss: 0.0087\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 1.0285 - val_loss: 0.0063\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 0.9936 - val_loss: 0.0112\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8745 - val_loss: 0.0061\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.7660 - val_loss: 0.0114\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002506FBF8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_522 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 477ms/step - loss: 1.3887 - val_loss: 0.0144\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.1904 - val_loss: 0.0105\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 27.1667 - val_loss: 0.0103\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 10.1193 - val_loss: 0.0106\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.2186 - val_loss: 0.0085\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.7129 - val_loss: 0.0106\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 6.0838 - val_loss: 0.0097\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.5988 - val_loss: 0.0099\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.0178 - val_loss: 0.0100\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.4978 - val_loss: 0.0087\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.4826 - val_loss: 0.0109\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.9323 - val_loss: 0.0080\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.5868 - val_loss: 0.0107\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.9336 - val_loss: 0.0088\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.6344 - val_loss: 0.0095\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4748 - val_loss: 0.0072\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1415 - val_loss: 0.0105\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2078 - val_loss: 0.0066\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8754 - val_loss: 0.0098\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4738 - val_loss: 0.0066\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.2726 - val_loss: 0.0084\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.1605 - val_loss: 0.0068\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.1610 - val_loss: 0.0074\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2308 - val_loss: 0.0068\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3345 - val_loss: 0.0067\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3814 - val_loss: 0.0066\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.3240 - val_loss: 0.0063\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.3085 - val_loss: 0.0060\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3024 - val_loss: 0.0063\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 94ms/step - loss: 0.3191 - val_loss: 0.0058\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3542 - val_loss: 0.0060\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3557 - val_loss: 0.0056\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3979 - val_loss: 0.0059\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5227 - val_loss: 0.0055\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6339 - val_loss: 0.0063\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8047 - val_loss: 0.0058\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7904 - val_loss: 0.0059\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.9444 - val_loss: 0.0060\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.3150 - val_loss: 0.0062\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.4454 - val_loss: 0.0056\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002503916E940> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_523 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 2.3323 - val_loss: 0.0089\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 32ms/step - loss: 12.7974 - val_loss: 0.0113\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 14.0156 - val_loss: 0.0087\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.3918 - val_loss: 0.0109\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 7.9773 - val_loss: 0.0097\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 8.8026 - val_loss: 0.0113\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.4671 - val_loss: 0.0107\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.4726 - val_loss: 0.0109\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.4191 - val_loss: 0.0105\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 3.6400 - val_loss: 0.0104\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2487 - val_loss: 0.0098\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2.7528 - val_loss: 0.0132\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.1362 - val_loss: 0.0092\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.2740 - val_loss: 0.0135\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2822 - val_loss: 0.0105\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7996 - val_loss: 0.0157\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5222 - val_loss: 0.0114\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.3136 - val_loss: 0.0173\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025063104700> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_524 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 376ms/step - loss: 2.8989 - val_loss: 0.0162\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.4258 - val_loss: 0.0095\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 26.9036 - val_loss: 0.0084\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 13.1138 - val_loss: 0.0093\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 8.9911 - val_loss: 0.0073\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 8.8545 - val_loss: 0.0087\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.3029 - val_loss: 0.0071\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.5468 - val_loss: 0.0083\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.3212 - val_loss: 0.0074\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0317 - val_loss: 0.0087\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6938 - val_loss: 0.0076\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6007 - val_loss: 0.0081\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0407 - val_loss: 0.0074\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5172 - val_loss: 0.0076\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2453 - val_loss: 0.0076\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.1485 - val_loss: 0.0073\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.1747 - val_loss: 0.0078\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3168 - val_loss: 0.0066\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5524 - val_loss: 0.0082\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6983 - val_loss: 0.0063\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6186 - val_loss: 0.0079\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6169 - val_loss: 0.0062\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5043 - val_loss: 0.0073\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4544 - val_loss: 0.0061\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.4378 - val_loss: 0.0074\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.4152 - val_loss: 0.0056\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4441 - val_loss: 0.0084\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5803 - val_loss: 0.0055\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.8804 - val_loss: 0.0090\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0658 - val_loss: 0.0056\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0086 - val_loss: 0.0087\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2138 - val_loss: 0.0068\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1636 - val_loss: 0.0064\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.4612 - val_loss: 0.0054\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4941 - val_loss: 0.0060\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.3093 - val_loss: 0.0054\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.5767 - val_loss: 0.0069\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.7979 - val_loss: 0.0054\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3136 - val_loss: 0.0059\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.4089 - val_loss: 0.0068\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250DEF34790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_525 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 404ms/step - loss: 1.9958 - val_loss: 0.0141\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.8588 - val_loss: 0.0091\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 18.8650 - val_loss: 0.0094\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 13.0678 - val_loss: 0.0075\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.8580 - val_loss: 0.0073\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.3873 - val_loss: 0.0064\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.2146 - val_loss: 0.0083\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.9951 - val_loss: 0.0071\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.5056 - val_loss: 0.0089\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 3.2834 - val_loss: 0.0080\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.2384 - val_loss: 0.0091\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.9088 - val_loss: 0.0084\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.2373 - val_loss: 0.0099\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8478 - val_loss: 0.0089\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4956 - val_loss: 0.0107\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.3116 - val_loss: 0.0093\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2979 - val_loss: 0.0119\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4257 - val_loss: 0.0098\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7191 - val_loss: 0.0138\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9331 - val_loss: 0.0108\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.0343 - val_loss: 0.0170\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250EF47E0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_526 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 325ms/step - loss: 1.7704 - val_loss: 0.0132\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 11.4636 - val_loss: 0.0104\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.1682 - val_loss: 0.0083\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.8473 - val_loss: 0.0098\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 11.7003 - val_loss: 0.0090\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.0734 - val_loss: 0.0084\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.0563 - val_loss: 0.0090\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.7347 - val_loss: 0.0080\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 5.5454 - val_loss: 0.0091\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 3.1910 - val_loss: 0.0083\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.3267 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.3785 - val_loss: 0.0077\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.7723 - val_loss: 0.0086\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.7962 - val_loss: 0.0078\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.5898 - val_loss: 0.0096\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6605 - val_loss: 0.0074\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0629 - val_loss: 0.0105\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4593 - val_loss: 0.0069\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1493 - val_loss: 0.0095\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8503 - val_loss: 0.0073\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.5676 - val_loss: 0.0076\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3304 - val_loss: 0.0081\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.2289 - val_loss: 0.0066\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2230 - val_loss: 0.0094\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.3352 - val_loss: 0.0060\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5323 - val_loss: 0.0106\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6621 - val_loss: 0.0060\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.6573 - val_loss: 0.0084\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.6312 - val_loss: 0.0067\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6063 - val_loss: 0.0075\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5932 - val_loss: 0.0077\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.6173 - val_loss: 0.0064\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5884 - val_loss: 0.0090\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.7759 - val_loss: 0.0059\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.8652 - val_loss: 0.0093\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.8700 - val_loss: 0.0065\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9535 - val_loss: 0.0076\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 0.8230 - val_loss: 0.0078\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.7087 - val_loss: 0.0070\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.6516 - val_loss: 0.0097\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002510B6EEAF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_527 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 356ms/step - loss: 1.5112 - val_loss: 0.0120\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 11.8155 - val_loss: 0.0124\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 22.7940 - val_loss: 0.0082\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 13.8257 - val_loss: 0.0090\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 9.0475 - val_loss: 0.0086\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 6.3408 - val_loss: 0.0088\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.2200 - val_loss: 0.0086\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 3.6680 - val_loss: 0.0087\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 2.7247 - val_loss: 0.0081\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.2362 - val_loss: 0.0088\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.5091 - val_loss: 0.0079\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1.3520 - val_loss: 0.0096\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9564 - val_loss: 0.0076\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6710 - val_loss: 0.0091\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.6112 - val_loss: 0.0074\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7319 - val_loss: 0.0092\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.8607 - val_loss: 0.0073\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.7845 - val_loss: 0.0098\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6683 - val_loss: 0.0065\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.5399 - val_loss: 0.0106\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4853 - val_loss: 0.0064\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.4386 - val_loss: 0.0110\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4024 - val_loss: 0.0063\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4116 - val_loss: 0.0116\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4148 - val_loss: 0.0060\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4121 - val_loss: 0.0109\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4120 - val_loss: 0.0060\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.4116 - val_loss: 0.0090\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.4396 - val_loss: 0.0070\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.503 - 0s 26ms/step - loss: 0.5035 - val_loss: 0.0063\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7031 - val_loss: 0.0104\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.9602 - val_loss: 0.0053\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.1528 - val_loss: 0.0124\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0068 - val_loss: 0.0053\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2415 - val_loss: 0.0127\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.3115 - val_loss: 0.0062\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.1191 - val_loss: 0.0074\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0110 - val_loss: 0.0097\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.9934 - val_loss: 0.0054\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9873 - val_loss: 0.0108\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250F9B7F820> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_528 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 369ms/step - loss: 1.1713 - val_loss: 0.0146\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 8.1129 - val_loss: 0.0095\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 21.4176 - val_loss: 0.0093\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.0289 - val_loss: 0.0099\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.8229 - val_loss: 0.0079\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 8.0285 - val_loss: 0.0085\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.2370 - val_loss: 0.0064\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 7.4000 - val_loss: 0.0089\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.6016 - val_loss: 0.0063\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.0409 - val_loss: 0.0078\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 2.0667 - val_loss: 0.0073\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.7470 - val_loss: 0.0091\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.2733 - val_loss: 0.0087\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.8367 - val_loss: 0.0083\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.740 - 0s 21ms/step - loss: 0.7405 - val_loss: 0.0109\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7509 - val_loss: 0.0086\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7169 - val_loss: 0.0119\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.7558 - val_loss: 0.0102\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.8005 - val_loss: 0.0125\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.5634 - val_loss: 0.0112\n",
      "Epoch 21/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5005 - val_loss: 0.0134\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 0.3989 - val_loss: 0.0124\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2883 - val_loss: 0.0136\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2544 - val_loss: 0.0134\n",
      "Epoch 00024: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250025388B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_529 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 282ms/step - loss: 1.6212 - val_loss: 0.0161\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 10.4029 - val_loss: 0.0075\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 13.1817 - val_loss: 0.0076\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 13.8603 - val_loss: 0.0063\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.4600 - val_loss: 0.0057\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.2416 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 6.2693 - val_loss: 0.0057\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.4841 - val_loss: 0.0082\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.4337 - val_loss: 0.0069\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.8955 - val_loss: 0.0071\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6220 - val_loss: 0.0081\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.2349 - val_loss: 0.0064\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.6469 - val_loss: 0.0073\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.2918 - val_loss: 0.0066\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.0202 - val_loss: 0.0062\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.5699 - val_loss: 0.0069\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.2676 - val_loss: 0.0059\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.1288 - val_loss: 0.0066\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0815 - val_loss: 0.0058\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.0751 - val_loss: 0.0067\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0934 - val_loss: 0.0055\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.1407 - val_loss: 0.0072\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.2302 - val_loss: 0.0053\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.3759 - val_loss: 0.0082\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.5461 - val_loss: 0.0056\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.6552 - val_loss: 0.0076\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.5999 - val_loss: 0.0055\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.4960 - val_loss: 0.0065\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5572 - val_loss: 0.0057\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7264 - val_loss: 0.0056\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0708 - val_loss: 0.0054\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2718 - val_loss: 0.0054\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.1871 - val_loss: 0.0060\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.1727 - val_loss: 0.0063\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.1763 - val_loss: 0.0075\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.5045 - val_loss: 0.0085\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.4420 - val_loss: 0.0085\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 1.0792 - val_loss: 0.0122\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025038DB1B80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_530 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 355ms/step - loss: 2.1203 - val_loss: 0.0110\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 11.0881 - val_loss: 0.0076\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 14.7348 - val_loss: 0.0070\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.9139 - val_loss: 0.0062\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 9.5684 - val_loss: 0.0069\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 6.0118 - val_loss: 0.0065\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5567 - val_loss: 0.0070\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.3292 - val_loss: 0.0066\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4115 - val_loss: 0.0066\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.7915 - val_loss: 0.0073\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.0535 - val_loss: 0.0067\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.1846 - val_loss: 0.0083\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8295 - val_loss: 0.0060\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.4307 - val_loss: 0.0095\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.8471 - val_loss: 0.0066\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.4744 - val_loss: 0.0090\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.3144 - val_loss: 0.0073\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.2077 - val_loss: 0.0093\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1501 - val_loss: 0.0078\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.1395 - val_loss: 0.0101\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.1729 - val_loss: 0.0075\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.2829 - val_loss: 0.0120\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4440 - val_loss: 0.0067\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 0.6333 - val_loss: 0.0144\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7659 - val_loss: 0.0063\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8002 - val_loss: 0.0162\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6939 - val_loss: 0.0070\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5553 - val_loss: 0.0138\n",
      "Epoch 00028: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250A1475550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_531 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 336ms/step - loss: 1.4555 - val_loss: 0.0077\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.6397 - val_loss: 0.0096\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 14.0650 - val_loss: 0.0064\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.2696 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 7.8966 - val_loss: 0.0070\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.2271 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 8.9305 - val_loss: 0.0077\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 4.6822 - val_loss: 0.0074\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.9931 - val_loss: 0.0093\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 2.6003 - val_loss: 0.0076\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.6156 - val_loss: 0.0084\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3080 - val_loss: 0.0077\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7501 - val_loss: 0.0076\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7072 - val_loss: 0.0079\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.6889 - val_loss: 0.0067\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.2050 - val_loss: 0.0099\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7550 - val_loss: 0.0067\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.4570 - val_loss: 0.0110\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002505FC7EB80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_532 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 278ms/step - loss: 2.2494 - val_loss: 0.0147\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 11.5317 - val_loss: 0.0067\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 22.9873 - val_loss: 0.0066\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.1139 - val_loss: 0.0078\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 8.7810 - val_loss: 0.0067\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 5.8731 - val_loss: 0.0081\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 4.8460 - val_loss: 0.0065\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4.4979 - val_loss: 0.0080\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.7448 - val_loss: 0.0065\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.5139 - val_loss: 0.0080\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8076 - val_loss: 0.0072\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 2.1001 - val_loss: 0.0080\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 1.2977 - val_loss: 0.0072\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.9776 - val_loss: 0.0082\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.5024 - val_loss: 0.0079\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.2703 - val_loss: 0.0085\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.2182 - val_loss: 0.0083\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.2544 - val_loss: 0.0091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.3687 - val_loss: 0.0077\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.4719 - val_loss: 0.0102\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.5413 - val_loss: 0.0069\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6084 - val_loss: 0.0119\n",
      "Epoch 00022: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002510C1F4550> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_533 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 322ms/step - loss: 1.8530 - val_loss: 0.0114\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 16.9598 - val_loss: 0.0120\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 12.7344 - val_loss: 0.0070\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 9.7501 - val_loss: 0.0058\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 9.3207 - val_loss: 0.0061\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.1477 - val_loss: 0.0054\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 3.8117 - val_loss: 0.0063\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 6.2325 - val_loss: 0.0059\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 4.3937 - val_loss: 0.0069\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.1385 - val_loss: 0.0071\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 2.4403 - val_loss: 0.0071\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 1.8337 - val_loss: 0.0087\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 1.2366 - val_loss: 0.0076\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.0105 - val_loss: 0.0095\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 0.9717 - val_loss: 0.0077\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.7907 - val_loss: 0.0101\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.5122 - val_loss: 0.0081\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.3118 - val_loss: 0.0105\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 0.2036 - val_loss: 0.0085\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 0.1429 - val_loss: 0.0107\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.1235 - val_loss: 0.0088\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FA34B5AF0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_534 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 341ms/step - loss: 2.4027 - val_loss: 0.0229\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 13.6504 - val_loss: 0.0062\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 15.9300 - val_loss: 0.0071\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 10.1763 - val_loss: 0.0057\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 8.9767 - val_loss: 0.0066\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 9.4440 - val_loss: 0.0074\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.0717 - val_loss: 0.0067\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 5.9665 - val_loss: 0.0073\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 5.2085 - val_loss: 0.0080\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4.3941 - val_loss: 0.0089\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 3.1290 - val_loss: 0.0083\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2.4801 - val_loss: 0.0096\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.5415 - val_loss: 0.0088\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 0.9911 - val_loss: 0.0099\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.4746 - val_loss: 0.0088\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 0.3011 - val_loss: 0.0114\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4554 - val_loss: 0.0080\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0811 - val_loss: 0.0132\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1.7429 - val_loss: 0.0080\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502B731670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_535 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 428ms/step - loss: 2.3105 - val_loss: 0.0110\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18.8642 - val_loss: 0.0085\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 20.3841 - val_loss: 0.0074\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.0657 - val_loss: 0.0103\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.2445 - val_loss: 0.0086\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.8590 - val_loss: 0.0103\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.6898 - val_loss: 0.0085\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.7549 - val_loss: 0.0106\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.9709 - val_loss: 0.0095\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.6091 - val_loss: 0.0105\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.1167 - val_loss: 0.0111\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.2839 - val_loss: 0.0094\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.9706 - val_loss: 0.0121\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 1.4600 - val_loss: 0.0090\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 1.1711 - val_loss: 0.0141\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8643 - val_loss: 0.0086\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4864 - val_loss: 0.0144\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3187 - val_loss: 0.0094\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FAFB8D790> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_536 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 261ms/step - loss: 1.8053 - val_loss: 0.0215\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 12.3605 - val_loss: 0.0064\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 17.7797 - val_loss: 0.0060\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.9611 - val_loss: 0.0065\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 11.2812 - val_loss: 0.0060\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.3079 - val_loss: 0.0065\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 5.3959 - val_loss: 0.0080\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.4271 - val_loss: 0.0063\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.4818 - val_loss: 0.0085\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9285 - val_loss: 0.0072\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.7896 - val_loss: 0.0084\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.3133 - val_loss: 0.0082\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1118 - val_loss: 0.0083\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6460 - val_loss: 0.0097\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4462 - val_loss: 0.0096\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3927 - val_loss: 0.0123\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4184 - val_loss: 0.0107\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5323 - val_loss: 0.0159\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025063104CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_537 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 377ms/step - loss: 1.2493 - val_loss: 0.0081\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 7.9277 - val_loss: 0.0062\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 22.1876 - val_loss: 0.0061\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 7.9870 - val_loss: 0.0052\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.7107 - val_loss: 0.0053\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.0768 - val_loss: 0.0049\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.2153 - val_loss: 0.0050\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 5.9105 - val_loss: 0.0046\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.5338 - val_loss: 0.0046\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 3.5772 - val_loss: 0.0046\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.3317 - val_loss: 0.0045\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5170 - val_loss: 0.0046\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 1.1894 - val_loss: 0.0046\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7013 - val_loss: 0.0045\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.3801 - val_loss: 0.0046\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3155 - val_loss: 0.0047\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3594 - val_loss: 0.0045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4330 - val_loss: 0.0052\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5138 - val_loss: 0.0045\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4596 - val_loss: 0.0056\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.4052 - val_loss: 0.0045\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 0.3939 - val_loss: 0.0059\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3865 - val_loss: 0.0045\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4253 - val_loss: 0.0062\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 0.5434 - val_loss: 0.0046\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6711 - val_loss: 0.0060\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.6779 - val_loss: 0.0049\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6558 - val_loss: 0.0052\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6224 - val_loss: 0.0059\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.6454 - val_loss: 0.0047\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.7661 - val_loss: 0.0077\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.7802 - val_loss: 0.0054\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8005 - val_loss: 0.0138\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.8719 - val_loss: 0.0062\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8881 - val_loss: 0.0169\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0904 - val_loss: 0.0066\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8954 - val_loss: 0.0107\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.9534 - val_loss: 0.0049\n",
      "Epoch 00038: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025028011040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_538 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 469ms/step - loss: 2.6151 - val_loss: 0.0054\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 20.2780 - val_loss: 0.0069\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 16.8308 - val_loss: 0.0062\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 7.9523 - val_loss: 0.0055\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 9.8183 - val_loss: 0.0067\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.7067 - val_loss: 0.0063\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.2604 - val_loss: 0.0055\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.8833 - val_loss: 0.0062\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.4972 - val_loss: 0.0053\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0749 - val_loss: 0.0059\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.3428 - val_loss: 0.0051\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.2832 - val_loss: 0.0057\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.7177 - val_loss: 0.0061\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1057 - val_loss: 0.0056\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5880 - val_loss: 0.0068\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3006 - val_loss: 0.0059\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1725 - val_loss: 0.0075\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.1451 - val_loss: 0.0058\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.1775 - val_loss: 0.0086\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2617 - val_loss: 0.0054\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.3877 - val_loss: 0.0108\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.5218 - val_loss: 0.0050\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.5613 - val_loss: 0.0125\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.5004 - val_loss: 0.0050\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3840 - val_loss: 0.0119\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.3089 - val_loss: 0.0052\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2411 - val_loss: 0.0116\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.2114 - val_loss: 0.0055\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2388 - val_loss: 0.0118\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3262 - val_loss: 0.0060\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5164 - val_loss: 0.0105\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7235 - val_loss: 0.0080\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.0276 - val_loss: 0.0060\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.7432 - val_loss: 0.0158\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 2.1732 - val_loss: 0.0048\n",
      "Epoch 36/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.7657 - val_loss: 0.0226\n",
      "Epoch 37/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 3.0248 - val_loss: 0.0085\n",
      "Epoch 38/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 2.0966 - val_loss: 0.0253\n",
      "Epoch 39/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.9575 - val_loss: 0.0058\n",
      "Epoch 40/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.7181 - val_loss: 0.0098\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002510E5B68B0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_539 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 513ms/step - loss: 3.4244 - val_loss: 0.0141\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 21.8361 - val_loss: 0.0051\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 13.1616 - val_loss: 0.0055\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 10.6379 - val_loss: 0.0051\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.3260 - val_loss: 0.0049\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 6.7443 - val_loss: 0.0048\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 7.3895 - val_loss: 0.0059\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.2437 - val_loss: 0.0055\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3.8427 - val_loss: 0.0066\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.6916 - val_loss: 0.0064\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2.6990 - val_loss: 0.0075\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 2.1460 - val_loss: 0.0075\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 1.1599 - val_loss: 0.0083\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.8432 - val_loss: 0.0085\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6778 - val_loss: 0.0090\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 0.5456 - val_loss: 0.0102\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.4608 - val_loss: 0.0093\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 0.4716 - val_loss: 0.0124\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.4257 - val_loss: 0.0096\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3632 - val_loss: 0.0146\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3036 - val_loss: 0.0100\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002511DF16430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_540 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 339ms/step - loss: 1.2886 - val_loss: 0.0081\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 8.5226 - val_loss: 0.0071\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 19.0293 - val_loss: 0.0088\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 11.4853 - val_loss: 0.0071\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 10.2810 - val_loss: 0.0081\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 6.4636 - val_loss: 0.0079\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.8001 - val_loss: 0.0089\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 4.0062 - val_loss: 0.0088\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 3.4802 - val_loss: 0.0123\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3266 - val_loss: 0.0107\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.9989 - val_loss: 0.0157\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9581 - val_loss: 0.0106\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7584 - val_loss: 0.0206\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6030 - val_loss: 0.0120\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1945 - val_loss: 0.0203\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.8137 - val_loss: 0.0153\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.4822 - val_loss: 0.0204\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.2659 - val_loss: 0.0185\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.2141 - val_loss: 0.0206\n",
      "Epoch 00019: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024FEBE5B0D0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_541 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 318ms/step - loss: 1.8989 - val_loss: 0.0070\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.4724 - val_loss: 0.0058\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 17.9838 - val_loss: 0.0066\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.9267 - val_loss: 0.0060\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 6.1254 - val_loss: 0.0070\n",
      "Epoch 6/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 22ms/step - loss: 7.3611 - val_loss: 0.0058\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.8414 - val_loss: 0.0064\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.5779 - val_loss: 0.0058\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.5133 - val_loss: 0.0064\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 3.1795 - val_loss: 0.0057\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.8695 - val_loss: 0.0065\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 1.3888 - val_loss: 0.0059\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7384 - val_loss: 0.0060\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.7101 - val_loss: 0.0060\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.8460 - val_loss: 0.0061\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7871 - val_loss: 0.0059\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.6406 - val_loss: 0.0067\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.5056 - val_loss: 0.0059\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4061 - val_loss: 0.0072\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3379 - val_loss: 0.0059\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3567 - val_loss: 0.0082\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4068 - val_loss: 0.0061\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.4827 - val_loss: 0.0093\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5158 - val_loss: 0.0059\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5198 - val_loss: 0.0109\n",
      "Epoch 00025: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000024F336B2310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_542 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 320ms/step - loss: 2.0286 - val_loss: 0.0092\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.9595 - val_loss: 0.0116\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 18.4499 - val_loss: 0.0067\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.6105 - val_loss: 0.0060\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 11.3579 - val_loss: 0.0065\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 7.6734 - val_loss: 0.0055\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.0924 - val_loss: 0.0060\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 5.1498 - val_loss: 0.0052\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.0341 - val_loss: 0.0053\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.0318 - val_loss: 0.0051\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9698 - val_loss: 0.0054\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0229 - val_loss: 0.0053\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.9373 - val_loss: 0.0053\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 1.6301 - val_loss: 0.0055\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.1602 - val_loss: 0.0051\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6827 - val_loss: 0.0053\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.3945 - val_loss: 0.0051\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.1940 - val_loss: 0.0052\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0981 - val_loss: 0.0051\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0600 - val_loss: 0.0052\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.0543 - val_loss: 0.0051\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.0782 - val_loss: 0.0053\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.1423 - val_loss: 0.0051\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.3108 - val_loss: 0.0063\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7218 - val_loss: 0.0055\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.1356 - val_loss: 0.0084\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2760 - val_loss: 0.0055\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.9570 - val_loss: 0.0069\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6228 - val_loss: 0.0051\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.4431 - val_loss: 0.0064\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3210 - val_loss: 0.0057\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.2465 - val_loss: 0.0054\n",
      "Epoch 00032: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250B9E525E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_543 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 297ms/step - loss: 2.5186 - val_loss: 0.0087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 10.9805 - val_loss: 0.0095\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 19.1723 - val_loss: 0.0061\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 8.5442 - val_loss: 0.0062\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 10.2562 - val_loss: 0.0061\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.1978 - val_loss: 0.0064\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.7186 - val_loss: 0.0061\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.2817 - val_loss: 0.0066\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7536 - val_loss: 0.0058\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 2.7542 - val_loss: 0.0060\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 2.1389 - val_loss: 0.0057\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5197 - val_loss: 0.0060\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8361 - val_loss: 0.0057\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.4095 - val_loss: 0.0058\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2809 - val_loss: 0.0059\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.2481 - val_loss: 0.0057\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.2855 - val_loss: 0.0062\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4419 - val_loss: 0.0055\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6618 - val_loss: 0.0073\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.7678 - val_loss: 0.0055\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.7816 - val_loss: 0.0079\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6556 - val_loss: 0.0056\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.5788 - val_loss: 0.0072\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.5951 - val_loss: 0.0057\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.6801 - val_loss: 0.0064\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.7561 - val_loss: 0.0063\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.7747 - val_loss: 0.0060\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7170 - val_loss: 0.0065\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.8049 - val_loss: 0.0056\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.7257 - val_loss: 0.0077\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.9653 - val_loss: 0.0057\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1393 - val_loss: 0.0107\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0924 - val_loss: 0.0063\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8165 - val_loss: 0.0101\n",
      "Epoch 35/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6206 - val_loss: 0.0056\n",
      "Epoch 00035: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025024E53C10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_544 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 415ms/step - loss: 2.3598 - val_loss: 0.0104\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 10.3694 - val_loss: 0.0064\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 26.2767 - val_loss: 0.0062\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.1003 - val_loss: 0.0066\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 10.0401 - val_loss: 0.0056\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 6.5344 - val_loss: 0.0062\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 4.7447 - val_loss: 0.0056\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 5.5270 - val_loss: 0.0059\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 5.1310 - val_loss: 0.0056\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.4701 - val_loss: 0.0059\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.3947 - val_loss: 0.0056\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 2.1444 - val_loss: 0.0060\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6693 - val_loss: 0.0057\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3132 - val_loss: 0.0058\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.8815 - val_loss: 0.0057\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6913 - val_loss: 0.0057\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6811 - val_loss: 0.0057\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6051 - val_loss: 0.0056\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5208 - val_loss: 0.0056\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.5232 - val_loss: 0.0057\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5855 - val_loss: 0.0057\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.5743 - val_loss: 0.0059\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5816 - val_loss: 0.0057\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.6417 - val_loss: 0.0058\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.7199 - val_loss: 0.0056\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7795 - val_loss: 0.0056\n",
      "Epoch 27/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6555 - val_loss: 0.0057\n",
      "Epoch 28/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6015 - val_loss: 0.0057\n",
      "Epoch 29/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5330 - val_loss: 0.0056\n",
      "Epoch 30/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.4646 - val_loss: 0.0058\n",
      "Epoch 31/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.5185 - val_loss: 0.0056\n",
      "Epoch 32/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5396 - val_loss: 0.0058\n",
      "Epoch 33/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.5634 - val_loss: 0.0057\n",
      "Epoch 34/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6865 - val_loss: 0.0056\n",
      "Epoch 00034: early stopping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250183B8280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_545 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 290ms/step - loss: 2.5012 - val_loss: 0.0071\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 9.7153 - val_loss: 0.0066\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 19.0952 - val_loss: 0.0060\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 12.8797 - val_loss: 0.0061\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 11.1820 - val_loss: 0.0058\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.3699 - val_loss: 0.0056\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 4.8659 - val_loss: 0.0059\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.3291 - val_loss: 0.0056\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.2448 - val_loss: 0.0056\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 3.0113 - val_loss: 0.0060\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.0780 - val_loss: 0.0069\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.9222 - val_loss: 0.0063\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.0372 - val_loss: 0.0082\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3600 - val_loss: 0.0075\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.0842 - val_loss: 0.0090\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.0366 - val_loss: 0.0087\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.8517 - val_loss: 0.0115\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.7448 - val_loss: 0.0097\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6433 - val_loss: 0.0139\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 0.6261 - val_loss: 0.0112\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 0.5558 - val_loss: 0.0154\n",
      "Epoch 00021: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250FEF30CA0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_546 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 313ms/step - loss: 2.9220 - val_loss: 0.0281\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 24.4087 - val_loss: 0.0068\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 7.4248 - val_loss: 0.0077\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 10.5013 - val_loss: 0.0071\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.2481 - val_loss: 0.0070\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.4790 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 6.0695 - val_loss: 0.0062\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.5248 - val_loss: 0.0065\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 3.0399 - val_loss: 0.0061\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.5203 - val_loss: 0.0062\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4756 - val_loss: 0.0061\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6388 - val_loss: 0.0073\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5702 - val_loss: 0.0062\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.8445 - val_loss: 0.0079\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.7060 - val_loss: 0.0064\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.9001 - val_loss: 0.0091\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.7706 - val_loss: 0.0067\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6253 - val_loss: 0.0088\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.4859 - val_loss: 0.0073\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.4206 - val_loss: 0.0092\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.4127 - val_loss: 0.0082\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4654 - val_loss: 0.0094\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.5482 - val_loss: 0.0089\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.6028 - val_loss: 0.0096\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.6020 - val_loss: 0.0104\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.5307 - val_loss: 0.0085\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250C19AE5E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_547 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 310ms/step - loss: 1.8374 - val_loss: 0.0113\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.8266 - val_loss: 0.0061\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 14.7918 - val_loss: 0.0061\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 10.2256 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 7.9270 - val_loss: 0.0057\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.1009 - val_loss: 0.0069\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.3483 - val_loss: 0.0061\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.6886 - val_loss: 0.0065\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 3.9209 - val_loss: 0.0065\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8699 - val_loss: 0.0071\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.0284 - val_loss: 0.0072\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2909 - val_loss: 0.0076\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.9619 - val_loss: 0.0083\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - ETA: 0s - loss: 0.662 - 0s 26ms/step - loss: 0.6627 - val_loss: 0.0075\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.4495 - val_loss: 0.0091\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.3886 - val_loss: 0.0075\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3592 - val_loss: 0.0092\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 0.4005 - val_loss: 0.0077\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.4685 - val_loss: 0.0080\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.5784 - val_loss: 0.0089\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250025384C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_548 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 498ms/step - loss: 1.8640 - val_loss: 0.0059\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 16.2021 - val_loss: 0.0085\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 19.8756 - val_loss: 0.0068\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 9.0524 - val_loss: 0.0067\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 10.9438 - val_loss: 0.0072\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 10.1808 - val_loss: 0.0065\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 5.5042 - val_loss: 0.0067\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.4165 - val_loss: 0.0063\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.3712 - val_loss: 0.0066\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5375 - val_loss: 0.0066\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.0483 - val_loss: 0.0065\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.7918 - val_loss: 0.0068\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.6857 - val_loss: 0.0065\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.6757 - val_loss: 0.0065\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 78ms/step - loss: 0.7492 - val_loss: 0.0071\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.8340 - val_loss: 0.0060\n",
      "Epoch 00016: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000002502307F040> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_549 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 1s 518ms/step - loss: 2.3673 - val_loss: 0.0122\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 36ms/step - loss: 16.7812 - val_loss: 0.0082\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 18.7637 - val_loss: 0.0092\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.9398 - val_loss: 0.0078\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.0706 - val_loss: 0.0075\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 6.2816 - val_loss: 0.0077\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 6.2097 - val_loss: 0.0068\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 4.7961 - val_loss: 0.0074\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 4.0055 - val_loss: 0.0068\n",
      "Epoch 10/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step - loss: 2.8453 - val_loss: 0.0072\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.4226 - val_loss: 0.0067\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6369 - val_loss: 0.0069\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.2368 - val_loss: 0.0073\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.1728 - val_loss: 0.0070\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.1035 - val_loss: 0.0080\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.8323 - val_loss: 0.0076\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.4810 - val_loss: 0.0085\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 0.3280 - val_loss: 0.0080\n",
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 0.2794 - val_loss: 0.0090\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2459 - val_loss: 0.0082\n",
      "Epoch 21/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.2495 - val_loss: 0.0101\n",
      "Epoch 22/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.2727 - val_loss: 0.0081\n",
      "Epoch 23/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 0.3201 - val_loss: 0.0128\n",
      "Epoch 24/40\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 0.4566 - val_loss: 0.0070\n",
      "Epoch 25/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.6777 - val_loss: 0.0180\n",
      "Epoch 26/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.8684 - val_loss: 0.0067\n",
      "Epoch 00026: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000025039140430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_550 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 386ms/step - loss: 2.2842 - val_loss: 0.0077\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 8.1666 - val_loss: 0.0106\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 15.0239 - val_loss: 0.0075\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 10.2708 - val_loss: 0.0085\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.4325 - val_loss: 0.0108\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.8693 - val_loss: 0.0096\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 6.1139 - val_loss: 0.0140\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 5.3577 - val_loss: 0.0120\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 4.7117 - val_loss: 0.0167\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.3169 - val_loss: 0.0134\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 2.1596 - val_loss: 0.0214\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.5122 - val_loss: 0.0181\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 0.7956 - val_loss: 0.0232\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.6695 - val_loss: 0.0228\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 0.9129 - val_loss: 0.0269\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.0620 - val_loss: 0.0277\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.8918 - val_loss: 0.0339\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.7899 - val_loss: 0.0309\n",
      "Epoch 00018: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250245ACDC0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Epoch 1/40\n",
      "WARNING:tensorflow:Layer layer_normalization_551 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "1/1 [==============================] - 0s 274ms/step - loss: 1.6343 - val_loss: 0.0157\n",
      "Epoch 2/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 9.1876 - val_loss: 0.0099\n",
      "Epoch 3/40\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 23.6056 - val_loss: 0.0091\n",
      "Epoch 4/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 10.6952 - val_loss: 0.0081\n",
      "Epoch 5/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 10.5415 - val_loss: 0.0072\n",
      "Epoch 6/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.3521 - val_loss: 0.0081\n",
      "Epoch 7/40\n",
      "1/1 [==============================] - 0s 20ms/step - loss: 5.5468 - val_loss: 0.0079\n",
      "Epoch 8/40\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 6.5233 - val_loss: 0.0077\n",
      "Epoch 9/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.0673 - val_loss: 0.0080\n",
      "Epoch 10/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.0856 - val_loss: 0.0084\n",
      "Epoch 11/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.8183 - val_loss: 0.0083\n",
      "Epoch 12/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 1.2296 - val_loss: 0.0078\n",
      "Epoch 13/40\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.8698 - val_loss: 0.0093\n",
      "Epoch 14/40\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 0.7995 - val_loss: 0.0074\n",
      "Epoch 15/40\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 0.6238 - val_loss: 0.0097\n",
      "Epoch 16/40\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 0.3641 - val_loss: 0.0080\n",
      "Epoch 17/40\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.2790 - val_loss: 0.0090\n",
      "Epoch 18/40\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.3475 - val_loss: 0.0083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/40\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 0.5226 - val_loss: 0.0085\n",
      "Epoch 20/40\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.6087 - val_loss: 0.0083\n",
      "Epoch 00020: early stopping\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x00000250926E55E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "# optimizer = keras.optimizers.Adam(0.0001)\n",
    "# what if verbose=1? DONE!\n",
    "# try learning rate and patience\n",
    "optimizer=keras.optimizers.RMSprop(0.0005)\n",
    "\n",
    "loss = []\n",
    "preds = []\n",
    "seed(1)\n",
    "for retrain_idx in range(552):\n",
    "    X = Xs.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    X_idx = X.apply(pd.Series.nunique) != 1\n",
    "    X = X.loc[:,X_idx]\n",
    "    X_val = Xs.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],X_idx]\n",
    "    X_test = Xs.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],X_idx]\n",
    "    y = y_agg.iloc[t_train_start[retrain_idx]:t_train_end[retrain_idx],:]\n",
    "    y_val = y_agg.loc[t_val_start[retrain_idx]:t_val_end[retrain_idx],:]\n",
    "    y_test = y_agg.loc[t_test_start[retrain_idx]:t_test_end[retrain_idx],:]\n",
    "  \n",
    "    model = Sequential()\n",
    "    model.add(LayerNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(512,activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=optimizer,loss='mse')\n",
    "    early_stop = EarlyStopping(monitor='val_loss', verbose=1, patience=15)\n",
    "    history = model.fit(x=X,y=y,validation_data=(X_val,y_val), batch_size=512, epochs=40,verbose=1,callbacks=[early_stop])\n",
    "    \n",
    "    preds.append(model.predict(X_test))\n",
    "    loss.append(min(history.history['val_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_org = y_agg.loc[t_test_start[0]:t_test_end[551],:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -0.307925\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1-np.sum((np.squeeze(y_org)-np.squeeze(preds))**2)/np.sum((y_org-np.mean(y_org))**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>-0.277289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>-0.145559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>-0.080091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>-0.034259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>0.052814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>0.140529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>0.228682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>0.171740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>0.123710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.196907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>552 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "180 -0.277289\n",
       "181 -0.145559\n",
       "182 -0.080091\n",
       "183 -0.034259\n",
       "184  0.052814\n",
       "..        ...\n",
       "727  0.140529\n",
       "728  0.228682\n",
       "729  0.171740\n",
       "730  0.123710\n",
       "731  0.196907\n",
       "\n",
       "[552 rows x 1 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good R^2 is expected to be between 0 to 0.05."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Finance.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
